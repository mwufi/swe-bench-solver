{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zen.tang/swe-bench-solver/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 225 rows in the dev set\n",
      "                repo              instance_id  \\\n",
      "0  sqlfluff/sqlfluff  sqlfluff__sqlfluff-4764   \n",
      "1  sqlfluff/sqlfluff  sqlfluff__sqlfluff-2862   \n",
      "2  sqlfluff/sqlfluff  sqlfluff__sqlfluff-2336   \n",
      "3  sqlfluff/sqlfluff  sqlfluff__sqlfluff-5074   \n",
      "4  sqlfluff/sqlfluff  sqlfluff__sqlfluff-3436   \n",
      "\n",
      "                                base_commit  \\\n",
      "0  a820c139ccbe6d1865d73c4a459945cd69899f8f   \n",
      "1  447ecf862a4d2b977d0add9f444655357b9c4f1f   \n",
      "2  37a993f7ad841ab3035d1db5ce6525f2e5584fd5   \n",
      "3  7b7fd603a19755a9f3707ebbf95d18ee635716d8   \n",
      "4  23cd31e77a712a210c734e38488d7a34afd83a25   \n",
      "\n",
      "                                               patch  \\\n",
      "0  diff --git a/src/sqlfluff/cli/commands.py b/sr...   \n",
      "1  diff --git a/src/sqlfluff/core/linter/common.p...   \n",
      "2  diff --git a/src/sqlfluff/core/rules/analysis/...   \n",
      "3  diff --git a/src/sqlfluff/core/errors.py b/src...   \n",
      "4  diff --git a/src/sqlfluff/core/templaters/slic...   \n",
      "\n",
      "                                          test_patch  \\\n",
      "0  diff --git a/test/cli/commands_test.py b/test/...   \n",
      "1  diff --git a/test/api/simple_test.py b/test/ap...   \n",
      "2  diff --git a/test/core/rules/reference_test.py...   \n",
      "3  diff --git a/test/cli/commands_test.py b/test/...   \n",
      "4  diff --git a/test/core/templaters/jinja_test.p...   \n",
      "\n",
      "                                   problem_statement  \\\n",
      "0  Enable quiet mode/no-verbose in CLI for use in...   \n",
      "1  fix keep adding new line on wrong place \\n### ...   \n",
      "2  L026: Rule incorrectly flag column does not ex...   \n",
      "3  Inconsistent output depending on --processes f...   \n",
      "4  Fatal templating error with Jinja templater. T...   \n",
      "\n",
      "                                          hints_text            created_at  \\\n",
      "0                                                     2023-04-16T14:24:42Z   \n",
      "1  > Version\\r\\n> sqlfluff, version 0.6.2\\r\\n\\r\\n...  2022-03-14T19:46:08Z   \n",
      "2                                                     2022-01-17T21:35:10Z   \n",
      "3  This is _very_ interesting! I'll pick this one...  2023-08-08T23:31:59Z   \n",
      "4  I'll take a look.\\r\\n\\r\\nAnd darn it -- first ...  2022-06-07T21:36:59Z   \n",
      "\n",
      "  version                                       FAIL_TO_PASS  \\\n",
      "0     1.4  [\"test/cli/commands_test.py::test__cli__fix_mu...   \n",
      "1    0.10  [\"test/api/simple_test.py::test__api__lint_str...   \n",
      "2     0.8  [\"test/core/rules/reference_test.py::test_obje...   \n",
      "3     2.1  [\"test/cli/commands_test.py::test__cli__comman...   \n",
      "4    0.13  [\"test/core/templaters/jinja_test.py::test__te...   \n",
      "\n",
      "                                        PASS_TO_PASS  \\\n",
      "0  [\"test/cli/commands_test.py::test__cli__comman...   \n",
      "1  [\"test/api/simple_test.py::test__api__lint_str...   \n",
      "2                                                 []   \n",
      "3  [\"test/cli/commands_test.py::test__cli__comman...   \n",
      "4  [\"test/core/templaters/jinja_test.py::test__te...   \n",
      "\n",
      "                   environment_setup_commit  \n",
      "0  d19de0ecd16d298f9e3bfb91da122734c40c01e5  \n",
      "1  3d52e8270d82aeccf4c516d059a80a6947919aea  \n",
      "2  a5c4eae4e3e419fe95460c9afd9cf39a35a470c4  \n",
      "3  7b7fd603a19755a9f3707ebbf95d18ee635716d8  \n",
      "4  6e8ce43a4958dbaa56256365c2a89d8db92e07d6  \n",
      "------\n",
      "repo                                                        sqlfluff/sqlfluff\n",
      "instance_id                                           sqlfluff__sqlfluff-4764\n",
      "base_commit                          a820c139ccbe6d1865d73c4a459945cd69899f8f\n",
      "patch                       diff --git a/src/sqlfluff/cli/commands.py b/sr...\n",
      "test_patch                  diff --git a/test/cli/commands_test.py b/test/...\n",
      "problem_statement           Enable quiet mode/no-verbose in CLI for use in...\n",
      "hints_text                                                                   \n",
      "created_at                                               2023-04-16T14:24:42Z\n",
      "version                                                                   1.4\n",
      "FAIL_TO_PASS                [\"test/cli/commands_test.py::test__cli__fix_mu...\n",
      "PASS_TO_PASS                [\"test/cli/commands_test.py::test__cli__comman...\n",
      "environment_setup_commit             d19de0ecd16d298f9e3bfb91da122734c40c01e5\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# online is \"hf://datasets/princeton-nlp/SWE-bench/\"\n",
    "# local is \"~/SWE-bench\"\n",
    "\n",
    "# DATASET_PATH = \"~/SWE-bench\"\n",
    "DATASET_PATH = \"hf://datasets/princeton-nlp/SWE-bench/\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "splits = {\n",
    "    \"dev\": \"data/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "}\n",
    "df = pd.read_parquet(DATASET_PATH + splits[\"dev\"])\n",
    "print(f\"We have {len(df)} rows in the dev set\")\n",
    "print(df.head())\n",
    "print(\"------\")\n",
    "\n",
    "# print one example\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo': 'sqlfluff/sqlfluff',\n",
       " 'instance_id': 'sqlfluff__sqlfluff-4764',\n",
       " 'base_commit': 'a820c139ccbe6d1865d73c4a459945cd69899f8f',\n",
       " 'patch': 'diff --git a/src/sqlfluff/cli/commands.py b/src/sqlfluff/cli/commands.py\\n--- a/src/sqlfluff/cli/commands.py\\n+++ b/src/sqlfluff/cli/commands.py\\n@@ -44,6 +44,7 @@\\n     dialect_selector,\\n     dialect_readout,\\n )\\n+from sqlfluff.core.linter import LintingResult\\n from sqlfluff.core.config import progress_bar_configuration\\n \\n from sqlfluff.core.enums import FormatType, Color\\n@@ -691,12 +692,16 @@ def lint(\\n         sys.exit(EXIT_SUCCESS)\\n \\n \\n-def do_fixes(lnt, result, formatter=None, **kwargs):\\n+def do_fixes(\\n+    result: LintingResult, formatter: Optional[OutputStreamFormatter] = None, **kwargs\\n+):\\n     \"\"\"Actually do the fixes.\"\"\"\\n-    click.echo(\"Persisting Changes...\")\\n+    if formatter and formatter.verbosity >= 0:\\n+        click.echo(\"Persisting Changes...\")\\n     res = result.persist_changes(formatter=formatter, **kwargs)\\n     if all(res.values()):\\n-        click.echo(\"Done. Please check your files to confirm.\")\\n+        if formatter and formatter.verbosity >= 0:\\n+            click.echo(\"Done. Please check your files to confirm.\")\\n         return True\\n     # If some failed then return false\\n     click.echo(\\n@@ -708,7 +713,7 @@ def do_fixes(lnt, result, formatter=None, **kwargs):\\n     return False  # pragma: no cover\\n \\n \\n-def _stdin_fix(linter, formatter, fix_even_unparsable):\\n+def _stdin_fix(linter: Linter, formatter, fix_even_unparsable):\\n     \"\"\"Handle fixing from stdin.\"\"\"\\n     exit_code = EXIT_SUCCESS\\n     stdin = sys.stdin.read()\\n@@ -751,7 +756,7 @@ def _stdin_fix(linter, formatter, fix_even_unparsable):\\n \\n \\n def _paths_fix(\\n-    linter,\\n+    linter: Linter,\\n     formatter,\\n     paths,\\n     processes,\\n@@ -765,11 +770,12 @@ def _paths_fix(\\n ):\\n     \"\"\"Handle fixing from paths.\"\"\"\\n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\\n-    click.echo(\"==== finding fixable violations ====\")\\n+    if formatter.verbosity >= 0:\\n+        click.echo(\"==== finding fixable violations ====\")\\n     exit_code = EXIT_SUCCESS\\n \\n     with PathAndUserErrorHandler(formatter):\\n-        result = linter.lint_paths(\\n+        result: LintingResult = linter.lint_paths(\\n             paths,\\n             fix=True,\\n             ignore_non_existent_files=False,\\n@@ -781,20 +787,18 @@ def _paths_fix(\\n \\n     # NB: We filter to linting violations here, because they\\'re\\n     # the only ones which can be potentially fixed.\\n-    if result.num_violations(types=SQLLintError, fixable=True) > 0:\\n-        click.echo(\"==== fixing violations ====\")\\n-        click.echo(\\n-            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\\n-            \"linting violations found\"\\n-        )\\n+    num_fixable = result.num_violations(types=SQLLintError, fixable=True)\\n+    if num_fixable > 0:\\n+        if formatter.verbosity >= 0:\\n+            click.echo(\"==== fixing violations ====\")\\n+        click.echo(f\"{num_fixable} \" \"fixable linting violations found\")\\n         if force:\\n-            if warn_force:\\n+            if warn_force and formatter.verbosity >= 0:\\n                 click.echo(\\n                     f\"{formatter.colorize(\\'FORCE MODE\\', Color.red)}: \"\\n                     \"Attempting fixes...\"\\n                 )\\n             success = do_fixes(\\n-                linter,\\n                 result,\\n                 formatter,\\n                 types=SQLLintError,\\n@@ -809,9 +813,9 @@ def _paths_fix(\\n             c = click.getchar().lower()\\n             click.echo(\"...\")\\n             if c in (\"y\", \"\\\\r\", \"\\\\n\"):\\n-                click.echo(\"Attempting fixes...\")\\n+                if formatter.verbosity >= 0:\\n+                    click.echo(\"Attempting fixes...\")\\n                 success = do_fixes(\\n-                    linter,\\n                     result,\\n                     formatter,\\n                     types=SQLLintError,\\n@@ -829,8 +833,9 @@ def _paths_fix(\\n                 click.echo(\"Aborting...\")\\n                 exit_code = EXIT_FAIL\\n     else:\\n-        click.echo(\"==== no fixable linting violations found ====\")\\n-        formatter.completion_message()\\n+        if formatter.verbosity >= 0:\\n+            click.echo(\"==== no fixable linting violations found ====\")\\n+            formatter.completion_message()\\n \\n     error_types = [\\n         (\\n@@ -841,7 +846,7 @@ def _paths_fix(\\n     ]\\n     for num_violations_kwargs, message_format, error_level in error_types:\\n         num_violations = result.num_violations(**num_violations_kwargs)\\n-        if num_violations > 0:\\n+        if num_violations > 0 and formatter.verbosity >= 0:\\n             click.echo(message_format.format(num_violations))\\n             exit_code = max(exit_code, error_level)\\n \\n@@ -880,10 +885,20 @@ def _paths_fix(\\n     \"--force\",\\n     is_flag=True,\\n     help=(\\n-        \"skip the confirmation prompt and go straight to applying \"\\n+        \"Skip the confirmation prompt and go straight to applying \"\\n         \"fixes. **Use this with caution.**\"\\n     ),\\n )\\n+@click.option(\\n+    \"-q\",\\n+    \"--quiet\",\\n+    is_flag=True,\\n+    help=(\\n+        \"Reduces the amount of output to stdout to a minimal level. \"\\n+        \"This is effectively the opposite of -v. NOTE: It will only \"\\n+        \"take effect if -f/--force is also set.\"\\n+    ),\\n+)\\n @click.option(\\n     \"-x\",\\n     \"--fixed-suffix\",\\n@@ -913,6 +928,7 @@ def fix(\\n     force: bool,\\n     paths: Tuple[str],\\n     bench: bool = False,\\n+    quiet: bool = False,\\n     fixed_suffix: str = \"\",\\n     logger: Optional[logging.Logger] = None,\\n     processes: Optional[int] = None,\\n@@ -932,6 +948,13 @@ def fix(\\n     \"\"\"\\n     # some quick checks\\n     fixing_stdin = (\"-\",) == paths\\n+    if quiet:\\n+        if kwargs[\"verbose\"]:\\n+            click.echo(\\n+                \"ERROR: The --quiet flag can only be used if --verbose is not set.\",\\n+            )\\n+            sys.exit(EXIT_ERROR)\\n+        kwargs[\"verbose\"] = -1\\n \\n     config = get_config(\\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\\n--- a/src/sqlfluff/cli/formatters.py\\n+++ b/src/sqlfluff/cli/formatters.py\\n@@ -94,7 +94,7 @@ def __init__(\\n     ):\\n         self._output_stream = output_stream\\n         self.plain_output = self.should_produce_plain_output(nocolor)\\n-        self._verbosity = verbosity\\n+        self.verbosity = verbosity\\n         self._filter_empty = filter_empty\\n         self.output_line_length = output_line_length\\n \\n@@ -116,13 +116,13 @@ def _format_config(self, linter: Linter) -> str:\\n         \"\"\"Format the config of a `Linter`.\"\"\"\\n         text_buffer = StringIO()\\n         # Only show version information if verbosity is high enough\\n-        if self._verbosity > 0:\\n+        if self.verbosity > 0:\\n             text_buffer.write(\"==== sqlfluff ====\\\\n\")\\n             config_content = [\\n                 (\"sqlfluff\", get_package_version()),\\n                 (\"python\", get_python_version()),\\n                 (\"implementation\", get_python_implementation()),\\n-                (\"verbosity\", self._verbosity),\\n+                (\"verbosity\", self.verbosity),\\n             ]\\n             if linter.dialect:\\n                 config_content.append((\"dialect\", linter.dialect.name))\\n@@ -138,7 +138,7 @@ def _format_config(self, linter: Linter) -> str:\\n                         col_width=41,\\n                     )\\n                 )\\n-            if self._verbosity > 1:\\n+            if self.verbosity > 1:\\n                 text_buffer.write(\"\\\\n== Raw Config:\\\\n\")\\n                 text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\\n         return text_buffer.getvalue()\\n@@ -150,7 +150,7 @@ def dispatch_config(self, linter: Linter) -> None:\\n     def dispatch_persist_filename(self, filename, result):\\n         \"\"\"Dispatch filenames during a persist operation.\"\"\"\\n         # Only show the skip records at higher levels of verbosity\\n-        if self._verbosity >= 2 or result != \"SKIP\":\\n+        if self.verbosity >= 2 or result != \"SKIP\":\\n             self._dispatch(self.format_filename(filename=filename, success=result))\\n \\n     def _format_path(self, path: str) -> str:\\n@@ -159,14 +159,14 @@ def _format_path(self, path: str) -> str:\\n \\n     def dispatch_path(self, path: str) -> None:\\n         \"\"\"Dispatch paths for display.\"\"\"\\n-        if self._verbosity > 0:\\n+        if self.verbosity > 0:\\n             self._dispatch(self._format_path(path))\\n \\n     def dispatch_template_header(\\n         self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\\n     ) -> None:\\n         \"\"\"Dispatch the header displayed before templating.\"\"\"\\n-        if self._verbosity > 1:\\n+        if self.verbosity > 1:\\n             self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\\n             # This is where we output config diffs if they exist.\\n             if file_config:\\n@@ -182,12 +182,12 @@ def dispatch_template_header(\\n \\n     def dispatch_parse_header(self, fname: str) -> None:\\n         \"\"\"Dispatch the header displayed before parsing.\"\"\"\\n-        if self._verbosity > 1:\\n+        if self.verbosity > 1:\\n             self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\\n \\n     def dispatch_lint_header(self, fname: str, rules: List[str]) -> None:\\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\\n-        if self._verbosity > 1:\\n+        if self.verbosity > 1:\\n             self._dispatch(\\n                 self.format_filename(\\n                     filename=fname, success=f\"LINTING ({\\', \\'.join(rules)})\"\\n@@ -202,7 +202,7 @@ def dispatch_compilation_header(self, templater, message):\\n \\n     def dispatch_processing_header(self, processes: int) -> None:\\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\\n-        if self._verbosity > 0:\\n+        if self.verbosity > 0:\\n             self._dispatch(  # pragma: no cover\\n                 f\"{self.colorize(\\'effective configured processes: \\', Color.lightgrey)} \"\\n                 f\"{processes}\"\\n@@ -228,7 +228,7 @@ def _format_file_violations(\\n         show = fails + warns > 0\\n \\n         # Only print the filename if it\\'s either a failure or verbosity > 1\\n-        if self._verbosity > 0 or show:\\n+        if self.verbosity > 0 or show:\\n             text_buffer.write(self.format_filename(fname, success=fails == 0))\\n             text_buffer.write(\"\\\\n\")\\n \\n@@ -253,6 +253,8 @@ def dispatch_file_violations(\\n         self, fname: str, linted_file: LintedFile, only_fixable: bool\\n     ) -> None:\\n         \"\"\"Dispatch any violations found in a file.\"\"\"\\n+        if self.verbosity < 0:\\n+            return\\n         s = self._format_file_violations(\\n             fname,\\n             linted_file.get_violations(\\n@@ -392,10 +394,13 @@ def format_filename(\\n         if isinstance(success, str):\\n             status_string = success\\n         else:\\n-            status_string = self.colorize(\\n-                success_text if success else \"FAIL\",\\n-                Color.green if success else Color.red,\\n-            )\\n+            status_string = success_text if success else \"FAIL\"\\n+\\n+        if status_string in (\"PASS\", \"FIXED\", success_text):\\n+            status_string = self.colorize(status_string, Color.green)\\n+        elif status_string in (\"FAIL\", \"ERROR\"):\\n+            status_string = self.colorize(status_string, Color.red)\\n+\\n         return f\"== [{self.colorize(filename, Color.lightgrey)}] {status_string}\"\\n \\n     def format_violation(\\ndiff --git a/src/sqlfluff/core/linter/linted_dir.py b/src/sqlfluff/core/linter/linted_dir.py\\n--- a/src/sqlfluff/core/linter/linted_dir.py\\n+++ b/src/sqlfluff/core/linter/linted_dir.py\\n@@ -117,7 +117,11 @@ def persist_changes(\\n         for file in self.files:\\n             if file.num_violations(fixable=True, **kwargs) > 0:\\n                 buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\\n-                result = buffer[file.path]\\n+                result: Union[bool, str]\\n+                if buffer[file.path] is True:\\n+                    result = \"FIXED\"\\n+                else:  # pragma: no cover\\n+                    result = buffer[file.path]\\n             else:  # pragma: no cover TODO?\\n                 buffer[file.path] = True\\n                 result = \"SKIP\"\\n',\n",
       " 'test_patch': 'diff --git a/test/cli/commands_test.py b/test/cli/commands_test.py\\n--- a/test/cli/commands_test.py\\n+++ b/test/cli/commands_test.py\\n@@ -557,6 +557,18 @@ def test__cli__command_lint_parse(command):\\n             ),\\n             1,\\n         ),\\n+        # Test that setting --quiet with --verbose raises an error.\\n+        (\\n+            (\\n+                fix,\\n+                [\\n+                    \"--quiet\",\\n+                    \"--verbose\",\\n+                    \"test/fixtures/cli/fail_many.sql\",\\n+                ],\\n+            ),\\n+            2,\\n+        ),\\n     ],\\n )\\n def test__cli__command_lint_parse_with_retcode(command, ret_code):\\n@@ -1891,7 +1903,7 @@ def test_cli_fix_disabled_progress_bar_deprecated_option(\\n \\n \\n def test__cli__fix_multiple_errors_no_show_errors():\\n-    \"\"\"Basic checking of lint functionality.\"\"\"\\n+    \"\"\"Test the fix output.\"\"\"\\n     result = invoke_assert_code(\\n         ret_code=1,\\n         args=[\\n@@ -1910,8 +1922,57 @@ def test__cli__fix_multiple_errors_no_show_errors():\\n     assert result.output.replace(\"\\\\\\\\\", \"/\").startswith(multiple_expected_output)\\n \\n \\n+def test__cli__fix_multiple_errors_quiet_force():\\n+    \"\"\"Test the fix --quiet option with --force.\"\"\"\\n+    result = invoke_assert_code(\\n+        ret_code=0,\\n+        args=[\\n+            fix,\\n+            [\\n+                \"--disable-progress-bar\",\\n+                \"test/fixtures/linter/multiple_sql_errors.sql\",\\n+                \"--force\",\\n+                \"--quiet\",\\n+                \"-x\",\\n+                \"_fix\",\\n+            ],\\n+        ],\\n+    )\\n+    normalised_output = result.output.replace(\"\\\\\\\\\", \"/\")\\n+    assert normalised_output.startswith(\\n+        \"\"\"1 fixable linting violations found\\n+== [test/fixtures/linter/multiple_sql_errors.sql] FIXED\"\"\"\\n+    )\\n+\\n+\\n+def test__cli__fix_multiple_errors_quiet_no_force():\\n+    \"\"\"Test the fix --quiet option without --force.\"\"\"\\n+    result = invoke_assert_code(\\n+        ret_code=0,\\n+        args=[\\n+            fix,\\n+            [\\n+                \"--disable-progress-bar\",\\n+                \"test/fixtures/linter/multiple_sql_errors.sql\",\\n+                \"--quiet\",\\n+                \"-x\",\\n+                \"_fix\",\\n+            ],\\n+            # Test with the confirmation step.\\n+            \"y\",\\n+        ],\\n+    )\\n+    normalised_output = result.output.replace(\"\\\\\\\\\", \"/\")\\n+    assert normalised_output.startswith(\\n+        \"\"\"1 fixable linting violations found\\n+Are you sure you wish to attempt to fix these? [Y/n] ...\\n+== [test/fixtures/linter/multiple_sql_errors.sql] FIXED\\n+All Finished\"\"\"\\n+    )\\n+\\n+\\n def test__cli__fix_multiple_errors_show_errors():\\n-    \"\"\"Basic checking of lint functionality.\"\"\"\\n+    \"\"\"Test the fix --show-lint-violations option.\"\"\"\\n     result = invoke_assert_code(\\n         ret_code=1,\\n         args=[\\n',\n",
       " 'problem_statement': 'Enable quiet mode/no-verbose in CLI for use in pre-commit hook\\nThere seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\\r\\n\\r\\nIt would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\\r\\n![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\\r\\n\\r\\nThis hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\\n',\n",
       " 'hints_text': '',\n",
       " 'created_at': '2023-04-16T14:24:42Z',\n",
       " 'version': '1.4',\n",
       " 'FAIL_TO_PASS': '[\"test/cli/commands_test.py::test__cli__fix_multiple_errors_quiet_force\", \"test/cli/commands_test.py::test__cli__fix_multiple_errors_quiet_no_force\"]',\n",
       " 'PASS_TO_PASS': '[\"test/cli/commands_test.py::test__cli__command_directed\", \"test/cli/commands_test.py::test__cli__command_dialect\", \"test/cli/commands_test.py::test__cli__command_no_dialect\", \"test/cli/commands_test.py::test__cli__command_parse_error_dialect_explicit_warning\", \"test/cli/commands_test.py::test__cli__command_parse_error_dialect_implicit_warning\", \"test/cli/commands_test.py::test__cli__command_dialect_legacy\", \"test/cli/commands_test.py::test__cli__command_extra_config_fail\", \"test/cli/commands_test.py::test__cli__command_lint_stdin[command0]\", \"test/cli/commands_test.py::test__cli__command_lint_stdin[command1]\", \"test/cli/commands_test.py::test__cli__command_lint_stdin[command2]\", \"test/cli/commands_test.py::test__cli__command_lint_stdin[command3]\", \"test/cli/commands_test.py::test__cli__command_render_stdin\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command0]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command2]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command3]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command4]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command5]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command6]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command7]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command8]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command9]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command10]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command11]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command12]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command13]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command14]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command15]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command16]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command17]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command18]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command19]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command20]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command21]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command22]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command23]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command24]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command25]\", \"test/cli/commands_test.py::test__cli__command_lint_parse[command26]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command0-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command1-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command2-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command3-0]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command4-0]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command5-2]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command6-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command7-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command8-1]\", \"test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command9-2]\", \"test/cli/commands_test.py::test__cli__command_lint_warning_explicit_file_ignored\", \"test/cli/commands_test.py::test__cli__command_lint_skip_ignore_files\", \"test/cli/commands_test.py::test__cli__command_lint_ignore_local_config\", \"test/cli/commands_test.py::test__cli__command_lint_warning\", \"test/cli/commands_test.py::test__cli__command_versioning\", \"test/cli/commands_test.py::test__cli__command_version\", \"test/cli/commands_test.py::test__cli__command_rules\", \"test/cli/commands_test.py::test__cli__command_dialects\", \"test/cli/commands_test.py::test__cli__command__fix[LT01-test/fixtures/linter/indentation_errors.sql0]\", \"test/cli/commands_test.py::test__cli__command__fix[LT01-test/fixtures/linter/whitespace_errors.sql]\", \"test/cli/commands_test.py::test__cli__command__fix[LT01-test/fixtures/linter/indentation_errors.sql1]\", \"test/cli/commands_test.py::test__cli__command__fix[LT02-test/fixtures/linter/indentation_error_hard.sql]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[1_lint_error_1_unsuppressed_parse_error]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[1_lint_error_1_unsuppressed_templating_error]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[1_lint_error_1_suppressed_parse_error]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[0_lint_errors_1_unsuppressed_parse_error]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[0_lint_errors_1_suppressed_parse_error]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[1_lint_error_1_unsuppressed_parse_error_FIX_EVEN_UNPARSABLE]\", \"test/cli/commands_test.py::test__cli__fix_error_handling_behavior[2_files_with_lint_errors_1_unsuppressed_parse_error]\", \"test/cli/commands_test.py::test_cli_fix_even_unparsable[command-line-False]\", \"test/cli/commands_test.py::test_cli_fix_even_unparsable[command-line-True]\", \"test/cli/commands_test.py::test_cli_fix_even_unparsable[config-file-False]\", \"test/cli/commands_test.py::test_cli_fix_even_unparsable[config-file-True]\", \"test/cli/commands_test.py::test__cli__fix_loop_limit_behavior[--\", \"test/cli/commands_test.py::test__cli__command_fix_stdin[select\", \"test/cli/commands_test.py::test__cli__command_fix_stdin[\", \"test/cli/commands_test.py::test__cli__command_format_stdin[select\", \"test/cli/commands_test.py::test__cli__command_format_stdin[\", \"test/cli/commands_test.py::test__cli__command_fix_stdin_logging_to_stderr\", \"test/cli/commands_test.py::test__cli__command_fix_stdin_safety\", \"test/cli/commands_test.py::test__cli__command_fix_stdin_error_exit_code[create\", \"test/cli/commands_test.py::test__cli__command_fix_stdin_error_exit_code[select\", \"test/cli/commands_test.py::test__cli__command__fix_no_force[LT01-test/fixtures/linter/indentation_errors.sql-y-0-0]\", \"test/cli/commands_test.py::test__cli__command__fix_no_force[LT01-test/fixtures/linter/indentation_errors.sql-n-1-1]\", \"test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[None-yaml]\", \"test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[None-json]\", \"test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[outfile-yaml]\", \"test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[outfile-json]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_from_stdin[select\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_from_stdin[SElect\", \"test/cli/commands_test.py::test__cli__command_fail_nice_not_found[command0]\", \"test/cli/commands_test.py::test__cli__command_fail_nice_not_found[command1]\", \"test/cli/commands_test.py::test__cli__command_lint_nocolor\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-human]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-yaml]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-json]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-github-annotation]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-github-annotation-native]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[None-none]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-human]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-yaml]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-json]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-github-annotation]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-github-annotation-native]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[outfile-none]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_github_annotation\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_github_annotation_native\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_annotation_level_error_failure_equivalent[github-annotation]\", \"test/cli/commands_test.py::test__cli__command_lint_serialize_annotation_level_error_failure_equivalent[github-annotation-native]\", \"test/cli/commands_test.py::test___main___help\", \"test/cli/commands_test.py::test_encoding[utf-8-ascii]\", \"test/cli/commands_test.py::test_encoding[utf-8-sig-UTF-8-SIG]\", \"test/cli/commands_test.py::test_encoding[utf-32-UTF-32]\", \"test/cli/commands_test.py::test_cli_encoding[utf-8-command-line-False]\", \"test/cli/commands_test.py::test_cli_encoding[utf-8-SIG-command-line-True]\", \"test/cli/commands_test.py::test_cli_encoding[utf-8-config-file-False]\", \"test/cli/commands_test.py::test_cli_encoding[utf-8-SIG-config-file-True]\", \"test/cli/commands_test.py::test_cli_no_disable_noqa_flag\", \"test/cli/commands_test.py::test_cli_disable_noqa_flag\", \"test/cli/commands_test.py::test_cli_get_default_config\", \"test/cli/commands_test.py::TestProgressBars::test_cli_lint_disabled_progress_bar\", \"test/cli/commands_test.py::TestProgressBars::test_cli_lint_disabled_progress_bar_deprecated_option\", \"test/cli/commands_test.py::TestProgressBars::test_cli_lint_enabled_progress_bar\", \"test/cli/commands_test.py::TestProgressBars::test_cli_lint_enabled_progress_bar_multiple_paths\", \"test/cli/commands_test.py::TestProgressBars::test_cli_lint_enabled_progress_bar_multiple_files\", \"test/cli/commands_test.py::TestProgressBars::test_cli_fix_disabled_progress_bar\", \"test/cli/commands_test.py::TestProgressBars::test_cli_fix_disabled_progress_bar_deprecated_option\", \"test/cli/commands_test.py::test__cli__fix_multiple_errors_no_show_errors\", \"test/cli/commands_test.py::test__cli__fix_multiple_errors_show_errors\", \"test/cli/commands_test.py::test__cli__multiple_files__fix_multiple_errors_show_errors\", \"test/cli/commands_test.py::test__cli__render_fail\", \"test/cli/commands_test.py::test__cli__render_pass\"]',\n",
       " 'environment_setup_commit': 'd19de0ecd16d298f9e3bfb91da122734c40c01e5'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "dict(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 308 lines----------------\n",
      "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\n",
      "There seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\n",
      "\n",
      "It would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\n",
      "![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\n",
      "\n",
      "This hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 359 lines----------------\n",
      "fix keep adding new line on wrong place \n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "To replicate this issue you can create a file eg. test.template.sql \n",
      "\n",
      "```\n",
      "{% if true %}\n",
      "SELECT 1 + 1\n",
      "{%- endif %}\n",
      "```\n",
      "\n",
      "then run:\n",
      "```\n",
      "sqlfluff fix test.template.sql  \n",
      "```\n",
      "\n",
      "This will give you:\n",
      "```\n",
      "L:   2 | P:  12 | L009 | Files must end with a trailing newline.\n",
      "```\n",
      "\n",
      "And the result of the file is now:\n",
      "```\n",
      "{% if true %}\n",
      "SELECT 1 + 1\n",
      "\n",
      "{%- endif %}\n",
      "```\n",
      "\n",
      "If i run it again it will complain on the same issue and the result of the file would be: \n",
      "```\n",
      "{% if true %}\n",
      "SELECT 1 + 1\n",
      "\n",
      "\n",
      "{%- endif %}\n",
      "```\n",
      "\n",
      "And so on. \n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "The expected behavior would be to add the new line at the end of the file, that is after `{%- endif %}` instead of adding the new line at the end of the SQL query - so the result should look like this: \n",
      "\n",
      "```\n",
      "{% if true %}\n",
      "SELECT 1 + 1\n",
      "{%- endif %}\n",
      "\n",
      "```\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Adds a new line to the end of the SQL query instead of in the end of the file. \n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Already mentioned above (in What Happened section).\n",
      "\n",
      "### Dialect\n",
      "\n",
      "snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "sqlfluff, version 0.6.2\n",
      "\n",
      "### Configuration\n",
      "\n",
      "[sqlfluff]\n",
      "verbose = 1\n",
      "dialect = snowflake\n",
      "templater = jinja\n",
      "exclude_rules = L027,L031,L032,L036,L044,L046,L034\n",
      "output_line_length = 121\n",
      "sql_file_exts=.sql\n",
      "\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "max_line_length = 250\n",
      "indent_unit = space\n",
      "comma_style = trailing\n",
      "allow_scalar = True\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = aliases\n",
      "\n",
      "\n",
      "[sqlfluff:rules:L010]  # Keywords\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "extended_capitalisation_policy = lower\n",
      "\n",
      "[sqlfluff:rules:L030]  # function names\n",
      "capitalisation_policy = upper\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 364 lines----------------\n",
      "L026: Rule incorrectly flag column does not exist in `FROM` clause in an UPDATE statement.\n",
      "## Expected Behaviour\n",
      "\n",
      "L026 should not fail when a subquery in an UPDATE statement references a column from the UPDATE target.\n",
      "\n",
      "## Observed Behaviour\n",
      "\n",
      "L026 failed due to reference was not found in the FROM clause with the following error printed (When using `sample.sql` content below)\n",
      "\n",
      "```\n",
      "L:   7 | P:  28 | L026 | Reference 'my_table.id' refers to table/view not found\n",
      "                       | in the FROM clause or found in parent subquery.\n",
      "```\n",
      "\n",
      "## Steps to Reproduce\n",
      "\n",
      "1. Create `sample.sql` with the content below\n",
      "```\n",
      "UPDATE my_table\n",
      "SET row_sum = (\n",
      "    SELECT COUNT(*) AS row_sum\n",
      "    FROM\n",
      "        another_table\n",
      "    WHERE\n",
      "        another_table.id = my_table.id\n",
      ");\n",
      "```\n",
      "2. Run SQLFluff by `sqlfluff lint sample.sql`\n",
      "\n",
      "## Dialect\n",
      "\n",
      "Default / Ansi (No dialect specified)\n",
      "\n",
      "## Version\n",
      "```\n",
      "(.venv) ~/code/sqlfluff (main) $ sqlfluff --version\n",
      "sqlfluff, version 0.9.0\n",
      "```\n",
      "\n",
      "```\n",
      "(.venv) ~/code/sqlfluff (main) $ python --version\n",
      "Python 3.9.9\n",
      "```\n",
      "\n",
      "## Configuration\n",
      "Default. No customization.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 112 lines----------------\n",
      "Inconsistent output depending on --processes flag when --ignore linting is used\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Depending on the value you set for the `--processes` flag when also using `--ignore linting`, different output with different exit codes are generated.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "The same exit code should be generated, independently of the `--processes` flag. Furthermore, from https://docs.sqlfluff.com/en/stable/production.html#using-sqlfluff-on-a-whole-sql-codebase I would expect that exit codes should be either `0` or `65`, not `1`.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "See the How to reproduce section.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Create a `test.sql` file with the following content:\n",
      "\n",
      "```SQL\n",
      "CREATE TABLE example (\n",
      "    id TEXT DEFAULT 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. In condimentum congue est, ac orci aliquam.' PRIMARY KEY\n",
      ");\n",
      "```\n",
      "\n",
      "The line is too long according to SQLFluff, caused by the large default value, so let's see the the output of SQLFluff.\n",
      "\n",
      "Running\n",
      "\n",
      "```SHELL\n",
      "sqlfluff fix --dialect postgres --ignore linting --processes 2\n",
      "```\n",
      "\n",
      "results in \n",
      "\n",
      "```\n",
      "==== finding fixable violations ====\n",
      "==== no fixable linting violations found ====                                                                                                                                                                      \n",
      "All Finished ðŸ“œ ðŸŽ‰!\n",
      "  [1 unfixable linting violations found]\n",
      "```\n",
      "\n",
      "with exit code `1`. Running the same with one process instead:\n",
      "\n",
      "```SHELL\n",
      "sqlfluff fix --dialect postgres --ignore linting --processes 1\n",
      "```\n",
      "\n",
      "results in\n",
      "\n",
      "```\n",
      "==== finding fixable violations ====\n",
      "==== no fixable linting violations found ====                                                                                                                                                                      \n",
      "All Finished ðŸ“œ ðŸŽ‰!\n",
      "```\n",
      "\n",
      "and exit code `0`\n",
      "\n",
      "Same behaviour for `lint` and `format` commands.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Postgres\n",
      "\n",
      "### Version\n",
      "\n",
      "2.2.0, Python 3.10.6\n",
      "\n",
      "### Configuration\n",
      "\n",
      "None, it's all in the CLI flags.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 143 lines----------------\n",
      "Fatal templating error with Jinja templater. Tracer produces odd results.\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Issue found while assessing an Airflow project.\n",
      "\n",
      "The smallest query I can make which triggers the issue is: \n",
      "```sql\n",
      "SELECT\n",
      "\t{% block table_name %}a{% endblock %}.b\n",
      "FROM d.{{ self.table_name() }}\n",
      "```\n",
      "\n",
      "When running this query through `lint` I get an `AssertionError`, or if running on the more friendly error message PR (#3433) I get: `WARNING    Length of templated file mismatch with final slice: 21 != 19.`.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "This query should slice properly and probably eventually give a jinja error that the required variables are undefined.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "I've dug a little into the error and the sliced file being produced is:\n",
      "\n",
      "```python\n",
      "[\n",
      "    TemplatedFileSlice(slice_type='literal', source_slice=slice(0, 8, None), templated_slice=slice(0, 8, None)),\n",
      "    TemplatedFileSlice(slice_type='block_start', source_slice=slice(8, 30, None), templated_slice=slice(8, 8, None)),\n",
      "    TemplatedFileSlice(slice_type='literal', source_slice=slice(30, 31, None), templated_slice=slice(8, 9, None)),\n",
      "    TemplatedFileSlice(slice_type='block_end', source_slice=slice(31, 45, None), templated_slice=slice(9, 9, None)),\n",
      "    TemplatedFileSlice(slice_type='literal', source_slice=slice(45, 55, None), templated_slice=slice(9, 19, None)),\n",
      "    TemplatedFileSlice(slice_type='templated', source_slice=slice(55, 78, None), templated_slice=slice(19, 19, None)),\n",
      "    TemplatedFileSlice(slice_type='literal', source_slice=slice(78, 79, None), templated_slice=slice(19, 19, None))\n",
      "]\n",
      "```\n",
      "\n",
      "The issue is that while the `source_slice` looks correct for the slices, almost all of the `templated_slices` values have zero length, and importantly the last one doesn't end at position 21.\n",
      "\n",
      "The rendered file is `SELECT\\n\\ta.b\\nFROM d.a\\n` (I've included the escape chars) which is indeed 21 chars long.\n",
      "\n",
      "@barrywhart I might need your help to work out what's going on with the Jinja tracer here.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Run provided query, `main` branch. Set to the `jinja` templater.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "dialect is set to `snowflake`, but I don't think we're getting far enough for that to make a difference.\n",
      "\n",
      "### Version\n",
      "\n",
      "`main` branch commit `cb6357c540d2d968f766f3a7a4fa16f231cb80e4` (and a few branches derived from it)\n",
      "\n",
      "### Configuration\n",
      "\n",
      "N/A\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 198 lines----------------\n",
      "Lint and fix throws exception when having jinja for loop inside set\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "To reproduce the error, create test.template.sql\n",
      "```\n",
      "{% set whitelisted= [\n",
      "    {'name': 'COL_1'},\n",
      "    {'name': 'COL_2'},\n",
      "    {'name': 'COL_3'}\n",
      "] %}\n",
      "\n",
      "{% set some_part_of_the_query %}\n",
      "    {% for col in whitelisted %}\n",
      "    {{col.name}}{{ \", \" if not loop.last }}\n",
      "    {% endfor %}\n",
      "{% endset %}\n",
      "\n",
      "SELECT {{some_part_of_the_query}}\n",
      "FROM SOME_TABLE\n",
      "\n",
      "```\n",
      "\n",
      "when running lint i get this error:\n",
      "```\n",
      "==== sqlfluff ====\n",
      "sqlfluff:               0.11.0 python:                 3.8.12\n",
      "implementation:        cpython dialect:             snowflake\n",
      "verbosity:                   1 templater:               jinja\n",
      "\n",
      "==== readout ====\n",
      "\n",
      "=== [ path: test.template.sql ] ===\n",
      "\n",
      "WARNING    Unable to lint test.template.sql due to an internal error. Please report this as an issue with your query's contents and stacktrace below!\n",
      "To hide this warning, add the failing file to .sqlfluffignore\n",
      "Traceback (most recent call last):\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\n",
      "    yield partial()\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\n",
      "    parsed = cls.parse_rendered(rendered)\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendered\n",
      "    tokens, lvs, config = cls._lex_templated_file(\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_templated_file\n",
      "    tokens, lex_vs = lexer.lex(templated_file)\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\n",
      "    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_segments\n",
      "    source_slice = templated_file.templated_slice_to_source_slice(\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 258, in templated_slice_to_source_slice\n",
      "    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\n",
      "  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 177, in _find_slice_indices_of_templated_pos\n",
      "    raise ValueError(\"Position Not Found\")\n",
      "ValueError: Position Not Found\n",
      " \n",
      "==== summary ====\n",
      "violations:        0 status:         PASS\n",
      "All Finished ðŸ“œ ðŸŽ‰!\n",
      "\n",
      "```\n",
      "\n",
      "This is the rendered query:\n",
      "```\n",
      " SELECT\n",
      "\n",
      "    COL_1,\n",
      "\n",
      "    COL_2,\n",
      "\n",
      "    COL_3\n",
      "\n",
      "\n",
      "FROM SOME_TABLE\n",
      "\n",
      "```\n",
      "\n",
      "And when trying around to make this work i removed the new lines between the selected columns like this:\n",
      "```\n",
      "{% set whitelisted= [\n",
      "    {'name': 'COL_1'},\n",
      "    {'name': 'COL_2'},\n",
      "    {'name': 'COL_3'}\n",
      "] %}\n",
      "\n",
      "{% set some_part_of_the_query %}\n",
      "    {% for col in whitelisted -%}\n",
      "    {{col.name}}{{ \", \" if not loop.last }}\n",
      "    {% endfor -%}\n",
      "{% endset %}\n",
      "\n",
      "SELECT {{some_part_of_the_query}}\n",
      "FROM SOME_TABLE\n",
      "\n",
      "```\n",
      "\n",
      "which renders:\n",
      "```\n",
      "SELECT\n",
      "    COL_1,\n",
      "    COL_2,\n",
      "    COL_3\n",
      "\n",
      "FROM SOME_TABLE\n",
      "\n",
      "```\n",
      "\n",
      "And this will make the linter pass:\n",
      "\n",
      "```\n",
      "==== sqlfluff ====\n",
      "sqlfluff:               0.11.0 python:                 3.8.12\n",
      "implementation:        cpython dialect:             snowflake\n",
      "verbosity:                   1 templater:               jinja\n",
      "\n",
      "==== readout ====\n",
      "\n",
      "=== [ path: test.template.sql ] ===\n",
      "\n",
      "== [test.template.sql] PASS                                                                                                                          \n",
      "==== summary ====\n",
      "violations:        0 status:         PASS\n",
      "All Finished ðŸ“œ ðŸŽ‰!\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "My expectations is that the linter and fix should pass.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Right now lint and fix throws exception (see \"What Happened\" section)\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Mentioned above.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "sqlfluff, version 0.11.0\n",
      "\n",
      "### Configuration\n",
      "\n",
      "[sqlfluff]\n",
      "verbose = 1\n",
      "dialect = snowflake\n",
      "templater = jinja\n",
      "exclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\n",
      "output_line_length = 121\n",
      "sql_file_exts=.sql\n",
      "\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "max_line_length = 250\n",
      "indent_unit = space\n",
      "comma_style = trailing\n",
      "allow_scalar = True\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = aliases\n",
      "\n",
      "[sqlfluff:rules:L042]\n",
      "forbid_subquery_in = both\n",
      "\n",
      "[sqlfluff:rules:L010]  # Keywords\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "extended_capitalisation_policy = lower\n",
      "\n",
      "[sqlfluff:rules:L030]  # function names\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 76 lines----------------\n",
      "Whitespace token is_whitespace is False\n",
      "I expect segment.is_whitespace of a Whitespace token is True, however, it is set to False.\n",
      "\n",
      "## Expected Behaviour\n",
      "segment.is_whitespace return True\n",
      "\n",
      "## Observed Behaviour\n",
      "segment.is_whitespace return False\n",
      "## Steps to Reproduce\n",
      "\n",
      "## Version\n",
      "Include the output of `sqlfluff --version` along with your Python version\n",
      "\n",
      "## Configuration\n",
      "```\n",
      "Include your SQLFluff configuration here\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 26 lines----------------\n",
      "--disable_progress_bar Flag Broken for Fix\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "I ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Should run as expected, with no error and no progress bar.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Exit code 2 and stderr:\n",
      "```\n",
      "Usage: sqlfluff fix [OPTIONS] [PATHS]...\n",
      "        Try 'sqlfluff fix -h' for help.\n",
      "\n",
      "        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Sql file:\n",
      "```\n",
      "SELECT foo FROM bar;\n",
      "```\n",
      "\n",
      "Command:\n",
      "```\n",
      "sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Version\n",
      "\n",
      "python 3.10.3\n",
      "sqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n",
      "\n",
      "### Configuration\n",
      "\n",
      "No special configuration. Ran hermetically with `trunk`.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "--disable_progress_bar Flag Broken for Fix\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "I ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Should run as expected, with no error and no progress bar.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Exit code 2 and stderr:\n",
      "```\n",
      "Usage: sqlfluff fix [OPTIONS] [PATHS]...\n",
      "        Try 'sqlfluff fix -h' for help.\n",
      "\n",
      "        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Sql file:\n",
      "```\n",
      "SELECT foo FROM bar;\n",
      "```\n",
      "\n",
      "Command:\n",
      "```\n",
      "sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Version\n",
      "\n",
      "python 3.10.3\n",
      "sqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n",
      "\n",
      "### Configuration\n",
      "\n",
      "No special configuration. Ran hermetically with `trunk`.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 35 lines----------------\n",
      "TypeError when using integer placeholder\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "An exception occurs when trying to use integer substituents.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Work without errors.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "\n",
      "An exception occurs:\n",
      "```\n",
      "  ...\n",
      "  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 816, in render_file\n",
      "    return self.render_string(raw_file, fname, config, encoding)\n",
      "  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 787, in render_string\n",
      "    templated_file, templater_violations = self.templater.process(\n",
      "  File \"venv/lib/python3.9/site-packages/sqlfluff/core/templaters/placeholder.py\", line 183, in process\n",
      "    start_template_pos, start_template_pos + len(replacement), None\n",
      "TypeError: object of type 'int' has no len()\n",
      "\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "1. Create a file `example.sql`:\n",
      "```\n",
      "SELECT 1\n",
      "LIMIT %(capacity)s;\n",
      "```\n",
      "2. Copy `.sqlfluff` from the Configuration section\n",
      "3. Run `sqlfluff lint --dialect postgres example.sql`\n",
      "\n",
      "### Dialect\n",
      "\n",
      "postgres\n",
      "\n",
      "### Version\n",
      "\n",
      "sqlfluff, version 0.13.1\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "exclude_rules = L031\n",
      "templater = placeholder\n",
      "\n",
      "[sqlfluff:templater:placeholder]\n",
      "param_style = pyformat\n",
      "capacity = 15\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "Support Postgres-style variable substitution\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "The Postgres `psql` utility supports flavor of colon-style variable substitution that currently confuses sqlfluff.  E.g.,\n",
      "\n",
      "```sql\n",
      "ALTER TABLE name:variable RENAME TO name;\n",
      "```\n",
      "\n",
      "Running the above through sqlfluff produces this output:\n",
      "\n",
      "```\n",
      "sqlfluff lint --dialect postgres 2.sql\n",
      "== [2.sql] FAIL\n",
      "L:   1 | P:   1 |  PRS | Line 1, Position 1: Found unparsable section: 'ALTER\n",
      "                       | TABLE name:variable RENAME TO name...'\n",
      "```\n",
      "\n",
      "### Use case\n",
      "\n",
      "I would like it if in the above the string \"name:variable\" were considered a valid table name (and other identifiers similarly).\n",
      "\n",
      "### Dialect\n",
      "\n",
      "This applies to the Postgres dialect.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 821 lines----------------\n",
      "L042 loop limit on fixes reached when CTE itself contains a subquery\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "While running `sqlfluff fix --dialect snowflake` on a sql file, I get \n",
      "```\n",
      "==== finding fixable violations ====\n",
      "WARNING    Loop limit on fixes reached [10].                                                                                                                                                              \n",
      "==== no fixable linting violations found ====                                                                                                                                                             \n",
      "All Finished ðŸ“œ ðŸŽ‰!\n",
      "  [22 unfixable linting violations found]\n",
      "```\n",
      "\n",
      "```\n",
      "INSERT OVERWRITE INTO dwh.test_table\n",
      "\n",
      "WITH cte1 AS (\n",
      "\tSELECT *\n",
      "\tFROM (SELECT\n",
      "\t\t*,\n",
      "\t\tROW_NUMBER() OVER (PARTITION BY r ORDER BY updated_at DESC) AS latest\n",
      "\t\tFROM mongo.temp\n",
      "\tWHERE latest = 1\n",
      "))\n",
      "\n",
      "SELECT * FROM cte1 WHERE 1=1;\n",
      "```\n",
      "\n",
      "All of the 22  violations are a mix of L002, L003 and L004.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "`sqlfluff` should be able to fix the violations\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Even if I try to fix the violations manually, it still shows the same error.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "I will try to generate a sql file that will be able to reproduce the issue\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "1.1.0\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "# https://docs.sqlfluff.com/en/stable/rules.html\n",
      "\n",
      "[sqlfluff]\n",
      "exclude_rules = L029, L031, L034\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "indented_joins = true\n",
      "indented_using_on = true\n",
      "\n",
      "[sqlfluff:rules:L002]\n",
      "tab_space_size = 4\n",
      "\n",
      "[sqlfluff:rules:L003]\n",
      "hanging_indents = true\n",
      "indent_unit = tab\n",
      "tab_space_size = 4\n",
      "\n",
      "[sqlfluff:rules:L004]\n",
      "indent_unit = tab\n",
      "tab_space_size = 4\n",
      "\n",
      "[sqlfluff:rules:L010]\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L011]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L012]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "extended_capitalisation_policy = lower\n",
      "\n",
      "[sqlfluff:rules:L016]\n",
      "ignore_comment_clauses = true\n",
      "ignore_comment_lines = true\n",
      "indent_unit = tab\n",
      "tab_space_size = 4\n",
      "\n",
      "[sqlfluff:rules:L019]\n",
      "comma_style = trailing\n",
      "\n",
      "[sqlfluff:rules:L022]\n",
      "comma_style = trailing\n",
      "\n",
      "[sqlfluff:rules:L028]\n",
      "single_table_references = unqualified\n",
      "\n",
      "[sqlfluff:rules:L030]\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L040]\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L042]\n",
      "forbid_subquery_in = both\n",
      "\n",
      "[sqlfluff:rules:L054]\n",
      "group_by_and_order_by_style = explicit\n",
      "\n",
      "[sqlfluff:rules:L063]\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L066]\n",
      "min_alias_length = 3\n",
      "max_alias_length = 15\n",
      "\n",
      "[sqlfluff:templater:jinja:context]\n",
      "params = {\"DB\": \"DEMO\"}\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 221 lines----------------\n",
      "Return codes are inconsistent\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Working on #3431 - I noticed that we're inconsistent in our return codes.\n",
      "\n",
      "In `commands.py` we call `sys.exit()` in 15 places (currently).\n",
      "\n",
      "- Twice we call `sys.exit(0)` on success, at the end of `parse` and `lint` (`fix` is a handled differently, see below). âœ”ï¸ \n",
      "- Six times we call `sys.exit(1)` for a selection of things:\n",
      "  - Not having `cProfiler` installed.\n",
      "  - Failing to apply fixes\n",
      "  - User Errors and OSError (in `PathAndUserErrorHandler`)\n",
      "- Five times we call `sys.exit(66)` for a selection of things:\n",
      "  - User Errors (including unknown dialect or failing to load a dialect or config)\n",
      "  - If parsing failed when calling `parse`.\n",
      "- Once we use `handle_files_with_tmp_or_prs_errors` to determine the exit code (which returns 1 or 0)\n",
      "- Once we use `LintingResult.stats` to determine the exit code (which returns either 65 or 0)\n",
      "- Once we do a mixture of the above (see end of `fix`)\n",
      "\n",
      "This neither DRY, or consistent ... or helpful?\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "We should have consistent return codes for specific scenarios. There are up for discussion, but I would suggest:\n",
      "\n",
      "- 0 for success (obviously)\n",
      "- 1 for a fail which is error related: not having libraries installed, user errors etc...\n",
      "- 65 for a linting fail (i.e. no errors in running, but issues were found in either parsing or linting).\n",
      "- 66 for a fixing fail (i.e. we tried to fix errors but failed to do so for some reason).\n",
      "\n",
      "These would be defined as constants at the top of `commands.py`.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "see above\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "see above\n",
      "\n",
      "### Dialect\n",
      "\n",
      "N/A\n",
      "\n",
      "### Version\n",
      "\n",
      "Description is as per code in #3431\n",
      "\n",
      "### Configuration\n",
      "\n",
      "-\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 28 lines----------------\n",
      "L027: outer-level table not found in WHERE clause sub-select\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Outer-level table/view referenced in sub-select inside `WHERE` clause is not being detected.\n",
      "\n",
      "This error seems to only occur when the sub-select contains joins.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "No error\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "```\n",
      "L:   7 | P:  32 | L027 | Qualified reference 'my_table.kind' not found in\n",
      "                       | available tables/view aliases ['other_table',\n",
      "                       | 'mapping_table'] in select with more than one referenced\n",
      "                       | table/view.\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "```sql\n",
      "SELECT my_col\n",
      "FROM my_table\n",
      "WHERE EXISTS (\n",
      "    SELECT 1\n",
      "    FROM other_table\n",
      "    INNER JOIN mapping_table ON (mapping_table.other_fk = other_table.id_pk)\n",
      "    WHERE mapping_table.kind = my_table.kind\n",
      ");\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "postgres\n",
      "\n",
      "### Version\n",
      "\n",
      "sqlfluff, version 0.12.0\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "nocolor = True\n",
      "dialect = postgres\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 154 lines----------------\n",
      "Standardise `--disable_progress_bar` naming\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "As noted in https://github.com/sqlfluff/sqlfluff/pull/3610#discussion_r926014745 `--disable_progress_bar` is the only command line option using underscores instead of dashes.\n",
      "\n",
      "Should we change this?\n",
      "\n",
      "This would be a breaking change, so do we leave until next major release?\n",
      "Or do we accept both options?\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "We should be standard in out command line option format\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "`--disable_progress_bar` is the only non-standard one\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "N/A\n",
      "\n",
      "### Dialect\n",
      "\n",
      "N/A\n",
      "\n",
      "### Version\n",
      "\n",
      "1.2.1\n",
      "\n",
      "### Configuration\n",
      "\n",
      "N/A\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 53 lines----------------\n",
      "layout.end-of-file is the only rule in kebab case\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Our rules are all in `snake_case`, except for `layout.end-of-file`\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "All rules should be in snake case\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "As above\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "-\n",
      "\n",
      "### Dialect\n",
      "\n",
      "NA\n",
      "\n",
      "### Version\n",
      "\n",
      "Main\n",
      "\n",
      "### Configuration\n",
      "\n",
      "NA\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 34 lines----------------\n",
      "2.0.2 - LT02 issues when query contains \"do\" statement.\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "SQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\n",
      "The example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\n",
      "\n",
      "If I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I expect the SQL to pass the linting tests.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Output from SQLFluff v2.0.2:\n",
      "```\n",
      "L:   5 | P:   1 | LT02 | Line should not be indented.\n",
      "                       | [layout.indent]\n",
      "L:   6 | P:   1 | LT02 | Line should not be indented.\n",
      "                       | [layout.indent]\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "SQL to reproduce:\n",
      "```\n",
      "{% set cols = ['a', 'b'] %}\n",
      "{% do cols.remove('a') %}\n",
      "\n",
      "{% if true %}\n",
      "    select a\n",
      "    from some_table\n",
      "{% endif %}\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "`ansi`\n",
      "\n",
      "### Version\n",
      "\n",
      "```\n",
      "> sqlfluff --version\n",
      "sqlfluff, version 2.0.2\n",
      "\n",
      "> python --version\n",
      "Python 3.9.9\n",
      "```\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "dialect = ansi\n",
      "templater = jinja\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "template_blocks_indent = True\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "2.0.2 - LT02 issues when query contains \"do\" statement.\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "SQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\n",
      "The example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\n",
      "\n",
      "If I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I expect the SQL to pass the linting tests.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Output from SQLFluff v2.0.2:\n",
      "```\n",
      "L:   5 | P:   1 | LT02 | Line should not be indented.\n",
      "                       | [layout.indent]\n",
      "L:   6 | P:   1 | LT02 | Line should not be indented.\n",
      "                       | [layout.indent]\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "SQL to reproduce:\n",
      "```\n",
      "{% set cols = ['a', 'b'] %}\n",
      "{% do cols.remove('a') %}\n",
      "\n",
      "{% if true %}\n",
      "    select a\n",
      "    from some_table\n",
      "{% endif %}\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "`ansi`\n",
      "\n",
      "### Version\n",
      "\n",
      "```\n",
      "> sqlfluff --version\n",
      "sqlfluff, version 2.0.2\n",
      "\n",
      "> python --version\n",
      "Python 3.9.9\n",
      "```\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "dialect = ansi\n",
      "templater = jinja\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "template_blocks_indent = True\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 58 lines----------------\n",
      "sqlfluff doesn't recognise a jinja variable set inside of \"if\" statement\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "When I try to define a jinja variable using \"set\" jinja directive inside of an \"if\" jinja statement, sqlfluff complains: \n",
      "\"Undefined jinja template variable\".\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "to not have a linting issue\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "sqlfluff lint gives an error:\n",
      "\"Undefined jinja template variable\"\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "try to create a \"temp.sql\" file with the following content\n",
      "\n",
      "```\n",
      "{% if True %}\n",
      "    {% set some_var %}1{% endset %}\n",
      "    SELECT {{some_var}}\n",
      "{% endif %}\n",
      "```\n",
      "\n",
      "and run:\n",
      "```\n",
      "sqlfluff lint ./temp.sql\n",
      "```\n",
      "\n",
      "You will get the following error:\n",
      "```\n",
      "== [./temp.sql] FAIL                                                                                                                    \n",
      "L:   2 | P:  12 |  TMP | Undefined jinja template variable: 'some_var'\n",
      "L:   3 | P:  14 |  TMP | Undefined jinja template variable: 'some_var'\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "tested on 'snowflake' dialect\n",
      "\n",
      "### Version\n",
      "\n",
      "sqlfluff, version 0.11.1\n",
      "Python 3.8.12\n",
      "\n",
      "### Configuration\n",
      "\n",
      "[sqlfluff]\n",
      "verbose = 1\n",
      "dialect = snowflake\n",
      "templater = jinja\n",
      "exclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\n",
      "output_line_length = 121\n",
      "sql_file_exts=.sql\n",
      "\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "max_line_length = 250\n",
      "indent_unit = space\n",
      "comma_style = trailing\n",
      "allow_scalar = True\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = aliases\n",
      "\n",
      "[sqlfluff:rules:L042]\n",
      "forbid_subquery_in = both\n",
      "\n",
      "[sqlfluff:rules:L010]  # Keywords\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "extended_capitalisation_policy = lower\n",
      "\n",
      "[sqlfluff:rules:L030]  # function names\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 89 lines----------------\n",
      "Misleading path does not exist message\n",
      "It looks like if _at least one_ of the paths provided to sqlfluff do not exist, it will display an error message implying that _all_ of the supplied paths do not exist:\n",
      "\n",
      "```bash\n",
      "dbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff fix models/shared/dispensaries.sql models/shares/dispensary_chains.sql\n",
      "==== finding fixable violations ====\n",
      "=== [dbt templater] Compiling dbt project...\n",
      "== [models/shared/dispensaries.sql] FAIL\n",
      "L:   6 | P:   2 | L003 | Indentation not consistent with line #376\n",
      "L:   8 | P:   2 | L003 | Indentation not consistent with line #376\n",
      "L:   9 | P:   3 | L003 | Line over-indented compared to line #376\n",
      "L:  10 | P:   2 | L003 | Indentation not consistent with line #376\n",
      "L:  12 | P:   2 | L003 | Indentation not consistent with line #376\n",
      "L:  13 | P:   3 | L003 | Line over-indented compared to line #376\n",
      "L:  14 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  15 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  16 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  17 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  18 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  19 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  20 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  21 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  22 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  23 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  24 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  25 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  26 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  27 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  28 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  29 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  30 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  31 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  32 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  33 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  34 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  58 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L:  35 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  36 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  37 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  38 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  39 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  40 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  41 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  42 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  43 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  44 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  45 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  46 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  47 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  48 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  49 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  50 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  51 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  52 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  53 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  54 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  55 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  56 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  57 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  58 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  59 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  60 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  61 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  62 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  63 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  64 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  65 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  66 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  67 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  68 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  69 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  70 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  71 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  72 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  73 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  74 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  75 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  76 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  77 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  78 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  79 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  80 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  81 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  82 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  83 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  84 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  85 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  86 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  87 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  88 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  89 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  90 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  91 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  92 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  92 | P:  44 | L001 | Unnecessary trailing whitespace.\n",
      "L:  93 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  94 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  95 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  96 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  97 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  98 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L:  99 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 100 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 101 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 102 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 103 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 104 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 105 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 106 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 107 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 108 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 109 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 110 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 111 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 112 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 113 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 114 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 115 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 116 | P:   3 | L003 | Line over-indented compared to line #376\n",
      "L: 235 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 117 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 118 | P:   3 | L003 | Line over-indented compared to line #376\n",
      "L: 119 | P:   4 | L003 | Line over-indented compared to line #376\n",
      "L: 120 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 121 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 122 | P:   2 | L003 | Indentation not consistent with line #376\n",
      "L: 339 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 343 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 347 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 351 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 355 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 358 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 361 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 364 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 367 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "L: 370 | P:   1 | L004 | Incorrect indentation type found in file.\n",
      "The path(s) ('models/shared/dispensaries.sql', 'models/shares/dispensary_chains.sql') could not be accessed. Check it/they exist(s).\n",
      "```\n",
      "\n",
      "## Expected Behaviour\n",
      "I would expect only the unaccessible paths to be included in the error message.\n",
      "\n",
      "## Observed Behaviour\n",
      "See above\n",
      "\n",
      "## Version\n",
      "```bash\n",
      "dbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff --version\n",
      "sqlfluff, version 0.5.2\n",
      "```\n",
      "\n",
      "```bash\n",
      "dbt@b54bee9ced88:/workspaces/dbt-dutchie$ python --version\n",
      "Python 3.8.6\n",
      "```\n",
      "\n",
      "## Configuration\n",
      "```\n",
      "[sqlfluff]\n",
      "dialect = snowflake\n",
      "templater = dbt\n",
      "rules = L001,L002,L003,L004,L005,L006\n",
      "ignore = parsing,templating\n",
      "\n",
      "[sqlfluff:rules]\n",
      "max_line_length = 120\n",
      "comma_style = trailing\n",
      "\n",
      "[sqlfluff:rules:L010]\n",
      "capitalisation_policy = upper\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 45 lines----------------\n",
      "Enhance rule L036 to put all columns on separate lines if any of them are\n",
      "The current description is ambiguous, but after discussion, we decided to update the rule and keep the description at least _similar_ to what it is currently.. See discussion on #769.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n",
      "## Expected Behaviour\n",
      "\n",
      "Both of these queries should pass, the only difference is the addition of a table alias 'a':\n",
      "\n",
      "1/ no alias\n",
      "\n",
      "```\n",
      "SELECT [hello]\n",
      "FROM\n",
      "    mytable\n",
      "```\n",
      "\n",
      "2/ same query with alias\n",
      "\n",
      "```\n",
      "SELECT a.[hello]\n",
      "FROM\n",
      "    mytable AS a\n",
      "```\n",
      "\n",
      "## Observed Behaviour\n",
      "\n",
      "1/ passes\n",
      "2/ fails with: L031: Avoid using aliases in join condition.\n",
      "\n",
      "But there is no join condition :-)\n",
      "\n",
      "## Steps to Reproduce\n",
      "\n",
      "Lint queries above\n",
      "\n",
      "## Dialect\n",
      "\n",
      "TSQL\n",
      "\n",
      "## Version\n",
      "\n",
      "sqlfluff 0.6.9\n",
      "Python 3.6.9\n",
      "\n",
      "## Configuration\n",
      "\n",
      "N/A\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 152 lines----------------\n",
      "`AnySetOf` grammar\n",
      "<!--Note: This is for general enhancements to the project. Please use the Bug report template instead to raise parsing/linting/syntax issues for existing supported dialects-->\n",
      "I know this has been talked about before in PRs so making an issue to formally track.\n",
      "\n",
      "In many grammars there's a common situation where we have to denote several options that can be specified in any order but they cannot be specified more than once.\n",
      "\n",
      "Our general approach to this in the project has been denote this using `AnyNumberOf` as this allows for the different orderings:\n",
      "```python\n",
      "AnyNumberOf(\n",
      "    <option_1_grammar>,\n",
      "    <option_2_grammar>,\n",
      "    ...\n",
      ")\n",
      "```\n",
      "However, the issue with this is that it places no limit on how many times each option can be specified.\n",
      "\n",
      "This means that sqlfluff allows certain invalid statements to parse e.g.\n",
      "```sql\n",
      "CREATE TABLE ktw_account_binding (\n",
      "    ktw_id VARCHAR(32) NOT NULL REFERENCES ref_table(bla)\n",
      "    ON DELETE RESTRICT ON DELETE CASCADE ON DELETE CASCADE ON DELETE CASCADE\n",
      ");\n",
      "```\n",
      "https://github.com/sqlfluff/sqlfluff/pull/2315#issuecomment-1013847846\n",
      "\n",
      "We've accepted this limitation for the time being as it's more important to get the statements parsing for linting/formatting purposes rather than exactly reflecting the grammar (we'd expect a general degree of common sense when it comes to repeating these options).\n",
      "\n",
      "That being said it would be nice to address this to refine our grammar and reduce dealing with contributor confusion.\n",
      "\n",
      "`AnySetOf` would essentially allow all of it's grammar arguments to be parsed in any order a maximum of 1 time each. Hopefully we can inherit from `AnyNumberOf` to simplify this.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 21 lines----------------\n",
      "Config for fix_even_unparsable not being applied\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "When setting the any config file to `fix_even_unparsable = True` the config get's overriden by the default (or lack thereof) on the @click.option decorator for the fix command.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "When setting the config `fix_even_unparsable` it should be captured by the fix command as well.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "The `fix_even_unparsable` command is not being captured by the fix command\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Create a config file and include `fix_even_unparsable`\n",
      "Run `sqlfluff fix`\n",
      "Note that `fix_even_unparsable` is set to False at runtime\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Any\n",
      "\n",
      "### Version\n",
      "\n",
      "0.13.0\n",
      "\n",
      "### Configuration\n",
      "\n",
      "`pyproject.toml`\n",
      "\n",
      "```\n",
      "[tool.sqlfluff.core]\n",
      "verbose = 2\n",
      "dialect = \"snowflake\"\n",
      "fix_even_unparsable = true\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 14 lines----------------\n",
      "Rule L060 could give a specific error message\n",
      "At the moment rule L060 flags something like this:\n",
      "\n",
      "```\n",
      "L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\n",
      "```\n",
      "\n",
      "Since we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\n",
      "\n",
      "That is it should flag this:\n",
      "\n",
      "```\n",
      "L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\n",
      "```\n",
      " Or this:\n",
      "\n",
      "```\n",
      "L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\n",
      "```\n",
      "\n",
      "As appropriate.\n",
      "\n",
      "What do you think @jpy-git ?\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Commented dash character converted to non utf-8 character\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Upon fixing a query containing a multi-line comment, SQLFluff attempts to fix a commented line.\n",
      "\n",
      "This:\n",
      "```sql\n",
      "/*\n",
      "TODO\n",
      " - tariff scenario â€”> dm_tariff_scenario\n",
      "*/\n",
      "```\n",
      "\n",
      "Became:\n",
      "```sql\n",
      "/*\n",
      "TODO\n",
      " - tariff scenario Â—> dm_tariff_scenario\n",
      "*/\n",
      "``` \n",
      "This in an invisible char represented as `<97>`\n",
      "\n",
      "This causes an issue with dbt which can not compile with this char present\n",
      "\n",
      "Note this comment comes at the end of the file.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Does not replace/fix anything that is commented\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "```bash\n",
      " $  sqlfluff fix dbt/models/marts/core/f_utility_statements.sql                                                                                                                                                                                               \n",
      "==== finding fixable violations ====                                                                                                                                                                                                                          \n",
      "=== [dbt templater] Sorting Nodes...                                                                                                                                                                                                                          \n",
      "=== [dbt templater] Compiling dbt project...                                                                                                                                                                                                                  \n",
      "=== [dbt templater] Project Compiled.                                                                                                                                                                                                                         \n",
      "== [dbt/models/marts/core/f_utility_statements.sql] FAIL                                                                                                                                                                                                      \n",
      "L:   1 | P:   5 | L001 | Unnecessary trailing whitespace.                                                                                                                                                                                                     \n",
      "L:   2 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:   3 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \n",
      "L:   4 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:   4 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \n",
      "L:   6 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:   7 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \n",
      "L:   8 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:   8 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \n",
      "L:  10 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:  11 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \n",
      "L:  12 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \n",
      "L:  12 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \n",
      "L:  15 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]   \n",
      "L:  16 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                      [0/47960]\n",
      "L:  17 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  18 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  19 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  20 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  20 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  21 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  21 | P:  32 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  22 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\n",
      "L:  22 | P:   6 | L019 | Found trailing comma. Expected only leading.\n",
      "L:  24 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\n",
      "L:  26 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  26 | P:  15 | L001 | Unnecessary trailing whitespace.\n",
      "L:  27 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  28 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  29 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  30 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  31 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  32 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  32 | P:  24 | L011 | Implicit/explicit aliasing of table.\n",
      "L:  32 | P:  24 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  33 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  33 | P:  49 | L011 | Implicit/explicit aliasing of table.\n",
      "L:  33 | P:  49 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  33 | P:  52 | L001 | Unnecessary trailing whitespace.\n",
      "L:  34 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  36 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  37 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\n",
      "L:  37 | P:   6 | L019 | Found trailing comma. Expected only leading.\n",
      "L:  39 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\n",
      "L:  41 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  41 | P:   9 | L034 | Select wildcards then simple targets before calculations\n",
      "                       | and aggregates.\n",
      "L:  43 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  46 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  47 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  48 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  51 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  52 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  53 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  54 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  57 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  58 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  61 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  62 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  64 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  65 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  68 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  69 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  70 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  71 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  73 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  73 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  74 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  74 | P:  56 | L031 | Avoid aliases in from clauses and join conditions.\n",
      "L:  75 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  76 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  76 | P:  28 | L001 | Unnecessary trailing whitespace.\n",
      "L:  77 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  80 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\n",
      "L:  81 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  83 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\n",
      "L:  84 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\n",
      "L:  94 | P:   1 | L009 | Files must end with a single trailing newline.\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "`sqlfluff fix` with provided `.sqlfluff` configuration\n",
      "\n",
      "SQL contains proprietary code and I am, likely, unable to provide a full snippet of the SQL \n",
      "\n",
      "### Dialect\n",
      "\n",
      "Snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "0.13.0 and 0.11.1\n",
      "\n",
      "### Configuration\n",
      "\n",
      "`.sqlfluff`:\n",
      "```\n",
      "[sqlfluff]\n",
      "templater = dbt\n",
      "dialect = snowflake\n",
      "\n",
      "[sqlfluff:templater:dbt]\n",
      "project_dir = dbt/\n",
      "\n",
      "# Defaults on anything not specified explicitly: https://docs.sqlfluff.com/en/stable/configuration.html#default-configuration\n",
      "[sqlfluff:rules]\n",
      "max_line_length = 120\n",
      "comma_style = leading\n",
      "\n",
      "# Keyword capitalisation\n",
      "[sqlfluff:rules:L010]\n",
      "capitalisation_policy = lower\n",
      "\n",
      "# TODO: this supports pascal but not snake\n",
      "# TODO: this inherits throwing violation on all unquoted identifiers... we can limit to aliases or column aliases\n",
      "# [sqlfluff:rules:L014]\n",
      "# extended_capitalisation_policy = pascal\n",
      "\n",
      "# TODO: not 100% certain that this default is correct\n",
      "# [sqlfluff:rules:L029]\n",
      "## Keywords should not be used as identifiers.\n",
      "# unquoted_identifiers_policy = aliases\n",
      "# quoted_identifiers_policy = none\n",
      "## Comma separated list of words to ignore for this rule\n",
      "# ignore_words = None\n",
      "\n",
      "# Function name capitalisation\n",
      "[sqlfluff:rules:L030]\n",
      "extended_capitalisation_policy = lower\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 103 lines----------------\n",
      "ValueError: Position Not Found for lint/parse/fix, not clear why\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "I have admittedly messy dbt sql model that gets the following error when I try to lint, parse or fix it with sqlfluff - every other model can be processed using the same settings, but this one throws the same error below even if I only run a single rule e.g. L009.\n",
      "\n",
      "Unfortunately I cannot share the model itself but I can describe some notable features:\n",
      "- begins with a dbt incremental config\n",
      "- then sets three variables, each a list of strings\n",
      "- Has two `for` loops with nested `if` conditions\n",
      "- Has one very long line doing arithmetic operations involving both hardcoded values and columns from a two joined CTEs\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Not the above error\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "```\n",
      "WARNING    Unable to lint models/ltv_prediction_model/ltv_prediction.sql due to an internal error. Please report this as an issue w\n",
      "ith your query's contents and stacktrace below!\n",
      "To hide this warning, add the failing file to .sqlfluffignore\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\n",
      "    yield partial()\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\n",
      "    parsed = cls.parse_rendered(rendered)\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendere\n",
      "\n",
      "d\n",
      "    tokens, lvs, config = cls._lex_templated_file(\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_template\n",
      "d_file\n",
      "    tokens, lex_vs = lexer.lex(templated_file)\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\n",
      "    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_se\n",
      "gments\n",
      "    source_slice = templated_file.templated_slice_to_source_slice(\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 294, in templated_s\n",
      "lice_to_source_slice\n",
      "    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\n",
      "  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 180, in _find_slice\n",
      "_indices_of_templated_pos\n",
      "    raise ValueError(\"Position Not Found\")\n",
      "ValueError: Position Not Found\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "```\n",
      "{{\n",
      "    config(\n",
      "        materialized='incremental',\n",
      "        unique_key='md5_surrogate_key_main'\n",
      "    )\n",
      "}}\n",
      "\n",
      "{%- set first_list = [\"value1\", \"value2\", \"value3\"] -%}\n",
      "{%- set second_list = [\"value4\", \"value5\", \"value6\"] -%}\n",
      "{%- set third_list = [\"value7\", \"value8\", \"value9\"] -%}\n",
      "\n",
      "with fill_na_values as (\n",
      "    select\n",
      "        id,\n",
      "        run_date,\n",
      "        md5_surrogate_key_main,\n",
      "        {%- for features in second_list %}\n",
      "            {%- if features in third_list %}\n",
      "                coalesce({{features}}, (select feature_mode from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\n",
      "                {%- if not loop.last -%},{% endif %}\n",
      "            {%- else -%}\n",
      "                coalesce({{features}}, (select feature_mean from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\n",
      "                {%- if not loop.last -%},{% endif %}\n",
      "            {%- endif -%}\n",
      "        {%- endfor %}\n",
      "    from {{ ref('training_dataset') }}\n",
      "    {%- if is_incremental() %}\n",
      "    where current_date >= (select max(run_date) from {{ this }})\n",
      "    {%- else %}\n",
      "    where run_date >= '2021-01-01'\n",
      "    {%- endif %}\n",
      "),\n",
      "\n",
      "winsorize_data as (\n",
      "    select\n",
      "        md5_surrogate_key_main,\n",
      "        {%- for features in second_list %}\n",
      "            {%- if features in first_list %}\n",
      "                case\n",
      "                    when {{features}} < (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\n",
      "                    then (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\n",
      "                    when {{features}} > (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\n",
      "                    then (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\n",
      "                    else {{features}}\n",
      "                end as {{features}}\n",
      "                {%- if not loop.last -%},{% endif %}\n",
      "            {%- else %}\n",
      "                {{features}}\n",
      "                {%- if not loop.last -%},{% endif %}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "    from fill_na_values\n",
      "),\n",
      "\n",
      "scaling_data as (\n",
      "    select\n",
      "        md5_surrogate_key_main,\n",
      "        {%- for features in second_list %}\n",
      "            ({{features}} - (select feature_mean from {{ ref('second_list') }} where features = '{{features}}'))/(select feature_std from {{ ref('second_list') }} where features = '{{features}}') as {{features}}\n",
      "            {%- if not loop.last -%},{% endif %}\n",
      "        {%- endfor %}\n",
      "    from winsorize_data\n",
      "),\n",
      "\n",
      "apply_ceofficients as (\n",
      "    select\n",
      "        md5_surrogate_key_main,\n",
      "        {%- for features in second_list %}\n",
      "            {{features}} * (select coefficients from {{ ref('second_list') }} where features = '{{features}}') as {{features}}_coef\n",
      "            {%- if not loop.last -%},{% endif %}\n",
      "        {%- endfor %}\n",
      "    from scaling_data\n",
      "),\n",
      "\n",
      "logistic_prediction as (\n",
      "    select\n",
      "        fan.*,\n",
      "        1/(1+EXP(-(0.24602303+coef1+coef2+coef3+coef4+coef5+coef6+coef7+coef8+coef9+available_balance_coef+coef10+coef11+coef12+coef13+coef14))) as prediction_probability,\n",
      "        case when prediction_probability < .5 then 0 else 1 end as prediction_class\n",
      "    from apply_ceofficients ac\n",
      "    inner join fill_na_values fan\n",
      "        on ac.md5_surrogate_key_main = fan.md5_surrogate_key_main\n",
      ")\n",
      "\n",
      "select * from logistic_prediction\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "0.10.1\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "# verbose is an integer (0-2) indicating the level of log output\n",
      "verbose = 0\n",
      "# Turn off color formatting of output\n",
      "nocolor = False\n",
      "dialect = snowflake\n",
      "templater = jinja\n",
      "# Comma separated list of rules to check, or None for all\n",
      "rules = L001,L002,L003,L004,L005,L009,L010,L013,L014,L015,L017,L018,L019,L020,L021,L022,L023,L024,L026,L027,L028,L030,L036,L037,L038,L039,L040,L044,L045,L046,L050,L051,L058,L061\n",
      "# Comma separated list of rules to exclude, or None\n",
      "exclude_rules = L006,L008,L011,L012,L025,L029,L031,L034,L035,L041,L042,L043,L052\n",
      "# The depth to recursively parse to (0 for unlimited)\n",
      "recurse = 0\n",
      "# Below controls SQLFluff output, see max_line_length for SQL output\n",
      "output_line_length = 80\n",
      "# Number of passes to run before admitting defeat\n",
      "runaway_limit = 10\n",
      "# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\n",
      "ignore = None\n",
      "# Ignore linting errors found within sections of code coming directly from\n",
      "# templated code (e.g. from within Jinja curly braces. Note that it does not\n",
      "# ignore errors from literal code found within template loops.\n",
      "ignore_templated_areas = True\n",
      "# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\n",
      "encoding = autodetect\n",
      "# Ignore inline overrides (e.g. to test if still required)\n",
      "disable_noqa = False\n",
      "# Comma separated list of file extensions to lint\n",
      "# NB: This config will only apply in the root folder\n",
      "sql_file_exts = .sql,.sql.j2,.dml,.ddl\n",
      "# Allow fix to run on files, even if they contain parsing errors\n",
      "# Note altering this is NOT RECOMMENDED as can corrupt SQL\n",
      "fix_even_unparsable = False\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "# See https://docs.sqlfluff.com/en/stable/indentation.html\n",
      "indented_joins = False\n",
      "indented_ctes = False\n",
      "indented_using_on = True\n",
      "template_blocks_indent = True\n",
      "\n",
      "[sqlfluff:templater]\n",
      "unwrap_wrapped_queries = True\n",
      "\n",
      "[sqlfluff:templater:jinja]\n",
      "apply_dbt_builtins = True\n",
      "\n",
      "[sqlfluff:templater:jinja:macros]\n",
      "# Macros provided as builtins for dbt projects\n",
      "dbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\n",
      "dbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\n",
      "dbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\n",
      "dbt_var = {% macro var(variable, default='') %}item{% endmacro %}\n",
      "dbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\n",
      "\n",
      "# Some rules can be configured directly from the config common to other rules\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "max_line_length = 80\n",
      "indent_unit = space\n",
      "comma_style = trailing\n",
      "allow_scalar = True\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "# Some rules have their own specific config\n",
      "[sqlfluff:rules:L007]\n",
      "operator_new_lines = after\n",
      "\n",
      "[sqlfluff:rules:L010]\n",
      "# Keywords\n",
      "capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:L011]\n",
      "# Aliasing preference for tables\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L012]\n",
      "# Aliasing preference for columns\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "# Unquoted identifiers\n",
      "extended_capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:L016]\n",
      "# Line length\n",
      "ignore_comment_lines = False\n",
      "ignore_comment_clauses = False\n",
      "\n",
      "[sqlfluff:rules:L026]\n",
      "# References must be in FROM clause\n",
      "# Disabled for some dialects (e.g. bigquery)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L028]\n",
      "# References must be consistently used\n",
      "# Disabled for some dialects (e.g. bigquery)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L029]\n",
      "# Keywords should not be used as identifiers.\n",
      "unquoted_identifiers_policy = aliases\n",
      "quoted_identifiers_policy = none\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:L030]\n",
      "# Function names\n",
      "capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:L038]\n",
      "# Trailing commas\n",
      "select_clause_trailing_comma = forbid\n",
      "\n",
      "[sqlfluff:rules:L040]\n",
      "# Null & Boolean Literals\n",
      "capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:L042]\n",
      "# By default, allow subqueries in from clauses, but not join clauses\n",
      "forbid_subquery_in = join\n",
      "\n",
      "[sqlfluff:rules:L047]\n",
      "# Consistent syntax to count all rows\n",
      "prefer_count_1 = False\n",
      "prefer_count_0 = False\n",
      "\n",
      "[sqlfluff:rules:L052]\n",
      "# Semi-colon formatting approach\n",
      "multiline_newline = False\n",
      "require_final_semicolon = False\n",
      "\n",
      "[sqlfluff:rules:L054]\n",
      "# GROUP BY/ORDER BY column references\n",
      "group_by_and_order_by_style = consistent\n",
      "\n",
      "[sqlfluff:rules:L057]\n",
      "# Special characters in identifiers\n",
      "unquoted_identifiers_policy = all\n",
      "quoted_identifiers_policy = all\n",
      "allow_space_in_identifier = False\n",
      "additional_allowed_characters = \"\"\n",
      "\n",
      "[sqlfluff:rules:L059]\n",
      "# Policy on quoted and unquoted identifiers\n",
      "prefer_quoted_identifiers = False\n",
      "\n",
      "[sqlfluff:rules:L062]\n",
      "# Comma separated list of blocked words that should not be used\n",
      "blocked_words = None\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 171 lines----------------\n",
      "Number of processes configurable in .sqlfluff\n",
      "Being able to set the number of processes to run with in .sqlfluff might be useful to avoid having to pass it in the CLI every time.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 151 lines----------------\n",
      "Validate layout configurations on load\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "As raised in this comment: https://github.com/sqlfluff/sqlfluff/pull/4558#discussion_r1142745101\n",
      "\n",
      "At the moment, the layout configs are being validated _on use_ which is potentially flaky and convoluted. Better would be to validate configs _on load_.\n",
      "\n",
      "### Use case\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Dialect\n",
      "\n",
      "all\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 26 lines----------------\n",
      "BigQuery: Accessing `STRUCT` elements evades triggering L027\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Accessing unreferenced `STRUCT` elements using BigQuery dot notation in a multi table query does not trigger L027.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "L027 gets triggered.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "L027 does not get triggered.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    t1.col1,\n",
      "    t2.col2,\n",
      "    events.id\n",
      "FROM t_table1 AS t1\n",
      "LEFT JOIN t_table2 AS t2\n",
      "    ON TRUE\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "BigQUery\n",
      "\n",
      "### Version\n",
      "\n",
      "`0.11.2` using online.sqlfluff.com\n",
      "\n",
      "### Configuration\n",
      "\n",
      "N/A\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 15 lines----------------\n",
      "Extra space when first field moved to new line in a WITH statement\n",
      "Note, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\n",
      "\n",
      "Given the following SQL:\n",
      "\n",
      "```sql\n",
      "WITH example AS (\n",
      "    SELECT my_id,\n",
      "        other_thing,\n",
      "        one_more\n",
      "    FROM\n",
      "        my_table\n",
      ")\n",
      "\n",
      "SELECT *\n",
      "FROM example\n",
      "```\n",
      "\n",
      "## Expected Behaviour\n",
      "\n",
      "after running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\n",
      "\n",
      "```sql\n",
      "WITH example AS (\n",
      "    SELECT\n",
      "        my_id,\n",
      "        other_thing,\n",
      "        one_more\n",
      "    FROM\n",
      "        my_table\n",
      ")\n",
      "\n",
      "SELECT *\n",
      "FROM example\n",
      "```\n",
      "\n",
      "## Observed Behaviour\n",
      "\n",
      "after running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\n",
      "\n",
      "```sql\n",
      "WITH example AS (\n",
      "    SELECT\n",
      "         my_id,\n",
      "        other_thing,\n",
      "        one_more\n",
      "    FROM\n",
      "        my_table\n",
      ")\n",
      "\n",
      "SELECT *\n",
      "FROM example\n",
      "```\n",
      "\n",
      "## Steps to Reproduce\n",
      "\n",
      "Noted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\n",
      "\n",
      "## Dialect\n",
      "\n",
      "Running with default config.\n",
      "\n",
      "## Version\n",
      "Include the output of `sqlfluff --version` along with your Python version\n",
      "\n",
      "sqlfluff, version 0.7.0\n",
      "Python 3.7.5\n",
      "\n",
      "## Configuration\n",
      "\n",
      "Default config.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 155 lines----------------\n",
      "dbt & JinjaTracer results in passing invalid query to database (was: DBT Call statement() block causes invalid query generated)\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "When using the call statement() to run a query during compile time, the query generated is garbled causing the following sql error:\n",
      "```\n",
      "{% call statement('variables', fetch_result=true) %}\n",
      "\n",
      "select 1 as test;\n",
      "\n",
      "{% endcall %}\n",
      "\n",
      "{% set test = load_result('variables')['table'].columns.TEST.values()[0] %}\n",
      "```\n",
      "\n",
      "This results in the following error:\n",
      "\n",
      "dbt.exceptions.DatabaseException: Database Error\n",
      "  001003 (42000): SQL compilation error:\n",
      "  syntax error line 1 at position 0 unexpected '0'.\n",
      "\n",
      "The query ran looks like this when looking at the query runner history in snowflake:\n",
      "\n",
      "```\n",
      "â˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜º_0â˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜º_8â˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜ºâ˜º_0\n",
      "```\n",
      "\n",
      "Whereas it should show:\n",
      "```\n",
      "select 1 as test;\n",
      "```\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "Expected that the query runs properly.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "```\n",
      "=== [dbt templater] Compiling dbt project...\n",
      "=== [dbt templater] Project Compiled.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 219, in exception_handler\n",
      "    yield\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 70, in add_query\n",
      "    cursor.execute(sql, bindings)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/cursor.py\", line 794, in execute\n",
      "    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 273, in errorhandler_wrapper\n",
      "    handed_over = Error.hand_to_other_handler(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 328, in hand_to_other_handler\n",
      "    cursor.errorhandler(connection, cursor, error_class, error_value)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 207, in default_errorhandler\n",
      "    raise error_class(\n",
      "snowflake.connector.errors.ProgrammingError: 001003 (42000): SQL compilation error:\n",
      "syntax error line 1 at position 0 unexpected '0'.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/sqlfluff\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/cli/commands.py\", line 1008, in parse\n",
      "    parsed_strings = list(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 1171, in parse_path\n",
      "    yield self.parse_string(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 835, in parse_string\n",
      "    rendered = self.render_string(in_str, fname, config, encoding)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 784, in render_string\n",
      "    templated_file, templater_violations = self.templater.process(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 47, in _wrapped\n",
      "    return func(self, in_str=in_str, fname=fname, config=config, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 331, in process\n",
      "    processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 552, in _unsafe_process\n",
      "    raw_sliced, sliced_file, templated_sql = self.slice_file(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/jinja.py\", line 462, in slice_file\n",
      "    trace = tracer.trace(append_to_templated=kwargs.pop(\"append_to_templated\", \"\"))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/slicers/tracer.py\", line 77, in trace\n",
      "    trace_template_output = trace_template.render()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 1090, in render\n",
      "    self.environment.handle_exception()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception\n",
      "    reraise(*rewrite_traceback_stack(source=source))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"<template>\", line 16, in top-level template code\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\n",
      "    return __context.call(__obj, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 321, in __call__\n",
      "    return self.call_macro(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 248, in call_macro\n",
      "    return macro(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/runtime.py\", line 679, in _invoke\n",
      "    rv = self._func(*arguments)\n",
      "  File \"<template>\", line 10, in template\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\n",
      "    return __context.call(__obj, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/base/impl.py\", line 235, in execute\n",
      "    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 122, in execute\n",
      "    _, cursor = self.add_query(sql, auto_begin)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 458, in add_query\n",
      "    connection, cursor = super().add_query(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 78, in add_query\n",
      "    return connection, cursor\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 238, in exception_handler\n",
      "    raise DatabaseException(msg)\n",
      "dbt.exceptions.DatabaseException: Database Error\n",
      "  001003 (42000): SQL compilation error:\n",
      "  syntax error line 1 at position 0 unexpected '0'.\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Use the statement() block described here:\n",
      "https://docs.getdbt.com/reference/dbt-jinja-functions/statement-blocks\n",
      "\n",
      "\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Snowflake\n",
      "\n",
      "### Version\n",
      "\n",
      "1.2.0 with the dbt formatter\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "# verbose is an integer (0-2) indicating the level of log output\n",
      "verbose = 2\n",
      "# Turn off color formatting of output\n",
      "nocolor = False\n",
      "# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\n",
      "# Or run 'sqlfluff dialects'\n",
      "dialect = snowflake\n",
      "# One of [raw|jinja|python|placeholder]\n",
      "templater = dbt\n",
      "# Comma separated list of rules to check, default to all\n",
      "rules = all\n",
      "# Comma separated list of rules to exclude, or None\n",
      "exclude_rules = None\n",
      "# The depth to recursively parse to (0 for unlimited)\n",
      "recurse = 0\n",
      "# Below controls SQLFluff output, see max_line_length for SQL output\n",
      "output_line_length = 80\n",
      "# Number of passes to run before admitting defeat\n",
      "runaway_limit = 10\n",
      "# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\n",
      "ignore = None\n",
      "# Ignore linting errors found within sections of code coming directly from\n",
      "# templated code (e.g. from within Jinja curly braces. Note that it does not\n",
      "# ignore errors from literal code found within template loops.\n",
      "ignore_templated_areas = True\n",
      "# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\n",
      "encoding = autodetect\n",
      "# Ignore inline overrides (e.g. to test if still required)\n",
      "disable_noqa = False\n",
      "# Comma separated list of file extensions to lint\n",
      "# NB: This config will only apply in the root folder\n",
      "sql_file_exts = .sql,.sql.j2,.dml,.ddl\n",
      "# Allow fix to run on files, even if they contain parsing errors\n",
      "# Note altering this is NOT RECOMMENDED as can corrupt SQL\n",
      "fix_even_unparsable = False\n",
      "# Very large files can make the parser effectively hang.\n",
      "# This limit skips files over a certain character length\n",
      "# and warns the user what has happened.\n",
      "# Set this to 0 to disable.\n",
      "large_file_skip_char_limit = 20000\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "# See https://docs.sqlfluff.com/en/stable/indentation.html\n",
      "indented_joins = False\n",
      "indented_ctes = False\n",
      "indented_using_on = True\n",
      "indented_on_contents = True\n",
      "template_blocks_indent = True\n",
      "\n",
      "[sqlfluff:templater]\n",
      "unwrap_wrapped_queries = True\n",
      "\n",
      "[sqlfluff:templater:jinja]\n",
      "apply_dbt_builtins = True\n",
      "load_macros_from_path = macros/\n",
      "\n",
      "[sqlfluff:templater:jinja:macros]\n",
      "# Macros provided as builtins for dbt projects\n",
      "dbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\n",
      "dbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\n",
      "dbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\n",
      "dbt_var = {% macro var(variable, default='') %}item{% endmacro %}\n",
      "dbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\n",
      "\n",
      "[sqlfluff:templater:dbt]\n",
      "project_dir = ./\n",
      "\n",
      "# Some rules can be configured directly from the config common to other rules\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "max_line_length = 120\n",
      "indent_unit = space\n",
      "comma_style = trailing\n",
      "allow_scalar = True\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "# Some rules have their own specific config\n",
      "[sqlfluff:rules:L003]\n",
      "hanging_indents = True\n",
      "\n",
      "[sqlfluff:rules:L007]\n",
      "operator_new_lines = after\n",
      "\n",
      "[sqlfluff:rules:L010]\n",
      "# Keywords\n",
      "capitalisation_policy = lower\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L011]\n",
      "# Aliasing preference for tables\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L012]\n",
      "# Aliasing preference for columns\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:L014]\n",
      "# Unquoted identifiers\n",
      "extended_capitalisation_policy = lower\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L016]\n",
      "# Line length\n",
      "ignore_comment_lines = False\n",
      "ignore_comment_clauses = False\n",
      "\n",
      "[sqlfluff:rules:L027]\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L026]\n",
      "# References must be in FROM clause\n",
      "# Disabled for some dialects (e.g. bigquery)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L028]\n",
      "# References must be consistently used\n",
      "# Disabled for some dialects (e.g. bigquery)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L029]\n",
      "# Keywords should not be used as identifiers.\n",
      "unquoted_identifiers_policy = aliases\n",
      "quoted_identifiers_policy = none\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L030]\n",
      "# Function names\n",
      "extended_capitalisation_policy = lower\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L031]\n",
      "# Avoid table aliases in from clauses and join conditions.\n",
      "# Disabled for some dialects (e.g. bigquery)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L036]\n",
      "wildcard_policy = single\n",
      "\n",
      "[sqlfluff:rules:L038]\n",
      "# Trailing commas\n",
      "select_clause_trailing_comma = forbid\n",
      "\n",
      "[sqlfluff:rules:L040]\n",
      "# Null & Boolean Literals\n",
      "capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L042]\n",
      "# By default, allow subqueries in from clauses, but not join clauses\n",
      "forbid_subquery_in = join\n",
      "\n",
      "[sqlfluff:rules:L047]\n",
      "# Consistent syntax to count all rows\n",
      "prefer_count_1 = False\n",
      "prefer_count_0 = False\n",
      "\n",
      "[sqlfluff:rules:L051]\n",
      "# Fully qualify JOIN clause\n",
      "fully_qualify_join_types = inner\n",
      "\n",
      "[sqlfluff:rules:L052]\n",
      "# Semi-colon formatting approach\n",
      "multiline_newline = False\n",
      "require_final_semicolon = False\n",
      "\n",
      "[sqlfluff:rules:L054]\n",
      "# GROUP BY/ORDER BY column references\n",
      "group_by_and_order_by_style = consistent\n",
      "\n",
      "[sqlfluff:rules:L057]\n",
      "# Special characters in identifiers\n",
      "unquoted_identifiers_policy = all\n",
      "quoted_identifiers_policy = all\n",
      "allow_space_in_identifier = False\n",
      "additional_allowed_characters = \"\"\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L059]\n",
      "# Policy on quoted and unquoted identifiers\n",
      "prefer_quoted_identifiers = False\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L062]\n",
      "# Comma separated list of blocked words that should not be used\n",
      "blocked_words = None\n",
      "blocked_regex = None\n",
      "\n",
      "[sqlfluff:rules:L063]\n",
      "# Data Types\n",
      "extended_capitalisation_policy = consistent\n",
      "# Comma separated list of words to ignore for this rule\n",
      "ignore_words = None\n",
      "ignore_words_regex = None\n",
      "\n",
      "[sqlfluff:rules:L064]\n",
      "# Consistent usage of preferred quotes for quoted literals\n",
      "preferred_quoted_literal_style = consistent\n",
      "# Disabled for dialects that do not support single and double quotes for quoted literals (e.g. Postgres)\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L066]\n",
      "min_alias_length = None\n",
      "max_alias_length = None\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 103 lines----------------\n",
      "add ability to render the compiled sql\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "It would be nice to see the compiled sql in which any templates are rendered. I would be happy to work on this but it may be a struggle and would need some guidance.\n",
      "\n",
      "### Use case\n",
      "\n",
      " It would help debug linting errors around jinja templates.\n",
      "It would also make it easier to copy and use the query in the bigquery ui, for example. We process our queries through Airflow so currently I can start a dag run and look at the rendered template to get this effect. That's not very efficient though :)\n",
      "\n",
      "\n",
      "### Dialect\n",
      "\n",
      "We use bigquery but this could apply to all dialects.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 157 lines----------------\n",
      "noqa is ignored for jinja templated lines\n",
      "## Expected Behaviour\n",
      "Line with `noqa: TMP` should be ignored (despite of evaluation error)\n",
      "\n",
      "## Observed Behaviour\n",
      "trying to lint airflow sql-template for AWS Athena query\n",
      "setting up inline `-- noqa` or `--noqa: TMP` for jinja templated line not silenting templating error (typecasting error due to unable to pass datetime object while linting into template context):\n",
      "```\n",
      "== [transform/airflow/dags/queries/sfmc/player_balance.sql] FAIL\n",
      "L:   0 | P:   0 |  TMP | Unrecoverable failure in Jinja templating: unsupported operand type(s) for -: 'int' and 'datetime.timedelta'. Have you configured your variables?\n",
      "                       | https://docs.sqlfluff.com/en/latest/configuration.html\n",
      "```\n",
      "\n",
      "## Steps to Reproduce\n",
      "templated file:\n",
      "```sql\n",
      "select *, row_number() over (partition by player_id order by balance_change_date desc)  as rnk\n",
      "from raw\n",
      "where\n",
      "    balance_change_date >= cast(from_iso8601_timestamp('{{ execution_date - macros.timedelta(hours=2, minutes=10) }}') as timestamp)  and  --noqa: TMP\n",
      "    balance_change_date < cast(from_iso8601_timestamp('{{ next_execution_date - macros.timedelta(minutes=10) }}') as timestamp) --noqa: TMP\n",
      "```\n",
      "run:\n",
      "```bash\n",
      "sqlfluff lint transform/airflow/dags/queries/sfmc/player_balance.sql\n",
      "```\n",
      "\n",
      "## Dialect\n",
      "postgres (used for AWS Athena)\n",
      "\n",
      "## Version\n",
      "datalake % sqlfluff --version\n",
      "sqlfluff, version 0.8.1\n",
      "datalake % python3 --version\n",
      "Python 3.9.8\n",
      "\n",
      "## Configuration\n",
      "```ini\n",
      "# tox.ini\n",
      "[sqlfluff]\n",
      "templater = jinja\n",
      "output_line_length = 180\n",
      "exclude_rules = L011,L012,L022,L031,L034\n",
      "dialect = postgres\n",
      "\n",
      "[sqlfluff:rules]\n",
      "max_line_length = 120\n",
      "\n",
      "[sqlfluff:templater:jinja]\n",
      "library_path = operation/deploy/lint\n",
      "apply_dbt_builtins = false\n",
      "\n",
      "[sqlfluff:templater:jinja:context]\n",
      "ds = 2021-11-11\n",
      "ds_nodash = 20211111\n",
      "start_date = 2021-11-11\n",
      "end_date = 2021-11-11\n",
      "interval = 1\n",
      "# passed as int due to inabliity to pass datetime obkject \n",
      "data_interval_start = 1636588800\n",
      "data_interval_end = 1636588800\n",
      "```\n",
      "\n",
      "```python\n",
      "# operation/deploy/lint/macro.py\n",
      "from datetime import datetime, timedelta  # noqa: F401\n",
      "\n",
      "import dateutil  # noqa: F401\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 131 lines----------------\n",
      "Exception thrown when SELECT DISTINCT not on the same line\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Check a file containing this request:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "It fails this way:\n",
      "\n",
      "```log\n",
      "CRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \n",
      "Traceback (most recent call last):\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\n",
      "    res = self._eval(context=context)\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\n",
      "    self._analyze_table_references(\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\n",
      "    if not self._should_ignore_reference(r, selectable):\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\n",
      "    ref_path = selectable.selectable.path_to(reference)\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\n",
      "    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\n",
      "    assert self.pos_marker\n",
      "AssertionError\n",
      "== [file.sql] FAIL\n",
      "L:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\n",
      "                       | only one select target. [layout.select_targets]\n",
      "L:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\n",
      "                       | line as 'SELECT'. [layout.select_modifiers]\n",
      "L:   1 | P:   1 | RF01 | Unexpected exception: ;\n",
      "Could you open an issue at\n",
      "                       | https://github.com/sqlfluff/sqlfluff/issues ?\n",
      "You can\n",
      "                       | ignore this exception for now, by adding '-- noqa: RF01'\n",
      "                       | at the end\n",
      "of line 1\n",
      " [references.from]\n",
      "L:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\n",
      "L:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\n",
      "                       | [layout.end_of_file]\n",
      "All Finished!\n",
      "```\n",
      "\n",
      "Checking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\n",
      "\n",
      "```sql\n",
      "SELECT DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "Additionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\n",
      "```sql\n",
      "SELECT \n",
      "DISTINCT\n",
      "`FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "Then the website crashes.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I would expect not to have an exception.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "An exception was thrown whereas, I think, there is no reason to throw it.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Check the following SQL:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "MySQL\n",
      "\n",
      "### Version\n",
      "\n",
      "2.3.2\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\n",
      "dialect = mysql\n",
      "encoding = utf-8\n",
      "# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\n",
      "# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\n",
      "exclude_rules = LT01, ST05\n",
      "ignore = parsing\n",
      "max_line_length = 120\n",
      "# Below controls SQLFluff output, see max_line_length for SQL output\n",
      "output_line_length = 80\n",
      "templater = raw\n",
      "verbose = 0\n",
      "\n",
      "[sqlfluff:layout:type:binary_operator]\n",
      "line_position = leading\n",
      "\n",
      "[sqlfluff:layout:type:comma]\n",
      "line_position = trailing\n",
      "spacing_before = touch\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "# See https://docs.sqlfluff.com/en/stable/indentation.html\n",
      "indent_unit = space\n",
      "indented_joins = True\n",
      "indented_using_on = True\n",
      "tab_space_size = 4\n",
      "\n",
      "# Some rules can be configured directly from the config common to other rules\n",
      "[sqlfluff:rules]\n",
      "allow_scalar = True\n",
      "quoted_identifiers_policy = none\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "[sqlfluff:rules:aliasing.column]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:aliasing.table]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:ambiguous.column_references]\n",
      "group_by_and_order_by_style = consistent\n",
      "\n",
      "[sqlfluff:rules:capitalisation.functions]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.identifiers]\n",
      "extended_capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.keywords]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.literals]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.types]\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:convention.count_rows]\n",
      "prefer_count_0 = False\n",
      "prefer_count_1 = True\n",
      "\n",
      "[sqlfluff:rules:convention.select_trailing_comma]\n",
      "select_clause_trailing_comma = forbid\n",
      "\n",
      "[sqlfluff:rules:convention.terminator]\n",
      "multiline_newline = False\n",
      "require_final_semicolon = True\n",
      "\n",
      "[sqlfluff:rules:layout.long_lines]\n",
      "ignore_comment_lines = True\n",
      "\n",
      "[sqlfluff:rules:references.keywords]\n",
      "ignore_words = None\n",
      "quoted_identifiers_policy = none\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "[sqlfluff:rules:convention.quoted_literals]\n",
      "preferred_quoted_literal_style = single_quotes\n",
      "\n",
      "[sqlfluff:rules:references.quoting]\n",
      "prefer_quoted_identifiers = True\n",
      "\n",
      "[sqlfluff:rules:references.special_chars]\n",
      "additional_allowed_characters = \"\"\n",
      "allow_space_in_identifier = False\n",
      "quoted_identifiers_policy = all\n",
      "# Special characters in identifiers\n",
      "unquoted_identifiers_policy = all\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "Exception thrown when SELECT DISTINCT not on the same line\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Check a file containing this request:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "It fails this way:\n",
      "\n",
      "```log\n",
      "CRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \n",
      "Traceback (most recent call last):\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\n",
      "    res = self._eval(context=context)\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\n",
      "    self._analyze_table_references(\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\n",
      "    if not self._should_ignore_reference(r, selectable):\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\n",
      "    ref_path = selectable.selectable.path_to(reference)\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\n",
      "    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\n",
      "  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\n",
      "    assert self.pos_marker\n",
      "AssertionError\n",
      "== [file.sql] FAIL\n",
      "L:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\n",
      "                       | only one select target. [layout.select_targets]\n",
      "L:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\n",
      "                       | line as 'SELECT'. [layout.select_modifiers]\n",
      "L:   1 | P:   1 | RF01 | Unexpected exception: ;\n",
      "Could you open an issue at\n",
      "                       | https://github.com/sqlfluff/sqlfluff/issues ?\n",
      "You can\n",
      "                       | ignore this exception for now, by adding '-- noqa: RF01'\n",
      "                       | at the end\n",
      "of line 1\n",
      " [references.from]\n",
      "L:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\n",
      "L:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\n",
      "                       | [layout.end_of_file]\n",
      "All Finished!\n",
      "```\n",
      "\n",
      "Checking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\n",
      "\n",
      "```sql\n",
      "SELECT DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "Additionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\n",
      "```sql\n",
      "SELECT \n",
      "DISTINCT\n",
      "`FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "Then the website crashes.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I would expect not to have an exception.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "An exception was thrown whereas, I think, there is no reason to throw it.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Check the following SQL:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    DISTINCT `FIELD`\n",
      "FROM `TABLE`;\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "MySQL\n",
      "\n",
      "### Version\n",
      "\n",
      "2.3.2\n",
      "\n",
      "### Configuration\n",
      "\n",
      "```\n",
      "[sqlfluff]\n",
      "# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\n",
      "dialect = mysql\n",
      "encoding = utf-8\n",
      "# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\n",
      "# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\n",
      "exclude_rules = LT01, ST05\n",
      "ignore = parsing\n",
      "max_line_length = 120\n",
      "# Below controls SQLFluff output, see max_line_length for SQL output\n",
      "output_line_length = 80\n",
      "templater = raw\n",
      "verbose = 0\n",
      "\n",
      "[sqlfluff:layout:type:binary_operator]\n",
      "line_position = leading\n",
      "\n",
      "[sqlfluff:layout:type:comma]\n",
      "line_position = trailing\n",
      "spacing_before = touch\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "# See https://docs.sqlfluff.com/en/stable/indentation.html\n",
      "indent_unit = space\n",
      "indented_joins = True\n",
      "indented_using_on = True\n",
      "tab_space_size = 4\n",
      "\n",
      "# Some rules can be configured directly from the config common to other rules\n",
      "[sqlfluff:rules]\n",
      "allow_scalar = True\n",
      "quoted_identifiers_policy = none\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "[sqlfluff:rules:aliasing.column]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:aliasing.table]\n",
      "aliasing = explicit\n",
      "\n",
      "[sqlfluff:rules:ambiguous.column_references]\n",
      "group_by_and_order_by_style = consistent\n",
      "\n",
      "[sqlfluff:rules:capitalisation.functions]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.identifiers]\n",
      "extended_capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.keywords]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.literals]\n",
      "capitalisation_policy = upper\n",
      "ignore_words = None\n",
      "\n",
      "[sqlfluff:rules:capitalisation.types]\n",
      "extended_capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:convention.count_rows]\n",
      "prefer_count_0 = False\n",
      "prefer_count_1 = True\n",
      "\n",
      "[sqlfluff:rules:convention.select_trailing_comma]\n",
      "select_clause_trailing_comma = forbid\n",
      "\n",
      "[sqlfluff:rules:convention.terminator]\n",
      "multiline_newline = False\n",
      "require_final_semicolon = True\n",
      "\n",
      "[sqlfluff:rules:layout.long_lines]\n",
      "ignore_comment_lines = True\n",
      "\n",
      "[sqlfluff:rules:references.keywords]\n",
      "ignore_words = None\n",
      "quoted_identifiers_policy = none\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "[sqlfluff:rules:convention.quoted_literals]\n",
      "preferred_quoted_literal_style = single_quotes\n",
      "\n",
      "[sqlfluff:rules:references.quoting]\n",
      "prefer_quoted_identifiers = True\n",
      "\n",
      "[sqlfluff:rules:references.special_chars]\n",
      "additional_allowed_characters = \"\"\n",
      "allow_space_in_identifier = False\n",
      "quoted_identifiers_policy = all\n",
      "# Special characters in identifiers\n",
      "unquoted_identifiers_policy = all\n",
      "```\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 23 lines----------------\n",
      "\"ValueError: Position Not Found\" with macro spanning entire file\n",
      "## Expected Behaviour\n",
      "\n",
      "`sqlfluff parse` should probably not fail with an exception and stack trace.\n",
      "\n",
      "## Observed Behaviour\n",
      "\n",
      "`sqlfluff parse` throws an exception, given an input file which is entirely spanned by a Jinja macro.\n",
      "\n",
      "## Steps to Reproduce\n",
      "\n",
      "```console\n",
      "$ echo -n '{% macro foo() %}{% endmacro %}' | sqlfluff parse -\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/bin/sqlfluff\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')())\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/cli/commands.py\", line 701, in parse\n",
      "    lnt.parse_string(\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 596, in parse_string\n",
      "    return self.parse_rendered(rendered, recurse=recurse)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 294, in parse_rendered\n",
      "    tokens, lvs, config = cls._lex_templated_file(\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 127, in _lex_templated_file\n",
      "    tokens, lex_vs = lexer.lex(templated_file)\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 319, in lex\n",
      "    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 346, in elements_to_segments\n",
      "    source_slice = templated_file.templated_slice_to_source_slice(\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 319, in templated_slice_to_source_slice\n",
      "    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\n",
      "  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 214, in _find_slice_indices_of_templated_pos\n",
      "    raise ValueError(\"Position Not Found\")\n",
      "ValueError: Position Not Found\n",
      "```\n",
      "\n",
      "Note: the issue does not occur if the file ends with a newline. \n",
      "\n",
      "The contents of the macro also doesn't matter.\n",
      "\n",
      "## Dialect\n",
      "\n",
      "None specified\n",
      "\n",
      "## Version\n",
      "SQLFluff 6011bdbe05669b075045e8127cdf18cc537686d4, Python 3.9.6\n",
      "\n",
      "## Configuration\n",
      "\n",
      "None\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Double backticks in Lint description\n",
      "![image](https://user-images.githubusercontent.com/80432516/150420352-57452c80-ad25-423b-8251-645e541579ad.png)\n",
      "(n.b. this affects a lot more rules than L051)\n",
      "\n",
      "This was introduced in #2234 in which docstrings such as\n",
      "```\n",
      "`INNER JOIN` must be fully qualified.\n",
      "```\n",
      "were replaced with \n",
      "```\n",
      "``INNER JOIN`` must be fully qualified.\n",
      "```\n",
      "so that they appear as code blocks in Sphinx for docs.\n",
      "![image](https://user-images.githubusercontent.com/80432516/150420294-eb9d3127-db1d-457c-a637-d614e0267277.png)\n",
      "\n",
      "However, our rules will use the first line of these docstrings in the event that no `description` is provided to the lint results.\n",
      "\n",
      "This doesn't look great on the CLI so we should fix this. As far as I'm aware there are two approaches for this:\n",
      "1. Pass a `description` to all the `LintResult`s.\n",
      "2. Update the code that gets the default description from the docstring to do something like, replace the double backticks with a single one, or remove them, or do something clever like make them bold for the CLI and remove them for non-CLI.\n",
      "\n",
      "My strong preference is number 2, but I'm open to discussion as to how exactly we do this ðŸ˜„ \n",
      "\n",
      "@barrywhart @tunetheweb \n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 65 lines----------------\n",
      "Update warning for parsing errors found on the ansi dialect\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "In the past specifying a dialect was **optional**. If unspecified, the dialect defaulted to `ansi`. Because of this there is a warning presented when sqlfluff runs in parse mode and the dialect is set to ansi and parsing errors are encountered.\n",
      "\n",
      "`WARNING: Parsing errors found and dialect is set to 'ansi'. Have you configured your dialect?`\n",
      "\n",
      "Currently, specifying a dialect is **mandatory**. Therefore this warning is perhaps not needed... and certainly not needed in its current form.\n",
      "\n",
      "I opened this issue to document the idea and solicit feedback. \n",
      "1. The simplest improvement to make the message more appropriate is to just change it to this:\n",
      "\n",
      "`WARNING: Parsing errors found and dialect is set to 'ansi'. Is 'ansi' the correct dialect?`\n",
      "\n",
      "2. On the other hand, we know that the user explicitly set the dialect to `ansi`. So why bother asking if it was intentional? We don't ask if you meant postgres or tsql. There's an argument to simply remove the warning altogether.\n",
      "\n",
      "3. Finally, we could potentially differentiate between `--dialect ansi` passed on the command line vs the dialect being picked up from a `.sqlfluff` config file. Perhaps the warning should be displayed only the in the case where the dialect was picked up implicitly from the config file.\n",
      "\n",
      "### Use case\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 103 lines----------------\n",
      "L045:  Unused CTEs are not automatically detected when using jinja/dbt as a templater\n",
      "## Expected Behaviour\n",
      "When unused CTEs are used with jinja or dbt as a templater, these are detected by L045. \n",
      "\n",
      "## Observed Behaviour\n",
      "When ref() statements are included in a SQL file and dbt is used as a templater, these seem to interfere with the ability for rule L045 to detect the unused CTEs.  The same behavior is observed when Jinja is included under the \"FROM\" statement of the relevant queries.\n",
      "\n",
      "## Steps to Reproduce\n",
      "(1). Generate a valid dbt project with at least two models with one variable each.  For the purposes of this reproduction example, I am going to assume that one model is 'foo' with variable 'var_foo' and one model is 'bar' with variable 'var_bar'.\n",
      " \n",
      "(2) Using DBT as a templater and BigQuery as a dialect, run dbt lint on the following SQL file:\n",
      "\n",
      "```sql\n",
      "WITH\n",
      "random_gibberish AS (\n",
      "    SELECT var_foo\n",
      "    FROM\n",
      "        {{ ref('foo') }}\n",
      ")\n",
      "\n",
      "SELECT var_bar\n",
      "FROM\n",
      "    {{ ref('bar') }}\n",
      "```\n",
      "\n",
      "If the templater is switched to Jinja, L045 again doesn't produce any errors.\n",
      "\n",
      "## Dialect\n",
      "Bigquery\n",
      "\n",
      "## Version\n",
      "SQLFluff version is 0.10.0.  Python version is 3.8.10.\n",
      "I'm using dbt 1.0.1 but the same issue occurs when Jinja is used as a templater.\n",
      "\n",
      "## Configuration\n",
      "```\n",
      "[sqlfluff]\n",
      "dialect = bigquery\n",
      "exclude_rules = L003,L008,L011,L014,L016,L029,L031,L034\n",
      "\n",
      "[sqlfluff:rules]\n",
      "max_line_length = 120\n",
      "comma_style = leading\n",
      "\n",
      "[sqlfluff:rules:L010]\n",
      "capitalisation_policy = upper\n",
      "\n",
      "[sqlfluff:rules:L030]\n",
      "capitalisation_policy = upper\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 337 lines----------------\n",
      "`fix` per file linted instead of at the end\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "I am just testing sqlfluff on a small example project.\n",
      "We have configured it as part of `pre-commit`.\n",
      "\n",
      "```\n",
      "-   repo: https://github.com/sqlfluff/sqlfluff\n",
      "    rev: 1.0.0\n",
      "    hooks:\n",
      "    -   id: sqlfluff-fix\n",
      "        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\n",
      "        files: \\.(sql)$\n",
      "        exclude: sp_whoisactive.sql\n",
      "```\n",
      "\n",
      "Processing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\n",
      "\n",
      "At the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n",
      "\n",
      "### Use case\n",
      "\n",
      "Instead of writing all formatted sql at the end, I would like to see files written sequentially.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "most likely all, i am working with t-sql.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "`fix` per file linted instead of at the end\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "I am just testing sqlfluff on a small example project.\n",
      "We have configured it as part of `pre-commit`.\n",
      "\n",
      "```\n",
      "-   repo: https://github.com/sqlfluff/sqlfluff\n",
      "    rev: 1.0.0\n",
      "    hooks:\n",
      "    -   id: sqlfluff-fix\n",
      "        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\n",
      "        files: \\.(sql)$\n",
      "        exclude: sp_whoisactive.sql\n",
      "```\n",
      "\n",
      "Processing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\n",
      "\n",
      "At the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n",
      "\n",
      "### Use case\n",
      "\n",
      "Instead of writing all formatted sql at the end, I would like to see files written sequentially.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "most likely all, i am working with t-sql.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 29 lines----------------\n",
      "\"Dropped elements in sequence matching\" when doubled semicolon\n",
      "## Expected Behaviour\n",
      "Frankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\n",
      "## Observed Behaviour\n",
      "```console\n",
      "(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\n",
      "    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\n",
      "    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\n",
      "    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\n",
      "    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\n",
      "    return self.parse_rendered(rendered, recurse=recurse)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\n",
      "    parsed, pvs = cls._parse_tokens(\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\n",
      "    parsed: Optional[BaseSegment] = parser.parse(\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\n",
      "    parsed = root_segment.parse(parse_context=ctx)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\n",
      "    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\n",
      "  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\n",
      "\n",
      "```\n",
      "## Steps to Reproduce\n",
      "Run \n",
      "```console\n",
      "echo \"select id from tbl;;\" | sqlfluff lint -\n",
      "```\n",
      "## Dialect\n",
      "default (ansi)\n",
      "## Version\n",
      "```\n",
      "sqlfluff, version 0.6.6\n",
      "Python 3.9.5\n",
      "```\n",
      "## Configuration\n",
      "None\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 753 lines----------------\n",
      "Rename BaseCrawler class as BaseRule to be clearer, avoid confusion with analysis helper classes, e.g. SelectCrawler\n",
      "Discussed here:\n",
      "https://github.com/sqlfluff/sqlfluff/pull/779#pullrequestreview-604167034\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 121 lines----------------\n",
      "Deduplicate violations in the same position\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "When linting jinja files with loops we get multiple output violations for each time around the loop. e.g.\n",
      "\n",
      "```sql\n",
      "select\n",
      "    a,\n",
      "    {% for val in [1, 2, 3, 4, 5, 6] %}\n",
      "        d+ {{ val }},\n",
      "    {% endfor %}\n",
      "    b\n",
      "```\n",
      "\n",
      "we get\n",
      "\n",
      "```\n",
      "== [test.sql] FAIL\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   4 | P:  10 | L006 | Missing whitespace before +\n",
      "L:   7 | P:   1 | L001 | Unnecessary trailing whitespace.\n",
      "```\n",
      "\n",
      "The duplicated `Missing whitespace` isn't helpful for the user. Regardless of whether we keep them in the background (perhaps we should), they shouldn't be shown to the user here because we're showing the same issue multiple times.\n",
      "\n",
      "### Use case\n",
      "\n",
      "CLI linting\n",
      "\n",
      "### Dialect\n",
      "\n",
      "all\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 30 lines----------------\n",
      "Running `lint` on an empty file fails with critical Exception\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "This is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\n",
      "\n",
      "While trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I'm not entirely sure what the correct result is. Feasible option are\n",
      "\n",
      "- Passing\n",
      "- Raise some kind of lint error (but not a critical exception)\n",
      "\n",
      "My personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "`LT01` and `LT12` with an critical Exception\n",
      "\n",
      "```\n",
      "CRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\n",
      "CRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\n",
      "```\n",
      "\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "```sh\n",
      "cat /dev/null | sqlfluff lint --dialect ansi -\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Version\n",
      "\n",
      "latest main branch\n",
      "\n",
      "```\n",
      "git rev-parse HEAD\n",
      "d19de0ecd16d298f9e3bfb91da122734c40c01e5\n",
      "```\n",
      "\n",
      "### Configuration\n",
      "\n",
      "default\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "Running `lint` on an empty file fails with critical Exception\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "This is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\n",
      "\n",
      "While trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I'm not entirely sure what the correct result is. Feasible option are\n",
      "\n",
      "- Passing\n",
      "- Raise some kind of lint error (but not a critical exception)\n",
      "\n",
      "My personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "`LT01` and `LT12` with an critical Exception\n",
      "\n",
      "```\n",
      "CRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\n",
      "CRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\n",
      "```\n",
      "\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "```sh\n",
      "cat /dev/null | sqlfluff lint --dialect ansi -\n",
      "```\n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Version\n",
      "\n",
      "latest main branch\n",
      "\n",
      "```\n",
      "git rev-parse HEAD\n",
      "d19de0ecd16d298f9e3bfb91da122734c40c01e5\n",
      "```\n",
      "\n",
      "### Configuration\n",
      "\n",
      "default\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 1114 lines----------------\n",
      "[EXPERIMENT]: Rethink Matching routines\n",
      "This is another experiment, and also a biggie. It's a rethink of matching as part of #5124.\n",
      "\n",
      "This will need some tidying to get it into a state that it's reviewable, but given the scale of it - I think I shouldn't take it much further without getting some of it merged.\n",
      "\n",
      "It's mostly additions for now, so I now need to strip out the things that we can get rid of as a result. Opening PR for testing and in particular for coverage.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 38 lines----------------\n",
      "Jinja: sqlfluff fails in the presence of assignments with multiple targets\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "I did search, and I think this _may_ be related, but since no more information was provided I cannot confirm it: https://github.com/sqlfluff/sqlfluff/issues/2947. For this reason, I opened a new issue.\n",
      "\n",
      "### What Happened\n",
      "\n",
      "Jinja templates support multiple targets in [assignments](https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments). However, `sqlfluff` fails to lint a file in the presence of an assignment with multiple targets.\n",
      "\n",
      "I traced this back to the `update_inside_set_or_macro` function, specifically [this line](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L244=).\n",
      "\n",
      "The way `sqlfluff` is determining whether we are inside a [block assignment](https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments) is by checking for the presence of an equals in the second index of the trimmed parts of the current raw slice:\n",
      "\n",
      "```python\n",
      "if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\n",
      "```\n",
      "\n",
      "This condition is false for single target assignments:\n",
      "\n",
      "```sql\n",
      "{% set a = 1 %}\n",
      "```\n",
      "\n",
      "Which produce the expected trimmed parts (with spaces removed as in [line 243](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L243=)):\n",
      "\n",
      "```python\n",
      "['set', 'a', '=', '1']\n",
      "#             2    \n",
      "```\n",
      "\n",
      "However, with multiple targets:\n",
      "\n",
      "```sql\n",
      "{% set a, b = 1, 2 %}\n",
      "```\n",
      "\n",
      "```python\n",
      "['set', 'a', ',', 'b', '=', '1', '2']\n",
      "#                       4    \n",
      "```\n",
      "\n",
      "Equals is no longer in the index 2, but has been bumped to index 4, yet we are not in the expanded block form of set assignments. This causes the `inside_set_or_macro` flag to be incorrectly set to `True`, as if we were using a block assignment, which causes the entire template to be ignored (or something like that), and leads to the eventual `ValueError` raised.\n",
      "\n",
      "I played around a bit with potential solutions: first, I tried incrementing the index of the equals by the number of commas:\n",
      "\n",
      "```python\n",
      "equals_index = 2 + sum((c == ',' for c in  filtered_trimmed_parts))\n",
      "if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[equals_index] != \"=\":\n",
      "```\n",
      "\n",
      "However, this would bring issues if using the expanded form of set assignments with any commas in it, or in the presence of an uneven number of commas on both sides of the assignment.\n",
      "\n",
      "Another simpler option would be to check for the presence of a single equals:\n",
      "\n",
      "```python\n",
      "if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\n",
      "```\n",
      "\n",
      "This one seems more promising, specially considering that multiple targets appear not to be supported with block assignments (at least, that's what I think, as the docs don't mention it, and trying it locally raises a too many values to unpack error). Thus, the first condition will always be true for block assignments (so, even the presence of an equals in the body of the assignment would not cause issues).\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "sqlfluff should lint files properly, even in the presence of assignments with multiple targets.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Linting fails when an exception is raised:\n",
      "\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10dbt/bin/sqlfluff\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/cli/commands.py\", line 541, in lint\n",
      "    result = lnt.lint_paths(\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1098, in lint_paths\n",
      "    self.lint_path(\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1050, in lint_path\n",
      "    for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 101, in run\n",
      "    for fname, partial in self.iter_partials(fnames, fix=fix):\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 54, in iter_partials\n",
      "    for fname, rendered in self.iter_rendered(fnames):\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 43, in iter_rendered\n",
      "    yield fname, self.linter.render_file(fname, self.config)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 771, in render_file\n",
      "    return self.render_string(raw_file, fname, config, encoding)\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 742, in render_string\n",
      "    templated_file, templater_violations = self.templater.process(\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/jinja.py\", line 394, in process\n",
      "    TemplatedFile(\n",
      "  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/base.py\", line 94, in __init__\n",
      "    raise ValueError(\"Cannot instantiate a templated file unsliced!\")\n",
      "ValueError: Cannot instantiate a templated file unsliced!\n",
      "```\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "1. Save the following template to `model.sql` in an empty directory:\n",
      "```sql\n",
      "{% set a, b = 1, 2 %}\n",
      "\n",
      "SELECT {{ a }}\n",
      "```\n",
      "2. Run `sqlfluff lint model.sql --dialect 'postgres'`\n",
      "\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Tried with postgres and redshift dialects, however I think others may be affected as long as they use jinja templates.\n",
      "\n",
      "### Version\n",
      "\n",
      "v0.12.0\n",
      "\n",
      "### Configuration\n",
      "\n",
      "Nothing, ran from an empty directory.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [X] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 142 lines----------------\n",
      "Suppress dbt logs and warnings when using --format github-annotation\n",
      "Sometimes, running:\n",
      "```\n",
      "sqlfluff lint --format github-annotation --annotation-level failure --nofail \n",
      "```\n",
      "\n",
      "Can result in the first couple of output lines being logs which break the annotations, for example:\n",
      "```\n",
      "14:21:42  Partial parse save file not found. Starting full parse.\n",
      "Warning:  [WARNING]: Did not find matching node for patch with name 'xxxx' in the 'models' section of file 'models/production/xxxxx/xxxxx.yml'\n",
      "```\n",
      "\n",
      "## Version\n",
      "dbt 1.0.0, SQLFLuff 0.9.0\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 223 lines----------------\n",
      "Add \"enable\" and \"disable\" syntax to noqa to allow rules disabling across multiple lines\n",
      "See the `pylint` docs for an example: https://docs.pylint.org/en/1.6.0/faq.html#is-it-possible-to-locally-disable-a-particular-message\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 79 lines----------------\n",
      "Configuration from current working path not being loaded when path provided.\n",
      "I have the following directory structure.\n",
      "```\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  tree -a\n",
      ".\n",
      "â”œâ”€â”€ .sqlfluffignore\n",
      "â”œâ”€â”€ ignore_me_1.sql\n",
      "â”œâ”€â”€ path_a\n",
      "â”‚Â Â  â””â”€â”€ ignore_me_2.sql\n",
      "â””â”€â”€ path_b\n",
      "    â”œâ”€â”€ ignore_me_3.sql\n",
      "    â””â”€â”€ lint_me_1.sql\n",
      "\n",
      "2 directories, 5 files\n",
      "```\n",
      "And the following ignore file\n",
      "\n",
      "```\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  cat .sqlfluffignore\n",
      "\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  cat .sqlfluffignore\n",
      "ignore_me_1.sql\n",
      "path_a/\n",
      "path_b/ignore_me_3.sql%\n",
      "```\n",
      "\n",
      "When I run the following I get the expected result. Sqlfluff only lints the one file that is not ignored.\n",
      "```\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint .\n",
      "\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint .\n",
      "== [path_b/lint_me_1.sql] FAIL\n",
      "L:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\n",
      "L:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\n",
      "```\n",
      "\n",
      "However when I run the lint explicitly on one of the two directories then ignored files are also linted.\n",
      "\n",
      "```\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint path_a\n",
      "\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint path_a\n",
      "== [path_a/ignore_me_2.sql] FAIL\n",
      "L:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\n",
      "L:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\n",
      "\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint path_b\n",
      "\n",
      "~/GitHub/sqlfluff-bug\n",
      "âžœ  sqlfluff lint path_b\n",
      "== [path_b/ignore_me_3.sql] FAIL\n",
      "L:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\n",
      "L:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\n",
      "== [path_b/lint_me_1.sql] FAIL\n",
      "L:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\n",
      "L:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\n",
      "```\n",
      "\n",
      "If this is the expected behaviour then it might be worthwhile to add an example to the [docs](https://docs.sqlfluff.com/en/latest/configuration.html#sqlfluffignore).\n",
      "\n",
      "Edit: I've replicated this issue on sqlfluff version 0.3.2 to 0.3.6.\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 332 lines----------------\n",
      "Write-output human format does not produce result\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "When running SQLFluff using the following statement:\n",
      "`python -m sqlfluff lint --write-output test.txt --config=config/sql-lint.cfg`\n",
      "no result was produced.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "I expect a file to appear, in this case called test,txt, containing all violations found.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Looking through the code I saw human was the default format so expected adding --format=human would not make a difference. To be sure, I also ran the statement using the flag and it still produced nothing.\n",
      "\n",
      "To make sure it was just the human format which was having problems, I also executed the statement using --format=json,yaml,github-annotations, all of which did produce the expected result which leads me to believe there is something wrong with the human format.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "I imagine simply executing `sqlfluff lint --write-output test.txt example.sql`\n",
      "\n",
      "### Dialect\n",
      "\n",
      "T-SQL\n",
      "\n",
      "### Version\n",
      "\n",
      "0.11.2\n",
      "\n",
      "### Configuration\n",
      "\n",
      "[sqlfluff]\n",
      "dialect = tsql\n",
      "exclude_rules = L014,\n",
      "                L016,\n",
      "                L031,\n",
      "                L035,\n",
      "                L059\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 41 lines----------------\n",
      "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n",
      "_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\n",
      "\n",
      "## Expected Behaviour\n",
      "Violation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\n",
      "\n",
      "## Observed Behaviour\n",
      "Reported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\n",
      "\n",
      "## Steps to Reproduce\n",
      "SQL file:\n",
      "```sql\n",
      "SELECT\n",
      "    reacted_table_name_right.descendant_id AS category_id,\n",
      "    string_agg(redacted_table_name_left.name, ' â†’ ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\n",
      "FROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\n",
      "INNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\n",
      "    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\n",
      "GROUP BY reacted_table_name_right.descendant_id\n",
      "```\n",
      "Running `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\n",
      "\n",
      "## Dialect\n",
      "`postgres`, with `dbt` templater\n",
      "\n",
      "## Version\n",
      "`python 3.7.12`\n",
      "`sqlfluff 0.7.0`\n",
      "`sqlfluff-templater-dbt 0.7.0`\n",
      "\n",
      "## Configuration\n",
      "I've tried a few, here's one:\n",
      "```\n",
      "[sqlfluff]\n",
      "verbose = 2\n",
      "dialect = postgres\n",
      "templater = dbt\n",
      "exclude_rules = None\n",
      "output_line_length = 80\n",
      "runaway_limit = 10\n",
      "ignore_templated_areas = True\n",
      "processes = 3\n",
      "# Comma separated list of file extensions to lint.\n",
      "\n",
      "# NB: This config will only apply in the root folder.\n",
      "sql_file_exts = .sql\n",
      "\n",
      "[sqlfluff:indentation]\n",
      "indented_joins = False\n",
      "indented_using_on = True\n",
      "template_blocks_indent = True\n",
      "\n",
      "[sqlfluff:templater]\n",
      "unwrap_wrapped_queries = True\n",
      "\n",
      "[sqlfluff:templater:jinja]\n",
      "apply_dbt_builtins = True\n",
      "\n",
      "[sqlfluff:templater:jinja:macros]\n",
      "# Macros provided as builtins for dbt projects\n",
      "dbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\n",
      "dbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\n",
      "dbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\n",
      "dbt_var = {% macro var(variable, default='') %}item{% endmacro %}\n",
      "dbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\n",
      "\n",
      "# Common config across rules\n",
      "[sqlfluff:rules]\n",
      "tab_space_size = 4\n",
      "indent_unit = space\n",
      "single_table_references = consistent\n",
      "unquoted_identifiers_policy = all\n",
      "\n",
      "# L001 - Remove trailing whitespace (fix)\n",
      "# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\n",
      "# L003 - Keep consistent indentation (fix)\n",
      "# L004 - We use 4 spaces for indentation just for completeness (fix)\n",
      "# L005 - Remove space before commas (fix)\n",
      "# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\n",
      "\n",
      "# L007 - Operators should not be at the end of a line\n",
      "[sqlfluff:rules:L007]  # Keywords\n",
      "operator_new_lines = after\n",
      "\n",
      "# L008 - Always use a single whitespace after a comma (fix)\n",
      "# L009 - Files will always end with a trailing newline\n",
      "\n",
      "# L010 - All keywords will use full upper case (fix)\n",
      "[sqlfluff:rules:L010]  # Keywords\n",
      "capitalisation_policy = upper\n",
      "\n",
      "# L011 - Always explicitly alias tables (fix)\n",
      "[sqlfluff:rules:L011]  # Aliasing\n",
      "aliasing = explicit\n",
      "\n",
      "# L012 - Do not have to explicitly alias all columns\n",
      "[sqlfluff:rules:L012]  # Aliasing\n",
      "aliasing = explicit\n",
      "\n",
      "# L013 - Always explicitly alias a column with an expression in it (fix)\n",
      "[sqlfluff:rules:L013]  # Aliasing\n",
      "allow_scalar = False\n",
      "\n",
      "# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\n",
      "[sqlfluff:rules:L014]  # Unquoted identifiers\n",
      "extended_capitalisation_policy = lower\n",
      "\n",
      "# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\n",
      "\n",
      "# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\n",
      "[sqlfluff:rules:L016]\n",
      "ignore_comment_lines = False\n",
      "max_line_length = 120\n",
      "\n",
      "# L017 - There should not be whitespace between function name and brackets (fix)\n",
      "# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\n",
      "\n",
      "# L019 - Always use trailing commas / commas at the end of the line (fix)\n",
      "[sqlfluff:rules:L019]\n",
      "comma_style = trailing\n",
      "\n",
      "# L020 - Table aliases will always be unique per statement\n",
      "# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\n",
      "# L022 - Add blank lines after common table expressions (CTE) / WITH.\n",
      "# L023 - Always add a single whitespace after AS in a WITH clause (fix)\n",
      "\n",
      "[sqlfluff:rules:L026]\n",
      "force_enable = False\n",
      "\n",
      "# L027 - Always add references if more than one referenced table or view is used\n",
      "\n",
      "[sqlfluff:rules:L028]\n",
      "force_enable = False\n",
      "\n",
      "[sqlfluff:rules:L029]  # Keyword identifiers\n",
      "unquoted_identifiers_policy = aliases\n",
      "\n",
      "[sqlfluff:rules:L030]  # Function names\n",
      "capitalisation_policy = upper\n",
      "\n",
      "# L032 - We prefer use of join keys rather than USING\n",
      "# L034 - We prefer ordering of columns in select statements as (fix):\n",
      "# 1. wildcards\n",
      "# 2. single identifiers\n",
      "# 3. calculations and aggregates\n",
      "\n",
      "# L035 - Omit 'else NULL'; it is redundant (fix)\n",
      "# L036 - Move select targets / identifiers onto new lines each (fix)\n",
      "# L037 - When using ORDER BY, make the direction explicit (fix)\n",
      "\n",
      "# L038 - Never use trailing commas at the end of the SELECT clause\n",
      "[sqlfluff:rules:L038]\n",
      "select_clause_trailing_comma = forbid\n",
      "\n",
      "# L039 - Remove unnecessary whitespace (fix)\n",
      "\n",
      "[sqlfluff:rules:L040]  # Null & Boolean Literals\n",
      "capitalisation_policy = upper\n",
      "\n",
      "# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\n",
      "[sqlfluff:rules:L042]\n",
      "# By default, allow subqueries in from clauses, but not join clauses.\n",
      "forbid_subquery_in = join\n",
      "\n",
      "# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\n",
      "# L044 - Prefer a known number of columns along the path to the source data\n",
      "# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\n",
      "# L046 - Jinja tags should have a single whitespace on both sides\n",
      "\n",
      "# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\n",
      "[sqlfluff:rules:L047]  # Consistent syntax to count all rows\n",
      "prefer_count_1 = False\n",
      "prefer_count_0 = False\n",
      "\n",
      "# L048 - Quoted literals should be surrounded by a single whitespace (fix)\n",
      "# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 132 lines----------------\n",
      "Multiple processes not used when list of explicit filenames is passed\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### What Happened\n",
      "\n",
      "When providing a long list of file names to `sqlfluff lint -p -1`, only a single CPU is used. This seems to stem from the fact that https://github.com/sqlfluff/sqlfluff/blob/a006378af8b670f9235653694dbcddd4c62d1ab9/src/sqlfluff/core/linter/linter.py#L1190 is iterating over the list of files. For each listed path there, it would run the found files in parallel. As we are inputting whole filenames here, a path equals a single file and thus `sqlfluff` would only process one file at a time.\n",
      "\n",
      "The context here is the execution of `sqlfluff lint` inside a `pre-commit` hook.\n",
      "\n",
      "### Expected Behaviour\n",
      "\n",
      "All CPU cores are used as `-p -1` is passed on the commandline.\n",
      "\n",
      "### Observed Behaviour\n",
      "\n",
      "Only a single CPU core is used.\n",
      "\n",
      "### How to reproduce\n",
      "\n",
      "Run `sqlfluff lint -p -1` with a long list of files.\n",
      "\n",
      "### Dialect\n",
      "\n",
      "Affects all. \n",
      "\n",
      "### Version\n",
      "\n",
      "1.4.2\n",
      "\n",
      "### Configuration\n",
      "\n",
      "None.\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: sqlfluff/sqlfluff----------------\n",
      "-------------Fix: 162 lines----------------\n",
      "Rule suggestion: `UNION [ALL|DISTINCT]` on new line\n",
      "### Search before asking\n",
      "\n",
      "- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "I would like to suggest a new rule that puts `UNION [ALL|DISTINCT]` statements on their own line, aligned to the surrounding `SELECT` statements.\n",
      "\n",
      "For example, currently \n",
      "\n",
      "```sql\n",
      "SELECT 1 UNION ALL\n",
      "SELECT 2\n",
      "```\n",
      "\n",
      "passes without errors. This new rule could fix that to\n",
      "\n",
      "```sql\n",
      "SELECT 1 \n",
      "UNION ALL\n",
      "SELECT 2\n",
      "```\n",
      "\n",
      "Or in a more complex example\n",
      "\n",
      "```sql\n",
      "SELECT * FROM (\n",
      "    SELECT 1 UNION ALL\n",
      "    SELECT 2\n",
      ")\n",
      "```\n",
      "\n",
      "fixed to\n",
      "\n",
      "```sql\n",
      "SELECT * FROM (\n",
      "    SELECT 1 \n",
      "    UNION ALL\n",
      "    SELECT 2\n",
      ")\n",
      "```\n",
      "\n",
      "### Use case\n",
      "\n",
      "I have looked at a few SQL style guides and they don't really seem to mention any policy regarding `UNION` statements. However, in 99% of the SQL I have encountered `UNION` statements always seemed to be on a new line. It would be great to have an option to lint the remaining 1% ðŸ˜‰ \n",
      "\n",
      "### Dialect\n",
      "\n",
      "ansi\n",
      "\n",
      "### Are you willing to work on and submit a PR to address the issue?\n",
      "\n",
      "- [ ] Yes I am willing to submit a PR!\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 44 lines----------------\n",
      "2.x: Nested(many=True) eats first element from generator value when dumping\n",
      "As reproduced in Python 3.6.8:\n",
      "\n",
      "```py\n",
      "from marshmallow import Schema, fields\n",
      "\n",
      "class O(Schema):\n",
      "    i = fields.Int()\n",
      "\n",
      "class P(Schema):\n",
      "    os = fields.Nested(O, many=True)\n",
      "\n",
      "def gen():\n",
      "    yield {'i': 1}\n",
      "    yield {'i': 0}\n",
      "\n",
      "p = P()\n",
      "p.dump({'os': gen()})\n",
      "# MarshalResult(data={'os': [{'i': 0}]}, errors={})\n",
      "```\n",
      "\n",
      "Problematic code is here:\n",
      "\n",
      "https://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/fields.py#L447\n",
      "\n",
      "And here:\n",
      "\n",
      "https://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/schema.py#L832\n",
      "\n",
      "The easiest solution would be to cast `nested_obj` to list before calling `schema._update_fields`, just like a normal Schema with `many=True` does.\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 14 lines----------------\n",
      "ISO8601 DateTimes ending with Z considered not valid in 2.19.4\n",
      "Probably related to #1247 and #1234 - in marshmallow `2.19.4`, with `python-dateutil` _not_ installed, it seems that loading a datetime in ISO8601 that ends in `Z` (UTC time) results in an error:\n",
      "\n",
      "```python\n",
      "class Foo(Schema):\n",
      "    date = DateTime(required=True)\n",
      "\n",
      "\n",
      "foo_schema = Foo(strict=True)\n",
      "\n",
      "a_date_with_z = '2019-06-17T00:57:41.000Z'\n",
      "foo_schema.load({'date': a_date_with_z})\n",
      "```\n",
      "\n",
      "```\n",
      "marshmallow.exceptions.ValidationError: {'date': ['Not a valid datetime.']}\n",
      "```\n",
      "\n",
      "Digging a bit deeper, it seems [`from_iso_datetime`](https://github.com/marshmallow-code/marshmallow/blob/dev/src/marshmallow/utils.py#L213-L215) is failing with a `unconverted data remains: Z` - my understanding of the spec is rather limited, but it seems that they are indeed valid ISO8601 dates (and in `marshmallow==2.19.3` and earlier, the previous snippet seems to work without raising validation errors).\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\n",
      "Between releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\n",
      "\n",
      "```python\n",
      "from marshmallow import fields, Schema\n",
      "\n",
      "class MySchema(Schema):\n",
      "    times = fields.List(fields.DateTime())\n",
      "\n",
      "s = MySchema()\n",
      "```\n",
      "\n",
      "Traceback:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"test-mm.py\", line 8, in <module>\n",
      "    s = MySchema()\n",
      "  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\n",
      "    self.fields = self._init_fields()\n",
      "  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\n",
      "    self._bind_field(field_name, field_obj)\n",
      "  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\n",
      "    field_obj._bind_to_schema(field_name, self)\n",
      "  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\n",
      "    self.inner._bind_to_schema(field_name, self)\n",
      "  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\n",
      "    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "AttributeError: 'List' object has no attribute 'opts'\n",
      "```\n",
      "\n",
      "It seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 22 lines----------------\n",
      "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\n",
      "After update from version 2.19.5 to 2.20.0 I got error for code like:\n",
      "\n",
      "```python\n",
      "from marshmallow import Schema, fields, validates\n",
      "\n",
      "\n",
      "class Bar(Schema):\n",
      "    value = fields.String()\n",
      "\n",
      "    @validates('value')  # <- issue here\n",
      "    def validate_value(self, value):\n",
      "        pass\n",
      "\n",
      "\n",
      "class Foo(Schema):\n",
      "    bar = fields.Nested(Bar)\n",
      "\n",
      "\n",
      "sch = Foo()\n",
      "\n",
      "sch.validate({\n",
      "    'bar': 'invalid',\n",
      "})\n",
      "```\n",
      "\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/_/bug_mschema.py\", line 19, in <module>\n",
      "    'bar': 'invalid',\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\n",
      "    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\n",
      "    index_errors=self.opts.index_errors,\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\n",
      "    index=(index if index_errors else None)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\n",
      "    value = getter_func(data)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\n",
      "    data\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\n",
      "    output = self._deserialize(value, attr, data)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\n",
      "    data, errors = self.schema.load(value)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\n",
      "    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\n",
      "    self._invoke_field_validators(unmarshal, data=result, many=many)\n",
      "  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\n",
      "    value = data[field_obj.attribute or field_name]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 167 lines----------------\n",
      "fields.URL should allow relative-only validation\n",
      "Relative URLs may be used to redirect the user within the site, such as to sign in, and allowing absolute URLs without extra validation opens up a possibility of nefarious redirects.\n",
      "\n",
      "Current `fields.URL(relative = True)` allows relative URLs _in addition_ to absolute URLs, so one must set up extra validation to catch either all absolute URLs or just those that don't have a valid domain names.\n",
      "\n",
      "It would be helpful if there was a way to set up URL validation to allow only relative URLs. \n",
      "\n",
      "~One quick and dirty way to do this would be if there was a `validate.Not` operator, then at the expense of matching the value twice, it would be possible to use something like this:~\n",
      "\n",
      "~`fields.URL(relative = True, validate=validate.Not(validate.URL()))`~\n",
      "\n",
      "EDIT: Never mind the crossed out thought above - failed validations are handled only via exceptions and while failing the inner validator works in general, it requires suppressing exception handlers and is just not a good way to go about it. \n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 59 lines----------------\n",
      "`only` argument inconsistent between Nested(S, many=True) and List(Nested(S))\n",
      "```python\n",
      "from pprint import pprint\n",
      "\n",
      "from marshmallow import Schema\n",
      "from marshmallow.fields import Integer, List, Nested, String\n",
      "\n",
      "\n",
      "class Child(Schema):\n",
      "    name = String()\n",
      "    age = Integer()\n",
      "\n",
      "\n",
      "class Family(Schema):\n",
      "    children = List(Nested(Child))\n",
      "\n",
      "\n",
      "class Family2(Schema):\n",
      "    children = Nested(Child, many=True)\n",
      "\n",
      "family = {'children':[\n",
      "    {'name': 'Tommy', 'age': 12},\n",
      "    {'name': 'Lily', 'age': 15},\n",
      "]}\n",
      "\n",
      "pprint(Family( only=['children.name']).dump(family).data)\n",
      "pprint(Family2( only=['children.name']).dump(family).data)\n",
      "```\n",
      "returns\n",
      "```\n",
      "{'children': [{'age': 12, 'name': 'Tommy'}, {'age': 15, 'name': 'Lily'}]}\n",
      "{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\n",
      "```\n",
      "\n",
      "tested with marshmallow 2.15.4\n",
      "\n",
      "The same applies to `exclude` argument.\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 48 lines----------------\n",
      "3.12 no longer supports fields named `parent`\n",
      "Pretty sure that #1631 broke it. Reproducible example:\n",
      "\n",
      "```py\n",
      "from marshmallow import INCLUDE\n",
      "from marshmallow.fields import Nested\n",
      "from sqlalchemy import Column, DATE, create_engine, ForeignKey\n",
      "from sqlalchemy.dialects.postgresql import UUID\n",
      "from sqlalchemy.orm import declarative_base, relationship\n",
      "from marshmallow_sqlalchemy import SQLAlchemyAutoSchema\n",
      "from testing.postgresql import Postgresql\n",
      "\n",
      "\n",
      "Base = declarative_base()\n",
      "\n",
      "\n",
      "class Author(Base):\n",
      "    __tablename__ = 'author'\n",
      "    id = Column(UUID(as_uuid=True), primary_key=True)\n",
      "    docs = relationship('Document', back_populates='parent')\n",
      "\n",
      "\n",
      "class Document(Base):\n",
      "    __tablename__ = 'document'\n",
      "    id = Column(UUID(as_uuid=True), primary_key=True)\n",
      "    parent_id = Column(UUID(as_uuid=True), ForeignKey('author.id'))\n",
      "    parent = relationship(Author, back_populates='docs')\n",
      "    last_updated = Column(DATE)\n",
      "\n",
      "\n",
      "class AuthorSchema(SQLAlchemyAutoSchema):\n",
      "    class Meta(SQLAlchemyAutoSchema.Meta):\n",
      "        model = Author\n",
      "\n",
      "\n",
      "class DocumentSchema(SQLAlchemyAutoSchema):\n",
      "    parent = Nested(AuthorSchema)\n",
      "\n",
      "    class Meta(SQLAlchemyAutoSchema.Meta):\n",
      "        model = Document\n",
      "\n",
      "\n",
      "with Postgresql() as postgresql:\n",
      "    url = postgresql.url(drivername='postgresql+psycopg2')\n",
      "    engine = create_engine(url, echo=True)\n",
      "    Base.metadata.create_all(engine)\n",
      "\n",
      "    DocumentSchema(unknown=INCLUDE)\n",
      "```\n",
      "\n",
      "Results in:\n",
      "\n",
      "```pytb\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phil/.config/JetBrains/PyCharm2021.1/scratches/sqlalchemy-marshmallow-reprex.py\", line 44, in <module>\n",
      "    DocumentSchema(unknown=INCLUDE)\n",
      "  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow_sqlalchemy/schema/load_instance_mixin.py\", line 43, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 392, in __init__\n",
      "    self._init_fields()\n",
      "  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 971, in _init_fields\n",
      "    self._bind_field(field_name, field_obj)\n",
      "  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 1030, in _bind_field\n",
      "    field_obj._bind_to_schema(field_name, self)\n",
      "  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/fields.py\", line 1201, in _bind_to_schema\n",
      "    or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n",
      "AttributeError: 'NoneType' object has no attribute 'opts'\n",
      "```\n",
      "\n",
      "Here, `self.root` resolves to `None` for the `last_updated` field:\n",
      "\n",
      "https://github.com/marshmallow-code/marshmallow/blob/69270215ab9275dc566b010ecdb8777c186aa776/src/marshmallow/fields.py#L411-L420\n",
      "\n",
      "This happens since that fieldâ€™s `.parent` is the `DocumentSchema` class, which *does* have a `.parent` attribute. However that attribute is a `Nested` instance, not another schema as expected\n",
      "Return a fieldâ€™s root schema as soon as it is found\n",
      "This prevents accessing a schemaâ€™s `.parent` attribute if it has one (e.g. a field called `parent`)\n",
      "\n",
      "Fixes #1808, I think.\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 41 lines----------------\n",
      "RFC: Change the way we store metadata?\n",
      "Users are often bit by the fact that fields store arbitrary keyword arguments as metadata. See https://github.com/marshmallow-code/marshmallow/issues/683.\n",
      "\n",
      "> ...The reasons we use **kwargs instead of e.g. `metadata=` are mostly historical. The original decision was that storing kwargs 1) was more concise and 2) saved us from having to come up with an appropriate name... \"metadata\" didn't seem right because there are use cases where the things your storing aren't really metadata. At this point, it's not worth breaking the API.\n",
      "\n",
      "> Not the best reasons, but I think it's not terrible. We've discussed adding a [whitelist of metadata keys](https://github.com/marshmallow-code/marshmallow/issues/683#issuecomment-385113845) in the past, but we decided it wasn't worth the added API surface.\n",
      "\n",
      "_Originally posted by @sloria in https://github.com/marshmallow-code/marshmallow/issues/779#issuecomment-522283135_\n",
      "\n",
      "Possible solutions:\n",
      "\n",
      "1. Use `metadata=`.\n",
      "2. Specify a whitelist of allowed metadata arguments.\n",
      "\n",
      "Feedback welcome!\n",
      "\n",
      "-------------PROBLEM STATEMENT: marshmallow-code/marshmallow----------------\n",
      "-------------Fix: 36 lines----------------\n",
      "Incorrect Email Validation\n",
      "https://github.com/marshmallow-code/marshmallow/blob/fbe22eb47db5df64b2c4133f9a5cb6c6920e8dd2/src/marshmallow/validate.py#L136-L151\n",
      "\n",
      "The email validation regex will match `email@domain.com\\n`, `email\\n@domain.com`, and `email\\n@domain.com\\n`.\n",
      "\n",
      "The issue is that `$` is used to match until the end of a string. Instead, `\\Z` should be used. - https://stackoverflow.com/a/48730645\n",
      "\n",
      "It is possible that other validators might suffer from the same bug, so it would be good if other regexes were also checked.\n",
      "\n",
      "It is unclear, but this may lead to a security vulnerability in some projects that use marshmallow (depending on how the validator is used), so a quick fix here might be helpful. In my quick look around I didn't notice anything critical, however, so I figured it would be fine to open this issue.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 61 lines----------------\n",
      "ValueError: SingleAxisTracker, Array, and running the model on a tuple/list of weather\n",
      "**Describe the bug**\n",
      "I know a refactoring of the Array with single axis tracking is in the works #1146. In the meantime, a `ValueError` is raised when trying to run a SingleAxisTracker defined with an array and supplying (ghi, dni, dhi) weather as a tuple/list. I would expect calling `run_model([weather])` would work similarly to a modelchain for a fixed system with an array singleton. The error stems from `pvlib.tracking.SingleAxisTracker.get_irradiance`  because most inputs are `pandas.Series`, but ghi, dhi, dni are `Tuple[Series]`.\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "import pandas as pd\n",
      "from pvlib.location import Location\n",
      "from pvlib.pvsystem import Array\n",
      "from pvlib.tracking import SingleAxisTracker\n",
      "from pvlib.modelchain import ModelChain\n",
      "\n",
      "\n",
      "array_params = {\n",
      "    \"surface_tilt\": None,\n",
      "    \"surface_azimuth\": None,\n",
      "    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\n",
      "    \"albedo\": 0.2,\n",
      "    \"temperature_model_parameters\": {\n",
      "        \"u_c\": 29.0,\n",
      "        \"u_v\": 0.0,\n",
      "        \"eta_m\": 0.1,\n",
      "        \"alpha_absorption\": 0.9,\n",
      "    },\n",
      "    \"strings\": 5,\n",
      "    \"modules_per_string\": 7,\n",
      "    \"module_parameters\": {\n",
      "        \"alpha_sc\": 0.004539,\n",
      "        \"gamma_ref\": 1.2,\n",
      "        \"mu_gamma\": -0.003,\n",
      "        \"I_L_ref\": 5.11426,\n",
      "        \"I_o_ref\": 8.10251e-10,\n",
      "        \"R_sh_ref\": 381.254,\n",
      "        \"R_sh_0\": 400.0,\n",
      "        \"R_s\": 1.06602,\n",
      "        \"cells_in_series\": 96,\n",
      "        \"R_sh_exp\": 5.5,\n",
      "        \"EgRef\": 1.121,\n",
      "    },\n",
      "}\n",
      "inverter_parameters = {\n",
      "    \"Paco\": 250.0,\n",
      "    \"Pdco\": 259.589,\n",
      "    \"Vdco\": 40.0,\n",
      "    \"Pso\": 2.08961,\n",
      "    \"C0\": -4.1e-05,\n",
      "    \"C1\": -9.1e-05,\n",
      "    \"C2\": 0.000494,\n",
      "    \"C3\": -0.013171,\n",
      "    \"Pnt\": 0.075,\n",
      "}\n",
      "\n",
      "\n",
      "location = Location(latitude=33.98, longitude=-115.323, altitude=2300)\n",
      "\n",
      "\n",
      "tracking = SingleAxisTracker(\n",
      "    arrays=[Array(**array_params, name=0)],\n",
      "    axis_tilt=0,\n",
      "    axis_azimuth=180,\n",
      "    gcr=0.1,\n",
      "    backtrack=True,\n",
      "    inverter_parameters=inverter_parameters,\n",
      ")\n",
      "\n",
      "weather = pd.DataFrame(\n",
      "    {\n",
      "        \"ghi\": [1100.0, 1101.0],\n",
      "        \"dni\": [1000.0, 1001],\n",
      "        \"dhi\": [100.0, 100],\n",
      "        \"module_temperature\": [25.0, 25],\n",
      "    },\n",
      "    index=pd.DatetimeIndex(\n",
      "        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\n",
      "    ),\n",
      ")\n",
      "mc = ModelChain(\n",
      "    tracking,\n",
      "    location,\n",
      "    aoi_model=\"no_loss\",\n",
      "    spectral_model=\"no_loss\",\n",
      ")\n",
      "mc.run_model(weather)  # OK\n",
      "mc.run_model([weather])  # ValueError\n",
      "\n",
      "```\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.9.0-alpha.2+2.g47654a0\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "`pvlib.soiling.hsu` takes `tilt` instead of `surface_tilt`\n",
      "`pvlib.soiling.hsu` takes a `tilt` parameter representing the same thing we normally call `surface_tilt`:\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L13-L14\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L33-L34\n",
      "\n",
      "I don't see any good reason for this naming inconsistency (I suspect `tilt` just got copied from the matlab implementation) and suggest we rename the parameter to `surface_tilt` with a deprecation.\n",
      "\n",
      "Also, the docstring parameter type description says it must be `float`, but the model's reference explicitly says time series tilt is allowed: \n",
      "\n",
      "> The angle is variable for tracking systems and is taken as the average angle over the time step.\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 18 lines----------------\n",
      "_golden_sect_DataFrame changes in 0.9.4\n",
      "**Describe the bug**\n",
      "\n",
      "`0.9.4` introduced the following changes in the `_golden_sect_DataFrame`: We are checking `upper` and `lower` parameters and raise an error if `lower > upper`.\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/tools.py#L344-L345\n",
      "\n",
      "`_golden_sect_DataFrame` is used by `_lambertw`:\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/singlediode.py#L644-L649\n",
      "\n",
      "I often have slightly negative `v_oc` values (really close to 0) when running simulations (second number in the array below):\n",
      "```\n",
      "array([ 9.46949758e-16, -8.43546518e-15,  2.61042547e-15,  3.82769773e-15,\n",
      "        1.01292315e-15,  4.81308106e+01,  5.12484772e+01,  5.22675087e+01,\n",
      "        5.20708941e+01,  5.16481028e+01,  5.12364071e+01,  5.09209060e+01,\n",
      "        5.09076598e+01,  5.10187680e+01,  5.11328118e+01,  5.13997628e+01,\n",
      "        5.15121386e+01,  5.05621451e+01,  4.80488068e+01,  7.18224446e-15,\n",
      "        1.21386700e-14,  6.40136698e-16,  4.36081007e-16,  6.51236255e-15])\n",
      "```\n",
      "\n",
      "If we have one negative number in a large timeseries, the simulation will crash which seems too strict.\n",
      "\n",
      "**Expected behavior**\n",
      "\n",
      "That would be great to either:\n",
      "* Have this data check be less strict and allow for slightly negative numbers, which are not going to affect the quality of the results.\n",
      "* On `_lambertw`: Do not allow negative `v_oc` and set negative values to `np.nan`, so that the error is not triggered. It will be up to the upstream code (user) to manage those `np.nan`.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: >= 0.9.4\n",
      " - ``pandas.__version__``: 1.5.3\n",
      " - python: 3.10.11\n",
      "\n",
      "singlediode error with very low effective_irradiance\n",
      "**Describe the bug**\n",
      "\n",
      "Since pvlib 0.9.4 release (https://github.com/pvlib/pvlib-python/pull/1606) I get an error while running the single-diode model with some very low effective irradiance values.\n",
      "\n",
      "**To Reproduce**\n",
      "\n",
      "```python\n",
      "from pvlib import pvsystem\n",
      "\n",
      "effective_irradiance=1.341083e-17\n",
      "temp_cell=13.7 \n",
      "\n",
      "cec_modules = pvsystem.retrieve_sam('CECMod')\n",
      "cec_module = cec_modules['Trina_Solar_TSM_300DEG5C_07_II_']\n",
      "\n",
      "mount = pvsystem.FixedMount()\n",
      "array = pvsystem.Array(mount=mount,\n",
      "                       module_parameters=cec_module)\n",
      "\n",
      "system = pvsystem.PVSystem(arrays=[array])\n",
      "\n",
      "params = system.calcparams_cec(effective_irradiance, \n",
      "                               temp_cell)\n",
      "\n",
      "system.singlediode(*params)\n",
      "```\n",
      "\n",
      "```in _golden_sect_DataFrame(params, lower, upper, func, atol)\n",
      "    303 \"\"\"\n",
      "    304 Vectorized golden section search for finding maximum of a function of a\n",
      "    305 single variable.\n",
      "   (...)\n",
      "    342 pvlib.singlediode._pwr_optfcn\n",
      "    343 \"\"\"\n",
      "    344 if np.any(upper - lower < 0.):\n",
      "--> 345     raise ValueError('upper >= lower is required')\n",
      "    347 phim1 = (np.sqrt(5) - 1) / 2\n",
      "    349 df = params\n",
      "\n",
      "ValueError: upper >= lower is required\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "This complicates the bifacial modeling procedure as `run_model_from_effective_irradiance` can be called with very low irradiance values estimated by pvfactors (at sunrise or sunset for instance). \n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``:  0.9.4\n",
      " - ``pandas.__version__``: 1.5.3\n",
      " - python: 3.10\n",
      "\n",
      "**Additional context**\n",
      "\n",
      "v_oc is negative in this case which causes the error. \n",
      "\n",
      "```python\n",
      "from pvlib.singlediode import _lambertw_v_from_i\n",
      "photocurrent = params[0]\n",
      "saturation_current = params[1]\n",
      "resistance_series = params[2]\n",
      "resistance_shunt = params[3]\n",
      "nNsVth = params[4]\n",
      "v_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\n",
      "                              saturation_current, photocurrent)\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 360 lines----------------\n",
      "Match `pvsystem.i_from_v`, `v_from_i` single diode parameters with `singlediode` order.\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "The single diode model parameters for `i_from_v`, `v_from_i` in `pvsystem` are expected in a different order than `pvsystem.singlediode`.\n",
      "This makes it difficult to pass the parameters to all of these functions using `*args`.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "Group and reorder the single diode parameters of `i_from_v`, `v_from_i` to match the order of `singlediode`.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 76 lines----------------\n",
      "deprecate existing code in forecast.py, possibly replace with solarforecastarbiter shim\n",
      "`forecast.py` is a burden to maintain. I haven't used it in years, I don't think any of the other pvlib maintainers are interested in it, and I don't see any users stepping up to volunteer to maintain it. The code is not up to my present standards and I don't see how I'd get it there without a complete rewrite. This leads to difficult to track bugs such as the one recently reported on the [google group](https://groups.google.com/g/pvlib-python/c/b9HdgWV6w6g). It also complicates the pvlib dependencies.\n",
      "\n",
      "[solarforecastarbiter](https://github.com/SolarArbiter/solarforecastarbiter-core) includes a [reference_forecasts](https://github.com/SolarArbiter/solarforecastarbiter-core/tree/master/solarforecastarbiter/reference_forecasts) package that is much more robust. See [documentation here](https://solarforecastarbiter-core.readthedocs.io/en/latest/reference-forecasts.html) and [example notebook here](https://github.com/SolarArbiter/workshop/blob/master/reference_forecasts.ipynb) (no promises that this works without modification for the latest version).\n",
      "\n",
      "The main reason to prefer `forecast.py` to `solarforecastarbiter` is the data fetch process. `forecast.py` pulls point data from a Unidata THREDDS server. `solarforecastarbiter.reference_forecasts` assumes you already have gridded data stored in a netcdf file. `solarforecastarbiter.io.nwp` provides functions to fetch that gridded data from NCEP. We have very good reasons for that approach in `solarforecastarbiter`, but I doubt that many `forecast.py` users are interested in configuring that two step process for their application.\n",
      "\n",
      "I'm very tempted to stop here, remove `forecast.py` after deprecation, and say \"not my problem anymore\", but it seems to attract a fair number of people to pvlib, so I hesitate to remove it without some kind of replacement. Let's explore a few ideas.\n",
      "\n",
      "1. Within `forecast.py`, rewrite code to fetch relevant data from Unidata. Make this function compatible with the expectations for the [`load_forecast`](https://github.com/SolarArbiter/solarforecastarbiter-core/blob/6200ec067bf83bc198a3af59da1d924d4124d4ec/solarforecastarbiter/reference_forecasts/models.py#L16-L19) function passed into `solarforecastarbiter.reference_forecasts.models` functions.\n",
      "2. Same as 1., except put that code somewhere else. Could be a documentation example, could be in solarforecastarbiter, or could be in a gist.\n",
      "3. Copy/refactor solarforecastarbiter code into `forecast.py`.\n",
      "4. Do nothing and let the forecast.py bugs and technical debt pile up. \n",
      "\n",
      "Other thoughts?\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 248 lines----------------\n",
      "Add Wavelet Variability Model (WVM) for calculating spatial smoothing of irradiance\n",
      "> > Should I spin this off to a separate issue, since it might be different (and more compartmented) than the broader downscaling discussion?\n",
      "> \n",
      "> Yes. Let's start a new module with this submission, `scaling.py` comes to mind, but I'm not enamored of it. Scope will be functions that operate on irradiance, perhaps other variables, to transform temporal or spatial characteristics.\n",
      "\n",
      "Spinoff from [issue #788 ](https://github.com/pvlib/pvlib-python/issues/788). Implementation is a python port of WVM, released as an auxiliary to the Matlab pvlib [here](https://pvpmc.sandia.gov/applications/wavelet-variability-model/). My implementation ports the original model logic, but deviates from the overall package, in that I begin at the point where the user already has a clear sky index to operate on (original starts from GHI and calculates POA clear sky index). I thought this would allow for more flexibility in choice of transposition model, etc, but it does ask a bit more work up front for a user to run the WVM.\n",
      "\n",
      "I am close to completion of a draft and will create a pull request when ready. This is my first contribution to the project (or any open source project really), so please accept my apologies in advance if it takes some guidance.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 119 lines----------------\n",
      "document or support modules_per_string strings_per_inverter with pvwatts in modelchain\n",
      "Hi, \n",
      "\n",
      "I am trying to run Modelchain with pvwatt model but it seems that the `modules_per_string` and `strings_per inverter ` doesn't have any affect on the total output. \n",
      "\n",
      "I am not sure why is it so. \n",
      "May be ModelChain isn't supporting so. If that's the case how can I achieve the desired result?\n",
      "\n",
      "Here is my code: \n",
      "\n",
      "Thanks in advance\n",
      "```\n",
      "# built-in python modules\n",
      "import os\n",
      "import inspect\n",
      "\n",
      "# scientific python add-ons\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# plotting stuff\n",
      "# first line makes the plots appear in the notebook\n",
      "%matplotlib inline \n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "# seaborn makes your plots look better\n",
      "try:\n",
      "    import seaborn as sns\n",
      "    sns.set(rc={\"figure.figsize\": (12, 6)})\n",
      "    sns.set_color_codes()\n",
      "except ImportError:\n",
      "    print('We suggest you install seaborn using conda or pip and rerun this cell')\n",
      "\n",
      "# finally, we import the pvlib library\n",
      "import pvlib\n",
      "\n",
      "tmy = pd.read_csv(\"http://re.jrc.ec.europa.eu/pvgis5/tmy.php?lat=29.74&lon=40.10\")\n",
      "\n",
      "tmy.Date = pd.to_datetime(tmy.Date, format='%Y-%d-%m %H:%M:%S')\n",
      "\n",
      "tmy.rename(columns={' Ghor':'ghi','Dhor':'dhi','DNI':'dni','Tair':'temp_air',\n",
      "                        'Ws':'wind_speed'},inplace=True)\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "tmy.set_index(tmy['Date'],inplace=True)\n",
      "    #Drop unnecessary column\n",
      "\n",
      "tmy = tmy.drop('Date', 1)\n",
      "tmy = tmy.drop('RH', 1)\n",
      "tmy = tmy.drop('IR', 1)\n",
      "tmy = tmy.drop(' Wd', 1)\n",
      "tmy = tmy.drop('Pres', 1)\n",
      "\n",
      "#module =Jinko_Solar_JKM320P_72_V\n",
      "#inverter = ABB__PVS980_58_2000kVA_K__N_A_V__CEC_2018_\n",
      "\n",
      "lat = 29.74\n",
      "lon = 40.10\n",
      "altitude = 676\n",
      "tz = 'Etc/GMT+3'  \n",
      "\n",
      "loc = pvlib.location.Location(latitude=lat,longitude= lon,tz=tz)\n",
      "\n",
      "#model = pvwatts \n",
      "pvwatts_system = pvlib.pvsystem.PVSystem(module_parameters={'pdc0': 320, 'gamma_pdc': -0.0041},inverter_parameters={'pdc' : 3200000, 'pdc0' : 2024292, 'eta_inv_nom':0.988, 'eta_inv_ref':0.986},surface_tilt = 20, surface_azimuth=0,\n",
      "                    modules_per_string=30,strings_per_inverter=267, albedo = 0.2)\n",
      "\t\t\t\t\t\n",
      "mc = pvlib.modelchain.ModelChain(pvwatts_system, loc, transposition_model =\"perez\",aoi_model = 'ashrae',spectral_model='no_loss')\n",
      "print(mc)\n",
      "mc.run_model(times=tmy.index,weather=tmy)\n",
      "a = mc.ac\n",
      "a = pd.Series.to_frame(a)\n",
      "a = a * 530  # 530 = number of inverters in the system \n",
      "\n",
      "a['month'] = a.index\n",
      "a.month = a.month.dt.month\n",
      "monthly = a.groupby('month').sum()\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 146 lines----------------\n",
      "Bug within scaling.py wavelet calculation methodology\n",
      "**Describe the bug**\n",
      "Mathematical error within the wavelet computation for the scaling.py WVM implementation. Error arises from the methodology, as opposed to just a software bug. \n",
      "\n",
      "**To Reproduce**\n",
      "Steps to reproduce the behavior:\n",
      "```\n",
      "import numpy as np\n",
      "from pvlib import scaling\n",
      "cs = np.random.rand(2**14)\n",
      "w, ts = scaling._compute_wavelet(cs,1)\n",
      "print(np.all( (sum(w)-cs) < 1e-8 ))  # Returns False, expect True\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "For a discrete wavelet transform (DWT) the sum of all wavelet modes should equate to the original data. \n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.7.2\n",
      " - ``pandas.__version__``: 1.2.3\n",
      " - python: 3.8.8\n",
      "\n",
      "**Additional context**\n",
      "This bug is also present in the [PV_LIB](https://pvpmc.sandia.gov/applications/wavelet-variability-model/) Matlab version that was used as the basis for this code (I did reach out to them using the PVLIB MATLAB email form, but don't know who actually wrote that code). Essentially, the existing code throws away the highest level of Detail Coefficient in the transform and keeps an extra level of Approximation coefficient. The impact on the calculation is small, but leads to an incorrect DWT and reconstruction. I have a fix that makes the code pass the theoretical test about the DWT proposed under 'To Reproduce' but there may be some question as to whether this should be corrected or left alone to match the MATLAB code it was based on. \n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 44 lines----------------\n",
      "regression: iam.physical returns nan for aoi > 90Â° when n = 1\n",
      "**Describe the bug**\n",
      "For pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90Â°, we get nan as result.\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "import pvlib\n",
      "pvlib.iam.physical(aoi=100, n=1)\n",
      "```\n",
      "returns `nan`.\n",
      "\n",
      "**Expected behavior**\n",
      "The result should be `0`, as it was for pvlib <= 0.9.4.\n",
      "\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: '0.9.5'\n",
      " - ``pandas.__version__``:  '1.5.3'\n",
      " - python: 3.10.4\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 73 lines----------------\n",
      "Add retrieval function for horizon profile from MINES Paris Tech\n",
      "<!-- Thank you for your contribution! The following items must be addressed before the code can be merged. Please don't hesitate to ask for help if you're unsure of how to accomplish any of the items. Feel free to remove checklist items that are not relevant to your change. -->\n",
      "\n",
      " - [x] I am familiar with the [contributing guidelines](https://pvlib-python.readthedocs.io/en/latest/contributing.html)\n",
      " - [x] Tests added\n",
      " - [x] Updates entries to [`docs/sphinx/source/api.rst`](https://github.com/pvlib/pvlib-python/blob/master/docs/sphinx/source/api.rst) for API changes.\n",
      " - [x] Adds description and name entries in the appropriate \"what's new\" file in [`docs/sphinx/source/whatsnew`](https://github.com/pvlib/pvlib-python/tree/master/docs/sphinx/source/whatsnew) for all changes. Includes link to the GitHub Issue with `` :issue:`num` `` or this Pull Request with `` :pull:`num` ``. Includes contributor name and/or GitHub username (link with `` :ghuser:`user` ``).\n",
      " - [x] New code is fully documented. Includes [numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html) compliant docstrings, examples, and comments where necessary.\n",
      " - [x] Pull request is nearly complete and ready for detailed review.\n",
      " - [x] Maintainer: Appropriate GitHub Labels and Milestone are assigned to the Pull Request and linked Issue.\n",
      "\n",
      "<!-- Brief description of the problem and proposed solution (if not already fully described in the issue linked to above): -->\n",
      "\n",
      "The proposed function retrieves the local horizon profile for a specific location (latitude, longitude, and elevation). The returned horizon profile has a resolution of 1 degree in the azimuth direction. The service is provided by MINES ParisTech though I cannot find any official documentation for it.\n",
      "\n",
      "The function added in this PR (``pvlib.iotools.get_mines_horizon``) is very similar to the function added in #1395 (``pvlib.iotools.get_pvgis_horizon``).\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 15 lines----------------\n",
      "`pvsystem.calcparams_cec()` does not propagate parameters\n",
      "**Describe the bug**\n",
      "\n",
      "The function calls `calcparams_desoto` with hardcoded reference values.\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/40ba4bd5c8b91754aa73e638ed984ab9657847cd/pvlib/pvsystem.py#L1850-L1855\n",
      "\n",
      "This means the function is silently ignoring its inputs, yielding incorrect results that may go unnoticed.\n",
      "\n",
      "\n",
      "**Expected behavior**\n",
      "\n",
      "The function parameters are propagated into the `calcparams_desoto` call. In particular: `EgRef`, `dEgdT`, `irrad_ref`, `temp_ref`\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 14 lines----------------\n",
      "irradiance.aoi can return NaN when module orientation is perfectly aligned with solar position\n",
      "**Describe the bug**\n",
      "I was playing with a dual-axis tracking mount with #1176 and found that when the modules are perfectly aligned with the sun (i.e. AOI should be exactly zero), floating point round-off can result in aoi projection values slightly greater than one, resulting in NaN aoi.  This only happens for some perfectly-aligned inputs (for example tilt=zenith=20, azimuth=180 returns aoi=0 as expected).\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "import pvlib\n",
      "zenith = 89.26778228223463\n",
      "azimuth = 60.932028605997004\n",
      "print(pvlib.irradiance.aoi_projection(zenith, azimuth, zenith, azimuth))\n",
      "print(pvlib.irradiance.aoi(zenith, azimuth, zenith, azimuth))\n",
      "\n",
      "# output:\n",
      "1.0000000000000002\n",
      "RuntimeWarning: invalid value encountered in arccos:  aoi_value = np.rad2deg(np.arccos(projection))\n",
      "nan\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "I expect aoi=0 whenever module orientation and solar position angles are identical.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: `0.9.0-alpha.4+14.g61650e9`\n",
      " - ``pandas.__version__``: `0.25.1`\n",
      " - ``numpy.__version__``: `1.17.0`\n",
      " - python: `3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]`\n",
      "\n",
      "**Additional context**\n",
      "Some ideas for fixes:\n",
      "1) In `irradiance.aoi_projection`, return a hard-coded `1.0` for inputs within some small tolerance\n",
      "2) In `irradiance.aoi_projection`, clamp return value to `[-1, +1]`\n",
      "3) In `irradiance.aoi`, clamp aoi_projection values to `[-1, +1`] before calling `arccos`\n",
      "4) Rework the `irradiance.aoi_projection` trig equations to not generate impossible values?\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 677 lines----------------\n",
      "backtracking for rare case when sun below tracker improvement\n",
      "**Describe the bug**\n",
      "- related to #656\n",
      "- in the rare case when the sun rays are below the tracker, then the top of the next row is shaded\n",
      "- currently tracker backtracks away from sun, back is facing sun instead of front\n",
      "- this only happens for tilted trackers and very low sun angles, either early morning or late evening when the sun rays are furthest north or south\n",
      "\n",
      "**To Reproduce**\n",
      "Steps to reproduce the behavior:\n",
      "1. create a tilted tracker\n",
      "```python\n",
      "# in Brazil so facing north\n",
      "axis_azimuth = 0.0 \n",
      "axis_tilt = 20\n",
      "max_angle = 75.0\n",
      "gcr = 0.35\n",
      "```\n",
      "2. pick the earliest morning (or latest evening) timestamp\n",
      "```python\n",
      "import pvlib\n",
      "import pandas as pd\n",
      "\n",
      "# Brazil, timezone is UTC-3[hrs]\n",
      "starttime = '2017-01-01T00:30:00-0300'\n",
      "stoptime = '2017-12-31T23:59:59-0300'\n",
      "lat, lon = -27.597300, -48.549610\n",
      "times = pd.DatetimeIndex(pd.date_range(\n",
      "    starttime, stoptime, freq='H'))\n",
      "solpos = pvlib.solarposition.get_solarposition(\n",
      "    times, lat, lon)\n",
      "# get the early times\n",
      "ts0 = '2017-01-01 05:30:00-03:00'\n",
      "ts1 = '2017-01-01 12:30:00-03:00'\n",
      "apparent_zenith = solpos['apparent_zenith'][ts0:ts1]\n",
      "azimuth = solpos['azimuth'][ts0:ts1]\n",
      "sat = pvlib.tracking.singleaxis(\n",
      "    apparent_zenith, azimuth, axis_tilt, axis_azimuth, max_angle, True, gcr)\n",
      "```\n",
      "3. notice that the tracker suddenly jumps from one side facing east to west\n",
      "```\n",
      "                           tracker_theta        aoi  surface_azimuth  surface_tilt\n",
      "2017-01-01 05:30:00-03:00     -21.964540  62.721237       310.299287     29.368272\n",
      "2017-01-01 06:30:00-03:00      16.231156  69.264752        40.403367     25.546154\n",
      "2017-01-01 07:30:00-03:00      69.073645  20.433849        82.548858     70.389280\n",
      "2017-01-01 08:30:00-03:00      54.554616  18.683626        76.316479     56.978562\n",
      "2017-01-01 09:30:00-03:00      40.131687  17.224233        67.917292     44.072837\n",
      "2017-01-01 10:30:00-03:00      25.769332  16.144347        54.683567     32.194782\n",
      "2017-01-01 11:30:00-03:00      11.439675  15.509532        30.610665     22.923644\n",
      "2017-01-01 12:30:00-03:00      -2.877428  15.358209       351.639727     20.197537\n",
      "```\n",
      "\n",
      "4. AOI is also wrong\n",
      "\n",
      "**Expected behavior**\n",
      "The tracker should avoid shade. It should not jump from one direction to the other. If the sun ray is below the tracker then it will need to track to it's max rotation or backtrack. If there is shading at it's max rotation then it should track backtrack to zero, or perhaps parallel to the sun rays. Perhaps if bifacial, then it could go backwards, 180 from the correct backtrack position to show it's backside to the sun.\n",
      "\n",
      "proposed algorithm (_updated after [this comment](#issuecomment-559154895)_):\n",
      "```python\n",
      "if backtracking:\n",
      "    # cos(R) = L / Lx, R is rotation, L is surface length,\n",
      "    # Lx is shadow on ground, tracker shades when Lx > x\n",
      "    # x is row spacing related to GCR, x = L/GCR\n",
      "    lrot = np.cos(tr_rot_no_lim)  # tracker rotation not limited by max angle\n",
      "\n",
      "    # Note: if tr_rot > 90[deg] then lrot < 0 \n",
      "    # which *can* happen at low angles if axis tilt > 0\n",
      "    # tracker should never backtrack more than 90[deg], when lrot = 0\n",
      "    cos_rot = np.minimum(np.abs(lrot) / self.gcr, 1)\n",
      "\n",
      "    # so if lrot<0 tracker should backtrack forward\n",
      "    # backtrack_rot = np.sign(lrot) * np.arccos(cos_rot)\n",
      "\n",
      "    # NOTE: updated after comment from @kevinsa5 at Nov 27, 2019, 8:16 AM PST\n",
      "    # to remove sign()\n",
      "    backtrack_rot = np.arccos(cos_rot)\n",
      "```\n",
      "\n",
      "also remove abs from aoi calculation\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/c699575cb6857674f0a96348b77e10c805e741c7/pvlib/tracking.py#L461\n",
      "\n",
      "**Screenshots**\n",
      "If applicable, add screenshots to help explain your problem.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``:  0.6.3\n",
      " - ``pandas.__version__``:  0.24\n",
      " - python: 3.7\n",
      "\n",
      "**Additional context**\n",
      "Add any other context about the problem here.\n",
      "\n",
      "[STY] remove old comments from single axis tracking\n",
      "**Describe the bug**\n",
      "After #823 is merged there may be stale comments in `pvlib.tracking.singleaxis` and commented code that can be removed. This might make the code more readable. It would also resolve some stickler complaints about long lines.\n",
      "\n",
      "**To Reproduce**\n",
      "Comments to remove:\n",
      "1. [L375-L379](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L375-L379) - the tracking algorithm now follows [1] that uses clockwise rotation around z-axis from north\n",
      "2. [L393-L395](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L393-L395) - ditto\n",
      "3. [L400-L410](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L400-L410) - ditto\n",
      "4. [L441-L452](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L441-L452) - pvlib has been using arctan2(x,z) in `pvlib.tracking.singleaxis` for 6 years since 1fb82cc262e43e1d2b55e4b5510a1a5e7e340667, so I believe these comments are unnecessary now\n",
      "5. [L471-L472](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L471-L472) - this commented code was updated in #823, should we leave it or delete it?\n",
      "3. [L553-L555](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L553-L555)\n",
      "\n",
      "etc.\n",
      "\n",
      "[1] https://www.nrel.gov/docs/fy20osti/76626.pdf\n",
      "\n",
      "**Expected behavior**\n",
      "A clear and concise description of what you expected to happen.\n",
      "\n",
      "**Screenshots**\n",
      "If applicable, add screenshots to help explain your problem.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: \n",
      " - ``pandas.__version__``: \n",
      " - python:\n",
      "\n",
      "**Additional context**\n",
      "Add any other context about the problem here.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 139 lines----------------\n",
      "`read_crn` returns -99999 instead of `NaN`\n",
      "**Describe the bug**\n",
      "`read_crn` fails to map -99999 to `NaN`\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "from pvlib.iotools import read_crn\n",
      "crn = read_crn('https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/2021/CRNS0101-05-2021-NY_Millbrook_3_W.txt')\n",
      "crn.loc['2021-12-14 0930':'2021-12-14 1130', 'ghi']\n",
      "2021-12-14 09:30:00+00:00        0.0\n",
      "2021-12-14 09:35:00+00:00        0.0\n",
      "2021-12-14 09:40:00+00:00        0.0\n",
      "2021-12-14 09:45:00+00:00        0.0\n",
      "2021-12-14 09:50:00+00:00        0.0\n",
      "2021-12-14 09:55:00+00:00        0.0\n",
      "2021-12-14 10:00:00+00:00        0.0\n",
      "2021-12-14 10:05:00+00:00   -99999.0\n",
      "2021-12-14 10:10:00+00:00   -99999.0\n",
      "2021-12-14 10:15:00+00:00   -99999.0\n",
      "2021-12-14 10:20:00+00:00   -99999.0\n",
      "2021-12-14 10:25:00+00:00   -99999.0\n",
      "2021-12-14 10:30:00+00:00   -99999.0\n",
      "2021-12-14 10:35:00+00:00   -99999.0\n",
      "2021-12-14 10:40:00+00:00   -99999.0\n",
      "2021-12-14 10:45:00+00:00   -99999.0\n",
      "2021-12-14 10:50:00+00:00   -99999.0\n",
      "2021-12-14 10:55:00+00:00   -99999.0\n",
      "2021-12-14 11:00:00+00:00   -99999.0\n",
      "2021-12-14 11:05:00+00:00        0.0\n",
      "2021-12-14 11:10:00+00:00        0.0\n",
      "2021-12-14 11:15:00+00:00        0.0\n",
      "2021-12-14 11:20:00+00:00        0.0\n",
      "2021-12-14 11:25:00+00:00        0.0\n",
      "2021-12-14 11:30:00+00:00        0.0\n",
      "Name: ghi, dtype: float64\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "Should return `NaN` instead of -99999\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.9.0\n",
      " - ``pandas.__version__``: 1.0.3 (doesn't matter)\n",
      " - python: 3.7\n",
      "\n",
      "**Additional context**\n",
      "\n",
      "Documentation [here](https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/) says\n",
      "\n",
      ">          C.  Missing data are indicated by the lowest possible integer for a \n",
      ">             given column format, such as -9999.0 for 7-character fields with \n",
      ">             one decimal place or -99.000 for 7-character fields with three\n",
      ">             decimal places.\n",
      "\n",
      "So we should change \n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/1ab0eb20f9cd9fb9f7a0ddf35f81283f2648e34a/pvlib/iotools/crn.py#L112-L117\n",
      "\n",
      "to include -99999 and perhaps -999999. Or do the smarter thing as discussed in the comment.\n",
      "\n",
      "also https://github.com/SolarArbiter/solarforecastarbiter-core/issues/773\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 90 lines----------------\n",
      "Output of solarposition.sun_rise_set_transit_ephem depends on installed ephem version\n",
      "**Describe the bug**\n",
      "`pvlib.solarposition.sun_rise_set_transit_ephem` returns a different answer depending on what version of `ephem` is installed. I think the problem is that our wrapper assumes that ephem doesn't pay attention to timezone localization, so it converts the timestamp components to UTC but doesn't bother to change the timestamp's tzinfo:\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/8d0f863da92739669e01ac4da4145e4547638b50/pvlib/solarposition.py#L577-L579\n",
      "\n",
      "However, starting in `ephem==4.1.1` the timezone is no longer ignored ([ref](https://rhodesmill.org/pyephem/CHANGELOG.html#version-4-1-1-2021-november-27)), so the UTC offset is applied twice.  This can shift the timestamp into the next solar period and return the rise/set/transit for the wrong day. \n",
      "\n",
      "\n",
      "**To Reproduce**\n",
      "See how the returned sunrise differs by ~24 hours (2019-01-01 vs 2019-01-02) here:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import pvlib\n",
      "times = pd.date_range('2019-01-01', freq='h', periods=1, tz='Etc/GMT+8')\n",
      "out = pvlib.solarposition.sun_rise_set_transit_ephem(times, 40, -120)\n",
      "print(out.T)\n",
      "```\n",
      "\n",
      "#### `ephem==4.0.0.1`:\n",
      "```\n",
      "\n",
      "               2019-01-01 00:00:00-08:00\n",
      "sunrise 2019-01-01 07:21:28.793036-08:00\n",
      "sunset  2019-01-01 16:45:50.959086-08:00\n",
      "transit 2019-01-01 12:03:35.730674-08:00\n",
      "```\n",
      "\n",
      "#### `ephem==4.1.2`:\n",
      "```\n",
      "               2019-01-01 00:00:00-08:00\n",
      "sunrise 2019-01-02 07:21:35.237404-08:00\n",
      "sunset  2019-01-01 16:45:50.947472-08:00\n",
      "transit 2019-01-01 12:03:35.728413-08:00\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "pvlib should give ephem timestamps consistent with its input requirements.  Replacing the above manual utc offset subtraction (which leaves the original tzinfo in place) with `thetime.astimezone(pytz.UTC)` may be suitable for both old and new versions of ephem.  I don't ever use pytz and python datetimes so maybe there's a better alternative.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.9.1\n",
      " - python: 3.7\n",
      "\n",
      "**Additional context**\n",
      "This difference would have popped up back in November when ephem 4.1.1 was released had it not been for #1447.  Here's an example failure: https://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=6027&view=logs&j=e1592cb8-2816-5754-b393-3839a187d454&t=377c4fd6-97bd-5996-bc02-4d072a8786ea&l=2267\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 29 lines----------------\n",
      "ZeroDivisionError when gcr is zero\n",
      "**Describe the bug**\n",
      "\n",
      "Though maybe not intuitive, setting ground coverage ratio to zero is useful when a plant consists of a single shed, e.g. calculating the irradiance on the backside of the panels. However, e.g., `bifacial.infinite_sheds.get_irradiance_poa` fails with `ZeroDivisionError` whenever `gcr=0`.\n",
      "\n",
      "**To Reproduce**\n",
      "\n",
      "```python\n",
      "from pvlib.bifacial.infinite_sheds import get_irradiance_poa\n",
      "\n",
      "get_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1000, ghi=200, dhi=200, dni=0, albedo=0.2)\n",
      "```\n",
      "returns:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-0cb583b2b311>\", line 3, in <cell line: 3>\n",
      "    get_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1, ghi=200, dhi=200, dni=0, albedo=0.2)\n",
      "  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 522, in get_irradiance_poa\n",
      "    vf_shade_sky, vf_noshade_sky = _vf_row_sky_integ(\n",
      "  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 145, in _vf_row_sky_integ\n",
      "    psi_t_shaded = masking_angle(surface_tilt, gcr, x)\n",
      "  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\shading.py\", line 56, in masking_angle\n",
      "    denominator = 1/gcr - (1 - slant_height) * cosd(surface_tilt)\n",
      "ZeroDivisionError: division by zero\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "\n",
      "One can easily solve this `ZeroDivisionError` by multiplying both numerator and denominator with `gcr` inside `shading.masking_angle` and the same inside `bifacial.infinite_sheds._ground_angle`.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: '0.9.3'\n",
      " - ``pandas.__version__``: '1.4.4'\n",
      " - python: '3.10.4'\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 40 lines----------------\n",
      "expand PVSystem repr\n",
      "The PVSystem repr is\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L239-L243\n",
      "\n",
      "The main issue that I have is that the repr doesn't give me enough information about the temperature model settings. It's relatively important because `temperature_model_params` (not printed) may be inferred from `module_type` (not printed) and `racking_model` (printed). So I'd like to add both `temperature_model_params` and `module_type`.\n",
      "\n",
      "We also don't include `module_parameters`, `inverter_parameters`, and `losses_parameters` in the repr. If I recall correctly, we decided against including these because they can be relatively long. I still think that's reasonable. We could add something like `if len(module_parameters): 'Set. See PVSystem.module_parameters'; else: {}`, but I don't know if that's worth the effort.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 37 lines----------------\n",
      "`spectrum.spectrl2` calculates negative irradiance for angle of incidence outside +/- 90Â°\n",
      "When using pvlib (but also the spectrl2 implementation provided by NREL), I obtain negative Irradiance for a north-facing panel.\n",
      "From @kevinsa5 's [reply on StackOverflow](https://stackoverflow.com/questions/70172766/pvlib-bird1984-north-facing-element-shows-negative-irradiance/70174010#70174010) I take that this is in fact not intended.\n",
      "\n",
      "In the example code below, the angle of incidence is calculated as values around 115Â°, so exceeding a possible (implicitly assumed) +/- 90Â° bound (sun behind panel).\n",
      "\n",
      "This seems to be left open in the original report ([Bird & Riordan, 1984](https://www.nrel.gov/docs/legosti/old/2436.pdf)).\n",
      "\n",
      "The direct irradiance `I_d` (*of a horizontal panel*, Eq 2-1) is obtained by multiplying by cosine of the sun zenith angle. I'd guess that setting that value strictly to zero for angles when cosZ is negative would not be too much of a stretch.\n",
      "\n",
      "Then, the direct irradiance `I_d` goes into (Eq 3-18):\n",
      "\n",
      "```\n",
      "I_T(t) = I_d*cos(aoi) + I_s * ( (I_d*cos(aoi) / (H_0*D*cos(Z)) ) + 0.5*(1+cos(t)) * (1 - I_d/(H_0*D)) + 0.5 * I_T0 * r_g * (1-cos(t))\n",
      "```\n",
      "\n",
      "As such, when you view the angle of incidence `aoi` as the analogue of the sun zenith angle in the prior example, the two first terms of the diffuse irradiation (Eq 3-18) would become zero, which - again - for the direct irradiance would kind of make sense. What remains of (Eq 3-18) would be\n",
      "\n",
      "```\n",
      "I_T(t) = 0 + 0 + 0.5*(1+cos(t))*(1 - 0) + 0.5*I_T0*r_g*(1-cos(t))\n",
      "```\n",
      "\n",
      "I'm not from the field, so I'm very, very wary about the implications of such a work-around suggestion. Can anyone with a proper background comment on this? (Maybe it's the future of air conditioning :-D)\n",
      "\n",
      "\n",
      "**MWE based on the tutorial below**\n",
      "\n",
      "```python\n",
      "## Using PV Lib\n",
      "\n",
      "from pvlib import spectrum, solarposition, irradiance, atmosphere\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# assumptions from the technical report:\n",
      "lat = 49.88\n",
      "lon = 8.63\n",
      "tilt = 45\n",
      "azimuth = 0 # North = 0\n",
      "pressure = 101300  # sea level, roughly\n",
      "water_vapor_content = 0.5  # cm\n",
      "tau500 = 0.1\n",
      "ozone = 0.31  # atm-cm\n",
      "albedo = 0.2\n",
      "\n",
      "times = pd.date_range('2021-11-30 8:00', freq='h', periods=6, tz=\"Europe/Berlin\") # , tz='Etc/GMT+9'\n",
      "solpos = solarposition.get_solarposition(times, lat, lon)\n",
      "aoi = irradiance.aoi(tilt, azimuth, solpos.apparent_zenith, solpos.azimuth)\n",
      "\n",
      "# The technical report uses the 'kasten1966' airmass model, but later\n",
      "# versions of SPECTRL2 use 'kastenyoung1989'.  Here we use 'kasten1966'\n",
      "# for consistency with the technical report.\n",
      "relative_airmass = atmosphere.get_relative_airmass(solpos.apparent_zenith,\n",
      "                                                   model='kasten1966')\n",
      "\n",
      "spectra = spectrum.spectrl2(\n",
      "    apparent_zenith=solpos.apparent_zenith,\n",
      "    aoi=aoi,\n",
      "    surface_tilt=tilt,\n",
      "    ground_albedo=albedo,\n",
      "    surface_pressure=pressure,\n",
      "    relative_airmass=relative_airmass,\n",
      "    precipitable_water=water_vapor_content,\n",
      "    ozone=ozone,\n",
      "    aerosol_turbidity_500nm=tau500,\n",
      ")\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(spectra['wavelength'], spectra['poa_global'])\n",
      "plt.xlim(200, 2700)\n",
      "# plt.ylim(0, 1.8)\n",
      "plt.title(r\"2021-11-30, Darmstadt, $\\tau=0.1$, Wv=0.5 cm\")\n",
      "plt.ylabel(r\"Irradiance ($W m^{-2} nm^{-1}$)\")\n",
      "plt.xlabel(r\"Wavelength ($nm$)\")\n",
      "time_labels = times.strftime(\"%H:%M %p\")\n",
      "labels = [\n",
      "    \"AM {:0.02f}, Z{:0.02f}, {}\".format(*vals)\n",
      "    for vals in zip(relative_airmass, solpos.apparent_zenith, time_labels)\n",
      "]\n",
      "plt.legend(labels)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "![Figure_ne](https://user-images.githubusercontent.com/15192310/144224709-dea899e4-435e-4ff2-a3de-9e9524b28eb8.png)\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 35 lines----------------\n",
      "pvlib.soiling.hsu model implementation errors\n",
      "**Describe the bug**\n",
      "I ran an example run using the Matlab version of the HSU soiling function and found that the python version did not give anywhere near the same results.  The Matlab results matched the results in the original JPV paper.  As a result of this test, I found two errors in the python implementation, which are listed below:\n",
      "\n",
      "1.  depo_veloc = {'2_5': 0.004, '10': 0.0009} has the wrong default values.  They are reversed.\n",
      "The proper dictionary should be: {'2_5': 0.0009, '10': 0.004}.  This is confirmed in the JPV paper and the Matlab version of the function.\n",
      "\n",
      "2. The horiz_mass_rate is in g/(m^2*hr) but should be in g/(m^2*s).  The line needs to be multiplied by 60x60 or 3600.\n",
      "The proper line of code should be: \n",
      "horiz_mass_rate = (pm2_5 * depo_veloc['2_5']+ np.maximum(pm10 - pm2_5, 0.) * depo_veloc['10'])*3600\n",
      "\n",
      "When I made these changes I was able to match the validation dataset from the JPV paper, as shown below.\n",
      "![image](https://user-images.githubusercontent.com/5392756/82380831-61c43d80-99e6-11ea-9ee3-2368fa71e580.png)\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 167 lines----------------\n",
      "warnings in test_sdm\n",
      "https://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=209\n",
      "\n",
      "through\n",
      "\n",
      "https://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=295\n",
      "\n",
      "So almost 100 lines of warnings.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 436 lines----------------\n",
      "Add variable mapping of read_tmy3\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "This PR proposes that a `map_variables` parameter be added to the `read_tmy3` function. Additionally, the current `rename_columns` parameter (which removes the units from the column names) should be deprecated. See #714 for a discussion on the topic.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "A `map_variables` parameter should be added (defaulting to None), and if specified as True then it should override the `rename_columns` parameter and map the column names to standard pvlib names. A deperecation warning should be added stating that the `rename_columns` parameter will be retired starting in pvlib 0.11.0 - the deprecation warning should be silenced if `map_variables` is specified as either True or False.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 15 lines----------------\n",
      "Incorrect AOI from pvlib.tracking.singleaxis\n",
      "`pvlib.tracking.singleaxis` produces an incorrect AOI when the sun is above the earth horizon but behind the module plane.\n",
      "\n",
      "**To Reproduce**\n",
      "Model a fixed tilt system (90 tilt, 180 azimuth) and compare to a vertical single axis tracker with very small rotation limit.\n",
      "\n",
      "```\n",
      "\n",
      "import pandas as pd\n",
      "import pytz\n",
      "import pvlib\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "loc = pvlib.location.Location(40.1134, -88.3695)\n",
      "\n",
      "dr = pd.date_range(start='02-Jun-1998 00:00:00', end='02-Jun-1998 23:55:00',\n",
      "                   freq='5T')\n",
      "tz = pytz.timezone('Etc/GMT+6')\n",
      "dr = dr.tz_localize(tz)\n",
      "hr = dr.hour + dr.minute/60\n",
      "\n",
      "sp = loc.get_solarposition(dr)\n",
      "\n",
      "cs = loc.get_clearsky(dr)\n",
      "\n",
      "tr = pvlib.tracking.singleaxis(sp['apparent_zenith'], sp['azimuth'],\n",
      "                               axis_tilt=90, axis_azimuth=180, max_angle=0.01,\n",
      "                               backtrack=False)\n",
      "\n",
      "fixed = pvlib.irradiance.aoi(90, 180, sp['apparent_zenith'], sp['azimuth'])\n",
      "\n",
      "plt.plot(hr, fixed)\n",
      "plt.plot(hr, tr['aoi'])\n",
      "plt.plot(hr, sp[['apparent_elevation']])\n",
      "plt.show()\n",
      "\n",
      "plt.legend(['aoi - fixed', 'aoi - tracked', 'apparent_elevation'])\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "The AOI for the fixed tilt system shows values greater than 90 when the sun is behind the module plane. The AOI from `singleaxis` does not.\n",
      "\n",
      "I think the source of the error is the use of `abs` in [this ](https://github.com/pvlib/pvlib-python/blob/ca61503fa83e76631f84fb4237d9e11ae99f3c77/pvlib/tracking.py#L446)line.\n",
      "\n",
      "**Screenshots**\n",
      "![aoi_fixed_vs_tracked](https://user-images.githubusercontent.com/5393711/117505270-01087a80-af41-11eb-9220-10cccf2714e1.png)\n",
      "\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.8.1\n",
      "\n",
      "First reported by email from Jim Wilzcak (NOAA) for the PVlib Matlab function [pvl_singleaxis.m](https://github.com/sandialabs/MATLAB_PV_LIB/blob/master/pvl_singleaxis.m)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 362 lines----------------\n",
      "getter/parser for PVGIS hourly-radiation\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "Related to #845 \n",
      "\n",
      "**Describe the solution you'd like**\n",
      "Similar to `get_pvgis_tmy` retrieve pvgis hourly radiation data from their api\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "Pvgis is becoming a popular resource more and more people are asking me for it, it is nice because it is a global collection of several different radiation databases including nsrdb and others, and different from cams, the data is complete, ie it has air temperature, wind speed as well as all 3 components of irradiance\n",
      "\n",
      "**Additional context**\n",
      "This would be part of the `iotool` sub-package. There's already a `pvgis.py` module with a getter for tmy be data\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 250 lines----------------\n",
      "singlediode: newton solver fails with Series input of length one\n",
      "The vectorized newton solver doesn't work if parameters are Series of length one.\n",
      "\n",
      "```\n",
      "\n",
      "import pandas as pd\n",
      "import pvlib\n",
      "\n",
      "\n",
      "args = (0.001, 1.5, 6., 5e-9, 1000., 0.5)\n",
      "params = pvlib.pvsystem.calcparams_desoto(1000., 25, *args)\n",
      "params_series = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000.]),\n",
      "                                                 pd.Series([25.]), *args)\n",
      "params_series2 = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000., 1000.]),\n",
      "                                                  pd.Series([25., 25.]), *args)\n",
      "# works with each input as float\n",
      "result = pvlib.pvsystem.singlediode(*params, method='newton')\n",
      "\n",
      "# works with Series if length > 1\n",
      "result_series2 = pvlib.pvsystem.singlediode(*params_series2, method='newton')\n",
      "\n",
      "# errors with Series if length is 1\n",
      "result_series = pvlib.pvsystem.singlediode(*params_series, method='newton')\n",
      "```\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.9.5\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 54 lines----------------\n",
      "ValueError: ModelChain.run_from_effective_irradiance([weather]) when only providing temp_air and wind_speed\n",
      "**Describe the bug**\n",
      "According to the (new) docstring for `ModelChain.run_from_effective_irradiance`, cell temperature can be calculated from temperature_model using `'effective_irradiance'`. This is not the case when using one or more arrays \n",
      "https://github.com/pvlib/pvlib-python/blame/master/pvlib/modelchain.py#L1589-L1606\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "from copy import deepcopy\n",
      "import pandas as pd\n",
      "from pvlib.location import Location\n",
      "from pvlib.pvsystem import Array, PVSystem\n",
      "from pvlib.modelchain import ModelChain\n",
      "\n",
      "\n",
      "array_params = {\n",
      "    \"surface_tilt\": 32.0,\n",
      "    \"surface_azimuth\": 180.0,\n",
      "    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\n",
      "    \"albedo\": 0.2,\n",
      "    \"temperature_model_parameters\": {\n",
      "        \"u_c\": 29.0,\n",
      "        \"u_v\": 0.0,\n",
      "        \"eta_m\": 0.1,\n",
      "        \"alpha_absorption\": 0.9,\n",
      "    },\n",
      "    \"strings\": 5,\n",
      "    \"modules_per_string\": 7,\n",
      "    \"module_parameters\": {\n",
      "        \"alpha_sc\": 0.004539,\n",
      "        \"gamma_ref\": 1.2,\n",
      "        \"mu_gamma\": -0.003,\n",
      "        \"I_L_ref\": 5.11426,\n",
      "        \"I_o_ref\": 8.10251e-10,\n",
      "        \"R_sh_ref\": 381.254,\n",
      "        \"R_sh_0\": 400.0,\n",
      "        \"R_s\": 1.06602,\n",
      "        \"cells_in_series\": 96,\n",
      "        \"R_sh_exp\": 5.5,\n",
      "        \"EgRef\": 1.121,\n",
      "    },\n",
      "}\n",
      "inverter_parameters = {\n",
      "    \"Paco\": 250.0,\n",
      "    \"Pdco\": 259.589,\n",
      "    \"Vdco\": 40.0,\n",
      "    \"Pso\": 2.08961,\n",
      "    \"C0\": -4.1e-05,\n",
      "    \"C1\": -9.1e-05,\n",
      "    \"C2\": 0.000494,\n",
      "    \"C3\": -0.013171,\n",
      "    \"Pnt\": 0.075,\n",
      "}\n",
      "\n",
      "\n",
      "location = Location(latitude=33.98, longitude=-115.323, altitude=2300)\n",
      "\n",
      "array_sys = PVSystem(\n",
      "    arrays=[\n",
      "        Array(**array_params, name=0),\n",
      "    ],\n",
      "    inverter_parameters=inverter_parameters,\n",
      ")\n",
      "weather = pd.DataFrame(\n",
      "    {\n",
      "        \"effective_irradiance\": [1100.0, 1101.0],\n",
      "        \"temp_air\": [25.0, 26.0],\n",
      "        \"wind_speed\": [10.0, 10.0],\n",
      "    },\n",
      "    index=pd.DatetimeIndex(\n",
      "        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\n",
      "    ),\n",
      ")\n",
      "mc0 = ModelChain(\n",
      "    array_sys,\n",
      "    location,\n",
      "    aoi_model=\"no_loss\",\n",
      "    spectral_model=\"no_loss\",\n",
      ")\n",
      "mc1 = deepcopy(mc0)\n",
      "\n",
      "mc0.run_model_from_effective_irradiance(weather)\n",
      "assert isinstance(mc0.results.cell_temperature, pd.Series)\n",
      "\n",
      "\n",
      "mc1.run_model_from_effective_irradiance([weather])  # ValueError\n",
      "\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "Running the model with both `weather` and `[weather]` work\n",
      "\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.9.0-alpha.2+5.gb40df75\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 56 lines----------------\n",
      "make read_crn accomodate bad files\n",
      "A couple of issues with our `read_crn` function. \n",
      "\n",
      "First, the character sequence '\\x00\\x00\\x00\\x00\\x00\\x00' occasionally shows up and trips up pandas. This can be fixed by adding `na_values=['\\x00\\x00\\x00\\x00\\x00\\x00']` to the reader.\n",
      "\n",
      "Second, we try to set the `CRX_VN` column to dtype int, but it occasionally has floats that cannot be coerced. The [documentation](https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/README.txt) says it should be treated like a string.\n",
      "\n",
      "Example below shows both issues in `'CRNS0101-05-2020-FL_Titusville_7_E.txt'`\n",
      "\n",
      "```\n",
      "92821 20200706 1145 20200706 0645      3  -80.69   28.62    24.5     0.0    151 0    24.7 C 0    94 0 -99.000 -9999.0   990 0   1.23 0\n",
      "92821 20200706 1150 20200706 0650      3  -80.69   28.62    24.7     0.0    168 0    25.0 C 0    94 0 -99.000 -9999.0   990 0   1.28 0\n",
      "92821 20200706 1155 20200706 0655      3  -80.69   28.62    24.9     0.0    173 0    25.3 C 0    93 0 -99.000 -9999.0   990 0   1.48 0\n",
      "92821 20200706 1200 20200706 0700      3  -80.69   28.62    24.9     0.0    190 0    25.5 C 0    93 0 -99.000 -9999.0   990 0   1.57 0\n",
      "\\x00\\x00\\x00\\x00\\x00\\x00 repeated\n",
      "92821 20200706 1305 20200706 0805  2.623  -80.69   28.62    26.8     0.0    409 0    30.0 C 0    87 0 -99.000 -9999.0   988 0   1.44 0\n",
      "92821 20200706 1310 20200706 0810  2.623  -80.69   28.62    26.9     0.0    430 0    30.2 C 0    87 0 -99.000 -9999.0   989 0   1.64 0\n",
      "92821 20200706 1315 20200706 0815  2.623  -80.69   28.62    27.0     0.0    445 0    30.4 C 0    86 0 -99.000 -9999.0   989 0   1.94 0\n",
      "92821 20200706 1320 20200706 0820  2.623  -80.69   28.62    27.3     0.0    463 0    30.8 C 0    86 0 -99.000 -9999.0   988 0   1.50 0\n",
      "92821 20200706 1325 20200706 0825  2.623  -80.69   28.62    27.6     0.0    478 0    31.1 C 0    85 0 -99.000 -9999.0   988 0   1.54 0\n",
      "92821 20200706 1330 20200706 0830  2.623  -80.69   28.62    27.6     0.0    496 0    31.5 C 0    84 0 -99.000 -9999.0   988 0   1.48 0\n",
      "```\n",
      "\n",
      "fyi @lboeman \n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 161 lines----------------\n",
      "IAM that supports AR coating like Fresnel\n",
      "# Problem\n",
      "Currently pvlib supports the DeSoto physical model (similar to normal glass), ASHRAE, Martin & Ruiz, and SAPM polynomial, but it doesn't have a pure Fresnel model that allows additional interfaces like an AR coating. \n",
      "\n",
      "* DeSoto physical model is most similar to the Fresnel for normal glass but only has one interface, so is limited to IAM curves below it only, while an AR coating would have a greater &rho; \n",
      "* Martin & Ruiz could be used to approximate an AR coated glass if the correct `a_r` were known. The default of `a_r=0.16` is slightly above the normal glass Fresnel IAM, but an `a_r=0.14` seems to match an AR coating with index of refraction of 1.2 most closely.\n",
      "\n",
      "![pvlib_iam](https://user-images.githubusercontent.com/1385621/180581071-0ff411f1-144a-40b6-a6a9-189ef55f019f.png)\n",
      "\n",
      "\n",
      "# Proposal\n",
      "a new method in `pvl.iam.fresnel_ar(aoi, n_ar=1.2, n_air=1.0, n_glass=1.56)` that implements the [Fresnel equation](https://en.wikipedia.org/wiki/Fresnel_equations)\n",
      "\n",
      "# Alternative\n",
      "Suggest readers to use Martin & Ruiz with `a_r=0.14` instead of default.\n",
      "\n",
      "# additional content\n",
      "PVsyst has switched to Fresnel equations. We can duplicate [their methods](https://www.pvsyst.com/help/iam_loss.htm) ignoring additional reflections and the encapsulant layer:\n",
      "![Fresnel-v-ASHRAE](https://user-images.githubusercontent.com/1385621/180581112-67f3ed9d-5bd3-4dfe-8180-8b5d173fcdd2.png)\n",
      "\n",
      "<details>\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "plt.ion()\n",
      "\n",
      "\n",
      "# constants\n",
      "n_glass = 1.56\n",
      "n_air = 1.0\n",
      "theta_inc = np.linspace(0, 88, 100)\n",
      "\n",
      "\n",
      "def snell(theta_1, n1, n2):\n",
      "    \"\"\"Snell's equation\"\"\"\n",
      "    sintheta_2 = n1/n2 * np.sin(np.radians(theta_1))\n",
      "    return sintheta_2, np.degrees(np.arcsin(sintheta_2))\n",
      "\n",
      "\n",
      "def refl_s(theta_1, theta_2, n1, n2):\n",
      "    \"\"\"Fresnel's equation\"\"\"\n",
      "    n1_costheta_1 = n1*np.cos(np.radians(theta_1))\n",
      "    n2_costheta_2 = n2*np.cos(np.radians(theta_2))\n",
      "    return np.abs((n1_costheta_1 - n2_costheta_2)/(n1_costheta_1 + n2_costheta_2))**2\n",
      "\n",
      "\n",
      "def refl_p(theta_1, theta_2, n1, n2):\n",
      "    \"\"\"Fresnel's equation\"\"\"\n",
      "    n1_costheta_2 = n1*np.cos(np.radians(theta_2))\n",
      "    n2_costheta_1 = n2*np.cos(np.radians(theta_1))\n",
      "    return np.abs((n1_costheta_2 - n2_costheta_1)/(n1_costheta_2 + n2_costheta_1))**2\n",
      "\n",
      "\n",
      "def refl_eff(rs, rp):\n",
      "    \"\"\"effective reflectivity\"\"\"\n",
      "    return (rs+rp)/2\n",
      "\n",
      "\n",
      "def trans(refl):\n",
      "    \"\"\"transmissivity\"\"\"\n",
      "    return 1-refl\n",
      "\n",
      "\n",
      "def refl0(n1, n2):\n",
      "    \"\"\"reflectivity at normal incidence\"\"\"\n",
      "    return np.abs((n1-n2)/(n1+n2))**2\n",
      "\n",
      "\n",
      "def fresnel(theta_inc, n1=n_air, n2=n_glass):\n",
      "    \"\"\"calculate IAM using Fresnel's Law\"\"\"\n",
      "    _, theta_tr = snell(theta_inc, n1, n2)\n",
      "    rs = refl_s(theta_inc, theta_tr, n1, n2)\n",
      "    rp = refl_p(theta_inc, theta_tr, n1, n2)\n",
      "    reff = refl_eff(rs, rp)\n",
      "    r0 = refl0(n1, n2)\n",
      "    return trans(reff)/trans(r0)\n",
      "\n",
      "\n",
      "def ashrae(theta_inc, b0=0.05):\n",
      "    \"\"\"ASHRAE equation\"\"\"\n",
      "    return 1 - b0*(1/np.cos(np.radians(theta_inc)) - 1)\n",
      "\n",
      "\n",
      "def fresnel_ar(theta_inc, n_ar, n1=n_air, n2=n_glass):\n",
      "    \"\"\"calculate IAM using Fresnel's law with AR\"\"\"\n",
      "    # use fresnel() for n2=n_ar\n",
      "    _, theta_ar = snell(theta_inc, n1, n_ar)\n",
      "    rs_ar1 = refl_s(theta_inc, theta_ar, n1, n_ar)\n",
      "    rp_ar1 = refl_p(theta_inc, theta_ar, n1, n_ar)\n",
      "    r0_ar1 = refl0(n1, n_ar)\n",
      "    # repeat with fresnel() with n1=n_ar\n",
      "    _, theta_tr = snell(theta_ar, n_ar, n2)\n",
      "    rs = refl_s(theta_ar, theta_tr, n_ar, n2)\n",
      "    rp = refl_p(theta_ar, theta_tr, n_ar, n2)\n",
      "    # note that combined reflectivity is product of transmissivity!\n",
      "    # so... rho12 = 1 - (1-rho1)(1-rho2) \n",
      "    reff = refl_eff(1-(1-rs_ar1)*(1-rs), 1-(1-rp_ar1)*(1-rp))\n",
      "    r0 = 1-(1-refl0(n_ar, n2))*(1-r0_ar1)\n",
      "    return trans(reff)/trans(r0)\n",
      "\n",
      "\n",
      "# plot Fresnel for normal glass and ASHRAE\n",
      "plt.plot(theta_inc, fresnel(theta_inc))\n",
      "plt.plot(theta_inc, ashrae(theta_inc))\n",
      "\n",
      "# calculate IAM for AR with n=1.1 and plot\n",
      "iam_ar11 = fresnel_ar(theta_inc, n_ar=1.1)\n",
      "plt.plot(theta_inc, iam_ar11)\n",
      "\n",
      "# repeat for AR with n=1.2\n",
      "iam_ar12 = fresnel_ar(theta_inc, n_ar=1.2)\n",
      "plt.plot(theta_inc, iam_ar12)\n",
      "\n",
      "# make plot pretty\n",
      "plt.legend(['Fresnel, normal glass', 'ASHRAE, $b_0=0.05$', 'Fresnel $n_{AR}=1.1$', 'Fresnel $n_{AR}=1.2$'])\n",
      "plt.title(\"IAM correction, Fresnel vs. ASHRAE, using basic eqn's\")\n",
      "plt.ylabel('IAM')\n",
      "plt.xlabel(r'incidence angle $\\theta_{inc} [\\degree]$')\n",
      "plt.grid()\n",
      "plt.ylim([0.55,1.05])\n",
      "```\n",
      "</details>\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 50 lines----------------\n",
      "Update CAMS/SoDa URL\n",
      "SoDa has developed a new load-balancing solution, such that requests are automatically redirected to the fastest server. This means that it might be advisable for us to update the URL in the [``pvlib.iotools.get_cams``](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html?highlight=get_cams#pvlib.iotools.get_cams) function. \n",
      "\n",
      "Email from SoDa (March 7th, 2023):\n",
      "> Our beta load balancing system for SoDa/CAMS API requests is extended to March 13th. All requests made on the beta-api.soda-solardata.com WILL NOT BE COUNTED in your subscription. The beta access will last until then. **From March 14th, the service will be fully operational and you have to use api.soda-solardata.com to process your API (machine to machine) requests.**\n",
      "\n",
      "and email from February 22nd, 2023:\n",
      "> This new functionality will automatically redirect any request to the fastest available SoDa server. As a result, future updates/maintenances won't need any action from your part as server switches will be completely autonomous.\n",
      "\n",
      "I will be following up on this issue in a couple of weeks.\n",
      "\n",
      "*Edit: email from March 20th, 2023*\n",
      "> We strongly advise you to switch your automatic commands on the load balancing system (api.soda-solardata.com). In that way, all future updates won't need any actions from your side. \n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 187 lines----------------\n",
      "Improve docstring or behavior for irradiance.get_total_irradiance and irradiance.get_sky_diffuse\n",
      "`pvlib.irradiance.get_total_irradiance` accepts kwargs `dni_extra` and `airmass`, both default to `None`. However, values for these kwargs are required for several of the irradiance transposition models. \n",
      "\n",
      "See discussion [here](https://groups.google.com/d/msg/pvlib-python/ZPMdpQOD6F4/cs1t23w8AwAJ)\n",
      "\n",
      "Docstring should specify when `dni_extra` and `airmass` are required, and which airmass is appropriate for each model.\n",
      "\n",
      "Could also test for kwarg values if e.g. `model=='perez'`\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "ModelChain.prepare_inputs can succeed with missing dhi\n",
      "From the docstring for `ModelChain.prepare_inputs()` I believe the method should fail if `weather` does not have a `dhi` column.\n",
      "\n",
      "The validation checks for `'ghi'` twice, but not `'dhi`'\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/11c356f9a89fc88b4d3ff368ce1aae170a97ebd7/pvlib/modelchain.py#L1136\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 21 lines----------------\n",
      "TypeError: running ModelChain with Arrays and module_temperature\n",
      "**Describe the bug**\n",
      "Another bug using Arrays. This time a TypeError is raised in `pvlib.modelchain._get_cell_temperature` because `self.system.temperature_model_parameters` is zipped with dataframe tuples but is never a tuple itself\n",
      "https://github.com/pvlib/pvlib-python/blob/dc617d0c182bc8eec57898a039cb5115b425645f/pvlib/modelchain.py#L1525\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "import traceback\n",
      "import pandas as pd\n",
      "from pvlib.location import Location\n",
      "from pvlib.pvsystem import Array, PVSystem\n",
      "from pvlib.modelchain import ModelChain\n",
      "\n",
      "\n",
      "array_params = {\n",
      "    \"surface_tilt\": 32.0,\n",
      "    \"surface_azimuth\": 180.0,\n",
      "    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\n",
      "    \"albedo\": 0.2,\n",
      "    \"temperature_model_parameters\": {\n",
      "        \"a\": -3.47,\n",
      "        \"b\": -0.0594,\n",
      "        \"deltaT\": 3.0,\n",
      "    },\n",
      "    \"strings\": 5,\n",
      "    \"modules_per_string\": 7,\n",
      "    \"module_parameters\": {\n",
      "        \"alpha_sc\": 0.004539,\n",
      "        \"gamma_ref\": 1.2,\n",
      "        \"mu_gamma\": -0.003,\n",
      "        \"I_L_ref\": 5.11426,\n",
      "        \"I_o_ref\": 8.10251e-10,\n",
      "        \"R_sh_ref\": 381.254,\n",
      "        \"R_sh_0\": 400.0,\n",
      "        \"R_s\": 1.06602,\n",
      "        \"cells_in_series\": 96,\n",
      "        \"R_sh_exp\": 5.5,\n",
      "        \"EgRef\": 1.121,\n",
      "    },\n",
      "}\n",
      "inverter_parameters = {\n",
      "    \"Paco\": 250.0,\n",
      "    \"Pdco\": 259.589,\n",
      "    \"Vdco\": 40.0,\n",
      "    \"Pso\": 2.08961,\n",
      "    \"C0\": -4.1e-05,\n",
      "    \"C1\": -9.1e-05,\n",
      "    \"C2\": 0.000494,\n",
      "    \"C3\": -0.013171,\n",
      "    \"Pnt\": 0.075,\n",
      "}\n",
      "\n",
      "\n",
      "location = Location(latitude=33.98, longitude=-115.323, altitude=2300)\n",
      "\n",
      "array_sys = PVSystem(\n",
      "    arrays=[\n",
      "        Array(**array_params, name=0),\n",
      "    ],\n",
      "    inverter_parameters=inverter_parameters,\n",
      ")\n",
      "poa = pd.DataFrame(\n",
      "    {\n",
      "        \"poa_global\": [1100.0, 1101.0],\n",
      "        \"poa_direct\": [1000.0, 1001.0],\n",
      "        \"poa_diffuse\": [100.0, 100.0],\n",
      "        \"module_temperature\": [35.0, 33.0],\n",
      "    },\n",
      "    index=pd.DatetimeIndex(\n",
      "        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\n",
      "    ),\n",
      ")\n",
      "standard = poa.copy().rename(\n",
      "    columns={\"poa_global\": \"ghi\", \"poa_direct\": \"dni\", \"poa_diffuse\": \"dhi\"}\n",
      ")\n",
      "effective = poa.copy()[[\"module_temperature\", \"poa_global\"]].rename(\n",
      "    columns={\"poa_global\": \"effective_irradiance\"}\n",
      ")\n",
      "mc = ModelChain(\n",
      "    array_sys,\n",
      "    location,\n",
      "    aoi_model=\"no_loss\",\n",
      "    spectral_model=\"no_loss\",\n",
      ")\n",
      "try:\n",
      "    mc.run_model([standard])\n",
      "except TypeError:\n",
      "    print(traceback.format_exc())\n",
      "else:\n",
      "    raise RuntimeError(\"expected a type error\")\n",
      "try:\n",
      "    mc.run_model_from_poa([poa])\n",
      "except TypeError:\n",
      "    print(traceback.format_exc())\n",
      "else:\n",
      "    raise RuntimeError(\"expected a type error\")\n",
      "try:\n",
      "    mc.run_model_from_effective_irradiance([effective])\n",
      "except TypeError:\n",
      "    print(traceback.format_exc())\n",
      "else:\n",
      "    raise RuntimeError(\"expected a type error\")\n",
      "\n",
      "```\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``:  master/g684b247\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 466 lines----------------\n",
      "make Array play nicely with fixed tilt systems and trackers\n",
      "#1076 is adding an `Array` class that largely describes a fixed-tilt array. However, the composition logic of `PVSystem: def __init__(arrays,...)` combined with the inheritance logic of `SingleAxisTracker(PVSystem)` makes for an odd combination of `Array` objects within `SingleAxisTrackers`. See, for example, https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539704316. \n",
      "\n",
      "In https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539686448 I proposed roughly:\n",
      "\n",
      "Split the `Array` into `BaseArray`, `FixedTiltArray(BaseArray)`, `SingleAxisTrackingArray(BaseArray)`? Basic idea:\n",
      "\n",
      "```python\n",
      "class FixedTiltArray(BaseArray)\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    surface_tilt: float or array-like, default 0\n",
      "        Surface tilt angles in decimal degrees.\n",
      "        The tilt angle is defined as degrees from horizontal\n",
      "        (e.g. surface facing up = 0, surface facing horizon = 90)\n",
      "\n",
      "    surface_azimuth: float or array-like, default 180\n",
      "        Azimuth angle of the module surface.\n",
      "        North=0, East=90, South=180, West=270.\n",
      "\n",
      "    **kwargs\n",
      "        Passed to Array. Or copy remainder of Array doc string to be explicit.\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "# could be in pvsystem.py (module is gradually becoming just the objects) or could be in tracking.py\n",
      "class SingleAxisTrackerArray(BaseArray)\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    axis_tilt : float, default 0\n",
      "        The tilt of the axis of rotation (i.e, the y-axis defined by\n",
      "        axis_azimuth) with respect to horizontal, in decimal degrees.\n",
      "\n",
      "    etc.\n",
      "\n",
      "    **kwargs\n",
      "        Passed to Array. Or copy remainder of Array doc string to be explicit.\n",
      "    \"\"\"\n",
      "```\n",
      "\n",
      "I believe the only major challenge is that the `get_aoi` and `get_irradiance` methods would either need to differ in signature (as they do now, and thus present a challenge to a `PVSystem` wrapper) or in implementation (tracker methods would include a call to `singleaxis`, and thus would be less efficient in some workflows). @wfvining suggests that the consistent signature is more important and I'm inclined to agree.\n",
      "\n",
      "We'd also deprecate the old `SingleAxisTracking` class.\n",
      "\n",
      "We should resolve this issue before releasing the new Array code into the wild in 0.9.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 277 lines----------------\n",
      "ModelChainResult.cell_temperature is not always a pandas.Series \n",
      "For a `ModelChain` configured as below, the `cell_temperature` when running the model chain with a list of data like `ModelChain.run_model([data])` is a tuple with a single number instead of the expected Series\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "import pandas as pd                                                                                                                                                                                                                                           \n",
      "from pvlib.location import Location                                                                                                                                                                                                                           \n",
      "from pvlib.pvsystem import PVSystem, Array                                                                                                                                                                                                                    \n",
      "from pvlib.modelchain import ModelChain                                                                                                                                                                                                                       \n",
      "                                                                                                                                                                                                                                                              \n",
      "data = pd.DataFrame(                                                                                                                                                                                                                                          \n",
      "    {                                                                                                                                                                                                                                                         \n",
      "        \"ghi\": [1100.0, 1101.0],                                                                                                                                                                                                                              \n",
      "        \"dni\": [1000.0, 1001],                                                                                                                                                                                                                                \n",
      "        \"dhi\": [100.0, 100],                                                                                                                                                                                                                                  \n",
      "        \"temp_air\": [25.0, 25],                                                                                                                                                                                                                               \n",
      "        \"wind_speed\": [10.0, 10],                                                                                                                                                                                                                             \n",
      "    },                                                                                                                                                                                                                                                        \n",
      "    index=pd.DatetimeIndex(                                                                                                                                                                                                                                   \n",
      "        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]                                                                                                                                                                      \n",
      "    ),                                                                                                                                                                                                                                                        \n",
      ")                                                                                                                                                                                                                                                             \n",
      "                                                                                                                                                                                                                                                              \n",
      "array_params = {                                                                                                                                                                                                                                              \n",
      "    \"name\": None,                                                                                                                                                                                                                                             \n",
      "    \"surface_tilt\": 20.0,                                                                                                                                                                                                                                     \n",
      "    \"surface_azimuth\": 180.0,                                                                                                                                                                                                                                 \n",
      "    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",                                                                                                                                                                                                                \n",
      "    \"albedo\": 0.2,                                                                                                                                                                                                                                            \n",
      "    \"temperature_model_parameters\": {                                                                                                                                                                                                                         \n",
      "        \"u_c\": 29.0,                                                                                                                                                                                                                                          \n",
      "        \"u_v\": 0.0,                                                                                                                                                                                                                                           \n",
      "        \"eta_m\": 0.1,                                                                                                                                                                                                                                         \n",
      "        \"alpha_absorption\": 0.9,                                                                                                                                                                                                                              \n",
      "    },                                                                                                                                                                                                                                                        \n",
      "    \"strings\": 5,                                                                                                                                                                                                                                             \n",
      "    \"modules_per_string\": 7,                                                                                                                                                                                                                                  \n",
      "    \"module_parameters\": {                                                                                                                                                                                                                                    \n",
      "        \"alpha_sc\": 0.004539,                                                                                                                                                                                                                                 \n",
      "        \"gamma_ref\": 1.2,                                                                                                                                                                                                                                     \n",
      "        \"mu_gamma\": -0.003,                                                                                                                                                                                                                                   \n",
      "        \"I_L_ref\": 5.11426,                                                                                                                                                                                                                                   \n",
      "        \"I_o_ref\": 8.10251e-10,                                                                                                                                                                                                                               \n",
      "        \"R_sh_ref\": 381.254,                                                                                                                                                                                                                                  \n",
      "        \"R_sh_0\": 400.0,                                                                                                                                                                                                                                      \n",
      "        \"R_s\": 1.06602,                                                                                                                                                                                                                                       \n",
      "        \"cells_in_series\": 96,                                                                                                                                                                                                                                \n",
      "        \"R_sh_exp\": 5.5,                                                                                                                                                                                                                                      \n",
      "        \"EgRef\": 1.121,                                                                                                                                                                                                                                       \n",
      "    },                                                                                                                                                                                                                                                        \n",
      "}\n",
      "inverter_parameters = {                                                                                                                                                                                                                                       \n",
      "    \"Paco\": 250.0,                                                                                                                                                                                                                                            \n",
      "    \"Pdco\": 259.589,                                                                                                                                                                                                                                          \n",
      "    \"Vdco\": 40.0,                                                                                                                                                                                                                                             \n",
      "    \"Pso\": 2.08961,                                                                                                                                                                                                                                           \n",
      "    \"C0\": -4.1e-05,                                                                                                                                                                                                                                           \n",
      "    \"C1\": -9.1e-05,                                                                                                                                                                                                                                           \n",
      "    \"C2\": 0.000494,                                                                                                                                                                                                                                           \n",
      "    \"C3\": -0.013171,                                                                                                                                                                                                                                          \n",
      "    \"Pnt\": 0.075,                                                                                                                                                                                                                                             \n",
      "}                                                                                                                                                                                                                                                             \n",
      "                                                                                                                                                                                                                                                              \n",
      "                                                                                                                                                                                                                                                              \n",
      "location = Location(latitude=33.98, longitude=-115.323, altitude=2300)                                                                                                                                                                                        \n",
      "                                                                                                                                                                                                                                                              \n",
      "                                                                                                                                                                                                                                                              \n",
      "array_sys = PVSystem(                                                                                                                                                                                                                                         \n",
      "    arrays=[Array(**array_params)], inverter_parameters=inverter_parameters                                                                                                                                                                                   \n",
      ")                                                                                                                                                                                                                                                             \n",
      "assert isinstance(                                                                                                                                                                                                                                            \n",
      "    ModelChain(array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                            \n",
      "    .run_model(data)                                                                                                                                                                                                                                          \n",
      "    .results.cell_temperature,                                                                                                                                                                                                                                \n",
      "    pd.Series,                                                                                                                                                                                                                                                \n",
      ")                                                                                                                                                                                                                                                             \n",
      "                                                                                                                                                                                                                                                              \n",
      "array_run = ModelChain(                                                                                                                                                                                                                                       \n",
      "    array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                        \n",
      ").run_model([data])                                                                                                                                                                                                                                           \n",
      "assert array_run.results.cell_temperature == array_run.cell_temperature                                                                                                                                                                                       \n",
      "print(array_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                            \n",
      "                                                                                                                                                                                                                                                              \n",
      "                                                                                                                                                                                                                                                              \n",
      "array_params[\"strings_per_inverter\"] = array_params.pop(\"strings\")                                                                                                                                                                                            \n",
      "standard_sys = PVSystem(**array_params, inverter_parameters=inverter_parameters)                                                                                                                                                                              \n",
      "assert isinstance(                                                                                                                                                                                                                                            \n",
      "    ModelChain(standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                         \n",
      "    .run_model(data)                                                                                                                                                                                                                                          \n",
      "    .results.cell_temperature,                                                                                                                                                                                                                                \n",
      "    pd.Series,                                                                                                                                                                                                                                                \n",
      ")                                                                                                                                                                                                                                                             \n",
      "                                                                                                                                                                                                                                                              \n",
      "standard_run = ModelChain(                                                                                                                                                                                                                                    \n",
      "    standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                     \n",
      ").run_model([data])                                                                                                                                                                                                                                           \n",
      "assert standard_run.results.cell_temperature == standard_run.cell_temperature                                                                                                                                                                                 \n",
      "print(standard_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                         \n",
      "assert not isinstance(standard_run.results.cell_temperature, pd.Series)                                                                                                                                                                                       \n",
      "                                                                                \n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "`type(ModelChain.run_model([data]).results.cell_temperature) == pd.Series`\n",
      "__\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``:  0.8.1+4.gba4a199\n",
      " - ``pandas.__version__``:  1.1.4\n",
      " - python: 3.8.5\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 111 lines----------------\n",
      "SolarAnywhere File -- pvlib.iotools.read_tmy3 Bug\n",
      "@AdamRJensen \n",
      "\n",
      "There's a bug report for reading SolarAnywhere Files with using the pvlib.iotools.read_tmy3 function. This bug is in the TMY3 file (I think?)\n",
      "\n",
      "\n",
      "![TMY3](https://github.com/pvlib/pvlib-python/assets/74630912/1f85b014-a40a-42af-9c07-76e51ccc606e)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 96 lines----------------\n",
      "Add `min_angle` argument to `tracking.singleaxis`\n",
      "In `tracking.singleaxis` the minimum angle of the tracker is assumed to be opposite of the maximum angle, although in some cases the minimum angle could be different. NREL SAM doesn't support that but PVsyst does.\n",
      "\n",
      "In order to support non symmetrical limiting angles, `tracking.singleaxis` should have another, optional, input, `min_angle`. By default, if not supplied (i.e. value is `None`), the current behavior (`min_angle = -max_angle`) would apply.\n",
      "\n",
      "Can I propose a PR for this, with modifications to `tracking.singleaxis`, `tracking.SingleAxisTracker` and to `pvsystem.SingleAxisTrackerMount` + corresponding tests?\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 398 lines----------------\n",
      "ModelChain should accept albedo in weather dataframe\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "Albedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\n",
      "\n",
      "Additionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "ModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "An alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 696 lines----------------\n",
      "is vf_row_sky correct?\n",
      "https://github.com/pvlib/pvlib-python/blob/7e88d212c786d0ad334dce6fcafaf29339ff60ab/pvlib/bifacial/infinite_sheds.py#L146\n",
      "\n",
      "I think this should be:\n",
      "\n",
      "$$\\frac{1 + \\cos \\left( \\text{surface tilt} + \\psi_{t}\\ \\text{shaded} \\right)}{2}$$\n",
      "\n",
      "because in the reference frame of the module surface the angle pointing along the slant height to the sky is actually zero, $cos(0) = 1$, and the angle above the slant height to a horizontal line would be the `surface_tilt` itself, then the angle from the horizontal to the top of the next row is `psi_t_shaded` so finally this angle from the slant height all the way up to the top of the next row is `surface_tilt + psi_t_shaded`:\n",
      "\n",
      "![infinite_sheds](https://user-images.githubusercontent.com/1385621/218985907-7fced67c-ccff-439f-8fc8-0774026b9501.png)\n",
      "\n",
      "For example, this is why if `psi_t_shaded` is zero, then the view factor should collapse to the isotropic view factor $(1+\\cos(\\beta))/2$ as given on the [PVPMC website modeling reference for POA sky diffuse](https://pvpmc.sandia.gov/modeling-steps/1-weather-design-inputs/plane-of-array-poa-irradiance/calculating-poa-irradiance/poa-sky-diffuse/isotropic-sky-diffuse-model/).\n",
      "\n",
      "The actual value difference between the two formulas can be quite small when `psi_t_shaded` is close to zero (_eg_ less than 5&deg;), but it's significant when as the masking angle is larger (_eg_ greater than 5&deg;).\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 368 lines----------------\n",
      "Allow user to set tol and maxiter for singlediode newton method\n",
      "The first few lines of `pvlib.singlediode` set `tol` and `maxiter` for all the solvers using the newton method:\n",
      "\n",
      "```\n",
      "from scipy.optimize import brentq, newton\n",
      "from scipy.special import lambertw\n",
      "\n",
      "# set keyword arguments for all uses of newton in this module\n",
      "newton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\n",
      "```\n",
      "\n",
      "However, I would like to change `tol` and `maxiter` for my application. It would be great if these could be added instead as keyword arguments to the various functions so they can be adjusted by the user. Using a variety of singlediode model params, I have found that by setting tol=0.1 and maxiter=10, I can realize a 1.4x speedup in the `singeldiode.bishop88_mpp` algorithm while incurring a maximum error of 0.007038% and a mean absolute error of  0.000042% in calculated V_mp.\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 435 lines----------------\n",
      "ModelChain should accept albedo in weather dataframe\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "Albedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\n",
      "\n",
      "Additionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "ModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "An alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 16 lines----------------\n",
      "temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0\n",
      "**Describe the bug**\n",
      "When the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:\n",
      "\n",
      "```\n",
      "  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n",
      "  \tTo accept the future behavior, pass 'dtype=object'.\n",
      "  \tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n",
      "    a = asanyarray(a)\n",
      "```\n",
      "\n",
      "**To Reproduce**\n",
      "```python\n",
      "In [1]: import pvlib\n",
      "   ...: import pandas as pd\n",
      "   ...: \n",
      "   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)\n",
      "   ...: \n",
      "   ...: kwargs = {\n",
      "   ...:     'poa_global': pd.Series(1000, index_naive),\n",
      "   ...:     'temp_air': pd.Series(20, index_naive),\n",
      "   ...:     'wind_speed': pd.Series(1, index_naive),\n",
      "   ...:     'noct_installed': 45\n",
      "   ...: }\n",
      "   ...: \n",
      "\n",
      "In [2]: print(pvlib.temperature.fuentes(**kwargs))\n",
      "2019-01-01 00:00:00    47.85\n",
      "2019-01-01 01:00:00    50.85\n",
      "2019-01-01 02:00:00    50.85\n",
      "Freq: H, Name: tmod, dtype: float64\n",
      "\n",
      "In [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')\n",
      "   ...: print(pvlib.temperature.fuentes(**kwargs))\n",
      "   ...: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<ipython-input-3-ff99badadc91>\", line 2, in <module>\n",
      "    print(pvlib.temperature.fuentes(**kwargs))\n",
      "\n",
      "  File \"/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py\", line 602, in fuentes\n",
      "    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n",
      "\n",
      "TypeError: float() argument must be a string or a number, not 'Timedelta'\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "`temperature.fuentes` should work with both tz-naive and tz-aware inputs.\n",
      "\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.8.0\n",
      " - ``pandas.__version__``: 1.0.0+\n",
      " - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \\n[GCC 7.3.0]\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 98 lines----------------\n",
      "expose pvlib.temperature.fuentes in PVSystem and ModelChain\n",
      "Follow up to #1032 and #1037 \n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 211 lines----------------\n",
      "remove ModelChain.orientation_strategy\n",
      "I don't like that `ModelChain(system, location, orientation_strategy='flat`|`south_at_latitude_tilt`) modifies the `system` object. It's not something we do anywhere else in pvlib. `orientation_strategy` only supports flat and south_at_latitude_tilt, neither of which are commonly used in the real world in 2020. \n",
      "\n",
      "I think we should remove it, maybe even without deprecation, in 0.8.\n",
      "\n",
      "I'm ok with keeping the `modelchain.get_orientation` function for now.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 63 lines----------------\n",
      "golden-section search fails when upper and lower bounds are equal\n",
      "**Describe the bug**\n",
      "I was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:\n",
      "```python\n",
      "import os\n",
      "import pathlib\n",
      "import time\n",
      "import json\n",
      "from datetime import datetime\n",
      "from time import mktime, gmtime\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from pvlib import pvsystem\n",
      "from pvlib import location as pvlocation\n",
      "from pvlib import modelchain\n",
      "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove\n",
      "from pvlib.bifacial.pvfactors import pvfactors_timeseries\n",
      "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
      "\n",
      "class PV:\n",
      "    def pv_transform_time(self, val):\n",
      "        # tt = gmtime(val / 1000)\n",
      "        tt = gmtime(val)\n",
      "        dd = datetime.fromtimestamp(mktime(tt))\n",
      "        timestamp = pd.Timestamp(dd)\n",
      "        return timestamp\n",
      "\n",
      "    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):\n",
      "        # super().__init__(**kwargs)\n",
      "\n",
      "        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[\"sapm\"][\n",
      "            \"open_rack_glass_glass\"\n",
      "        ]\n",
      "        # Load the database of CEC module model parameters\n",
      "        modules = pvsystem.retrieve_sam(\"cecmod\")\n",
      "        # Load the database of CEC inverter model parameters\n",
      "        inverters = pvsystem.retrieve_sam(\"cecinverter\")\n",
      "\n",
      "\n",
      "        # A bare bone PV simulator\n",
      "\n",
      "        # Load the database of CEC module model parameters\n",
      "        modules = pvsystem.retrieve_sam('cecmod')\n",
      "        inverters = pvsystem.retrieve_sam('cecinverter')\n",
      "        module_parameters = modules[model]\n",
      "        inverter_parameters = inverters[inverter]\n",
      "\n",
      "        location = pvlocation.Location(latitude=latitude, longitude=longitude)\n",
      "        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)\n",
      "        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=\"no_loss\")\n",
      "\n",
      "    def process(self, data):\n",
      "        weather = pd.read_json(data)\n",
      "        # print(f\"raw_weather: {weather}\")\n",
      "        weather.drop('time.1', axis=1, inplace=True)\n",
      "        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file\n",
      "        weather[\"time\"] = weather[\"time\"].apply(self.pv_transform_time)\n",
      "        weather.index = weather[\"time\"]\n",
      "        # print(f\"weather: {weather}\")\n",
      "        # print(weather.dtypes)\n",
      "        # print(weather['ghi'][0])\n",
      "        # print(type(weather['ghi'][0]))\n",
      "\n",
      "        # simulate\n",
      "        self.modelchain.run_model(weather)\n",
      "        # print(self.modelchain.results.ac.to_frame().to_json())\n",
      "        print(self.modelchain.results.ac)\n",
      "\n",
      "\n",
      "# good data\n",
      "good_data = \"{\\\"time\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"},\\\"ghi\\\":{\\\"12\\\":36},\\\"dhi\\\":{\\\"12\\\":36},\\\"dni\\\":{\\\"12\\\":0},\\\"Tamb\\\":{\\\"12\\\":8.0},\\\"WindVel\\\":{\\\"12\\\":5.0},\\\"WindDir\\\":{\\\"12\\\":270},\\\"time.1\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"}}\"\n",
      "\n",
      "# data that causes error\n",
      "data = \"{\\\"time\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"},\\\"ghi\\\":{\\\"4\\\":0},\\\"dhi\\\":{\\\"4\\\":0},\\\"dni\\\":{\\\"4\\\":0},\\\"Tamb\\\":{\\\"4\\\":8.0},\\\"WindVel\\\":{\\\"4\\\":4.0},\\\"WindDir\\\":{\\\"4\\\":240},\\\"time.1\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"}}\"\n",
      "p1 = PV(model=\"Trina_Solar_TSM_300DEG5C_07_II_\", inverter=\"ABB__MICRO_0_25_I_OUTD_US_208__208V_\", latitude=51.204483, longitude=5.265472)\n",
      "p1.process(good_data)\n",
      "print(\"=====\")\n",
      "p1.process(data)\n",
      "```\n",
      "Error:\n",
      "```log\n",
      "$ python3 ./tmp-pv.py \n",
      "time\n",
      "2010-01-01 13:30:00    7.825527\n",
      "dtype: float64\n",
      "=====\n",
      "/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide\n",
      "  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 88, in <module>\n",
      "    p1.process(data)\n",
      "  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 75, in process\n",
      "    self.modelchain.run_model(weather)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1770, in run_model\n",
      "    self._run_from_effective_irrad(weather)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1858, in _run_from_effective_irrad\n",
      "    self.dc_model()\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 790, in cec\n",
      "    return self._singlediode(self.system.calcparams_cec)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 772, in _singlediode\n",
      "    self.results.dc = tuple(itertools.starmap(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 931, in singlediode\n",
      "    return singlediode(photocurrent, saturation_current,\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 2826, in singlediode\n",
      "    out = _singlediode._lambertw(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py\", line 651, in _lambertw\n",
      "    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py\", line 364, in _golden_sect_DataFrame\n",
      "    raise Exception(\"Iterations exceeded maximum. Check that func\",\n",
      "Exception: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')\n",
      "```\n",
      "\n",
      "I have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).\n",
      "\n",
      "**Expected behavior**\n",
      "PVlib should have consistent behavior and regardless of GHI-DHI readings.\n",
      "\n",
      "**Versions:**\n",
      "```python\n",
      ">>> import pvlib\n",
      ">>> import pandas\n",
      ">>> pvlib.__version__\n",
      "'0.9.1'\n",
      ">>> pandas.__version__\n",
      "'1.4.3'\n",
      "``` \n",
      " - python: 3.10.6\n",
      "- OS: Ubuntu 22.04.1 LTS\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 227 lines----------------\n",
      "Consider extracting the surface orientation calculation in pvlib.tracking.singleaxis() to its own function\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "The usual workflow for modeling single-axis tracking in pvlib is to treat tracker rotation (`tracker_theta`) as an unknown to be calculated from solar position and array geometry.  However, sometimes a user might have their own tracker rotations but not have the corresponding `surface_tilt` and `surface_azimuth` values.  Here are a few motivating examples:\n",
      "- Using measured rotation angles\n",
      "- Post-processing the output of `tracking.singleaxis()` to include wind stow events or tracker stalls\n",
      "- Other tracking algorithms that determine rotation differently from the astronomical method\n",
      "\n",
      "Assuming I have my tracker rotations already in hand, getting the corresponding `surface_tilt` and `surface_azimuth` angles is not as easy as it should be.  For the specific case of horizontal N-S axis the math isn't so bad, but either way it's annoying to have to DIY when pvlib already has code to calculate those angles from tracker rotation.\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "A function `pvlib.tracking.rotation_to_orientation` that implements the same math in `pvlib.tracking.singleaxis` to go from `tracker_theta` to `surface_tilt` and `surface_azimuth`.  Basically extract out the second half of `tracking.singleaxis` into a new function.  Suggestions for the function name are welcome.  To be explicit, this is more or less what I'm imagining:\n",
      "\n",
      "```python\n",
      "def rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\n",
      "    # insert math from second half of tracking.singleaxis() here\n",
      "    out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n",
      "           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n",
      "    return pandas_if_needed(out)\n",
      "```\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "Continue suffering\n",
      "\n",
      "**Additional context**\n",
      "This is one step towards a broader goal I have for `pvlib.tracking` to house other methods to determine tracker rotation in addition to the current astronomical method, the same way we have multiple temperature and transposition models.  These functions would be responsible for determining tracker rotations, and they'd all use this `rotation_to_orientation` function to convert rotation to module orientation.\n",
      "\n",
      "Separately, I wonder if the code could be simplified using the tilt and azimuth equations in Bill's technical report (https://www.nrel.gov/docs/fy13osti/58891.pdf) -- seems like what we're doing is overly complicated, although maybe I've just not studied it closely enough.\n",
      "\n",
      "cc @williamhobbs @spaneja \n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 310 lines----------------\n",
      "Infinite sheds perf improvement: vectorize over surface_tilt\n",
      "Infinite sheds is quite a bit slower than the modelchain POA modeling we use for frontside (as expected). I see a TODO comment in the code for _vf_ground_sky_integ (`_TODO: vectorize over surface_tilt_`) that could potentially result in some perf improvement for Infinite sheds calls with tracking systems.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 155 lines----------------\n",
      "Add interp method for modelchain aoi model.\n",
      "I would like to simulate the effect of different IAM functions on performance. Pvlib already has an `interp` method for the iam_loss function. However, it is not possible to use `interp` within model chain. Can we add this feature?\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 125 lines----------------\n",
      "Apparent numerical instability in I_mp calculation using PVsyst model\n",
      "**Describe the bug**\n",
      "\n",
      "I used these parameters in `pvlib.pvsystem.calcparams_pvsyst()` in order to calculate `I_mp` vs. `T` using `pvlib.pvsystem.singlediode()` with `effective_irradiance` fixed at 1000 W/m^2 and `temp_cell` having 1001 values ranging from 15 to 50 degC:\n",
      "\n",
      "`{'alpha_sc': 0.006, 'gamma_ref': 1.009, 'mu_gamma': -0.0005, 'I_L_ref': 13.429, 'I_o_ref': 3.719506010004821e-11, 'R_sh_ref': 800.0, 'R_sh_0': 3200.0, 'R_s': 0.187, 'cells_in_series': 72, 'R_sh_exp': 5.5, 'EgRef': 1.121, 'irrad_ref': 1000, 'temp_ref': 25}`\n",
      "\n",
      "My purpose was to investigate the temperature coefficient of `I_mp`, and I got the following result, which appears to suffer from a numeric instability:\n",
      "\n",
      "![image](https://user-images.githubusercontent.com/1125363/98264917-ab2d2880-1f45-11eb-83a2-e146774abf44.png)\n",
      "\n",
      "For comparison, the corresponding `V_mp` vs. `T` plot:\n",
      "\n",
      "![image](https://user-images.githubusercontent.com/1125363/98264984-bc763500-1f45-11eb-9012-7c29efa25e1e.png)\n",
      "\n",
      "**To Reproduce**\n",
      "\n",
      "Run the above calculations using the parameters provided.\n",
      "\n",
      "**Expected behavior**\n",
      "\n",
      "Better numerical stability in `I_mp` vs. `T`.\n",
      "\n",
      "**Screenshots**\n",
      "\n",
      "See above.\n",
      "\n",
      "**Versions:**\n",
      "\n",
      " - ``pvlib.__version__``: 0.8.0\n",
      " - ``numpy.__version__``: 1.19.2\n",
      " - ``scipy.__version__``: 1.5.2\n",
      " - ``pandas.__version__``: 1.1.3\n",
      " - python: 3.8.5\n",
      "\n",
      "**Additional context**\n",
      "\n",
      "I was going to attempt a numerical computation of the temperature coefficient of `I_mp` for a model translation to the SAPM. I have seen reports from CFV in which this coefficient is actually negative, and I have computed it alternately using the `P_mp` and `V_mp` temperature coefficients, and gotten a negative value for this particular PV module. Despite the apparent numerical instability in the above plot, it still suggests that the coefficient should be positive, not negative. Perhaps I am missing something here?\n",
      "\n",
      "Also, I have not dug deep enough to figure out if the underlying issue is in `pvlib.pvsystem.singlediode()`.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 498 lines----------------\n",
      "PVSystem.temperature_model_parameters requirement\n",
      "The `temperature_model_parameters` handling code below suggests to me that in 0.8 we're going to \n",
      "\n",
      "1. set default values `module_type=None` and `racking_model=None`.\n",
      "2. require user to specify either `temperature_model_parameters` or both `module_type` and `racking_model`.\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L208-L221\n",
      "\n",
      "@cwhanse is that correct?\n",
      "\n",
      "The problem is that the only way to see this warning is to supply an invalid `module_type` or `racking_model`. That's because `PVSystem._infer_temperature_model` is called before the code above, and it looks up the default `module_type` and `racking_model` and successfully finds temperature coefficients.\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L201-L203\n",
      "\n",
      "So I'm guessing that this warning has been seen by only a small fraction of people that need to see it. I'm ok moving forward with the removal in 0.8 or pushing to 0.9. \n",
      "remove deprecated functions in 0.8\n",
      "`pvsystem`:\n",
      "* `sapm_celltemp`\n",
      "* `pvsyst_celltemp`\n",
      "* `ashraeiam`\n",
      "* `physicaliam`\n",
      "* `sapm_aoi_loss`\n",
      "* `PVSystem.ashraeiam`\n",
      "* `PVSystem.physicaliam`\n",
      "* `PVSystem.sapm_aoi_loss`\n",
      "* inference of `PVSystem.temperature_model_parameters`\n",
      "\n",
      "`modelchain.ModelChain`:\n",
      "* remove `times` from `complete_irradiance`, `prepare_inputs`, `run_model`\n",
      "* remove `temp_model` kwarg\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 381 lines----------------\n",
      "CEC 6-parameter coefficient generation\n",
      "SAM is able to extract the CEC parameters required for calcparams_desoto.  This is done through the 'CEC Performance Model with User Entered Specifications' module model, and coefficients are automatically extracted given nameplate parameters Voc, Isc, Imp, Vmp and TempCoeff.  The method is based on Aron Dobos' \"An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model \", 2012\n",
      "\n",
      "Ideally we should be able to work with the SAM open source code, extract the bit that does the coefficient generation, and put it into a PVLib function that would allow users to run calcparams_desoto with any arbitrary module type.  At the moment we are dependent on PV modules loaded into the SAM or CEC database.\n",
      "\n",
      "Thank you!\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 123 lines----------------\n",
      "Corrections to Townsend snow model\n",
      "Private communications with the model's author have turned up some issues with the pvlib implementation. Chief among the issues is  this part of the calculation:\n",
      "\n",
      "```\n",
      "    lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\n",
      "    gamma = (\n",
      "        slant_height\n",
      "        * effective_snow_weighted_m\n",
      "        * cosd(surface_tilt)\n",
      "        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\n",
      "        * 2\n",
      "        * tand(angle_of_repose)\n",
      "    )\n",
      "\n",
      "    ground_interference_term = 1 - C2 * np.exp(-gamma)\n",
      "```\n",
      "\n",
      "When `lower_edge_height_clipped` < `effective_snow_weighted_m`, `gamma` < 0 and the `ground_interference_term` can become negative. In contrast, the author's intent is that C2 < `ground_interference_terms` < 1. The author recommends clipping the squared difference (lower bound being worked out but will be something like 0.01.).\n",
      "\n",
      "Other issues appear to arise from the unit conversions. The published model uses inches for distance and snow depth. The pvlib code uses cm for snow depth (convenience for working with external snow data) and m for distances (for consistency with the rest of pvlib). After several steps, including the `ground_interference_term` calculation, the code converts from cm or m to inches to apply the final formula for loss (since the formula involves some coefficients determined by a regression). It would be easier to trace the pvlib code back to the paper if the internal unit conversions (from cm / m to inches) were done earlier.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 29 lines----------------\n",
      "PVSystem with single Array generates an error\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "\n",
      "When a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.\n",
      "\n",
      "```\n",
      "mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\n",
      "array = pvlib.pvsystem.Array(mount=mount)\n",
      "pv = pvlib.pvsystem.PVSystem(arrays=array)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-13-f5424e3db16a> in <module>\n",
      "      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\n",
      "      4 array = pvlib.pvsystem.Array(mount=mount)\n",
      "----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)\n",
      "\n",
      "~\\anaconda3\\lib\\site-packages\\pvlib\\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)\n",
      "    251                 array_losses_parameters,\n",
      "    252             ),)\n",
      "--> 253         elif len(arrays) == 0:\n",
      "    254             raise ValueError(\"PVSystem must have at least one Array. \"\n",
      "    255                              \"If you want to create a PVSystem instance \"\n",
      "\n",
      "TypeError: object of type 'Array' has no len()\n",
      "\n",
      "```\n",
      "\n",
      "Not a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this\n",
      "\n",
      "```\n",
      "mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\n",
      "array = pvlib.pvsystem.Array(mount=mount)\n",
      "pv = pvlib.pvsystem.PVSystem(arrays=[array])\n",
      "```\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "Handle `arrays=array` where `array` is an instance of `Array`\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "Status quo - either make the single Array into a list, or use the PVSystem kwargs.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 52 lines----------------\n",
      "Inconsistent default settings for _prep_inputs_solar_pos in prepare_inputs and prepare_inputs_from_poa\n",
      "Hi there,\n",
      "\n",
      "I find that `_prep_inputs_solar_pos` method has been both called in [`prepare_inputs`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs) and [`prepare_inputs_from_poa`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs_from_poa). However, the former takes an additional argument, press_temp that contains temperature pulled from the weather data provided outside. For the default `nrel_numpy` algorithm, I further checked its input requirement is [avg. yearly air temperature in degrees C](https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.solarposition.spa_python.html#pvlib.solarposition.spa_python) rather than the instantaneous temperature provided in weather. Hence I would like to ask if the following codes in `prepare_inputs` are redundant at least for the default 'nrel_numpy' algorithm?\n",
      "```\n",
      "        # build kwargs for solar position calculation\n",
      "        try:\n",
      "            press_temp = _build_kwargs(['pressure', 'temp_air'], weather)\n",
      "            press_temp['temperature'] = press_temp.pop('temp_air')\n",
      "        except KeyError:\n",
      "            pass\n",
      "```\n",
      "And thereby we change `self._prep_inputs_solar_pos(press_temp)` to `self._prep_inputs_solar_pos()` in `prepare_inputs`?\n",
      "\n",
      "Meanwhile, does the temperature really matter? How much uncertainty will it cause in the calculation of the sun's position? Should we provide avg. local temperature data if for a global modelling purpose?\n",
      "\n",
      "Any help would be appreciated!\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 139 lines----------------\n",
      "Add a model for spectral corrections\n",
      "**Additional context**\n",
      "First of all, I introduce myself, my name is Jose Antonio Caballero, and I have recently finished my PhD in photovoltaic engineering at the University of JaÃ©n, Spain.\n",
      "\n",
      "I have developed a python script to apply spectral corrections as a function of AM, AOD, PW based on this work (https://doi.org/10.1109/jphotov.2017.2787019).\n",
      "\n",
      "We have found that in pvlib there is already a similar methodology developed by First solar, in which the spectral corrections are based only on the AM and PW parameters, so we intend to include our proposed method in pvlib in a similar way.\n",
      "\n",
      "As an example, I attach the code developed in python (.zip file) to estimate the spectral effects related to different flat photovoltaic technologies from the AM, AOD and PW parameters included in a .csv file.\n",
      "[PV-MM-AM_AOD_PW_data.csv](https://github.com/pvlib/pvlib-python/files/6970716/PV-MM-AM_AOD_PW_data.csv)\n",
      "[PV_Spectral_Corrections.zip](https://github.com/pvlib/pvlib-python/files/6970727/PV_Spectral_Corrections.zip)\n",
      "\n",
      "Kind regards\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 67 lines----------------\n",
      "`pvlib.iotools.get_pvgis_hourly`'s `surface_azimuth` parameter doesn't use pvlib's azimuth convention\n",
      "Nearly everything in pvlib represents azimuth angles as values in [0, 360) clockwise from north, except `pvlib.iotools.get_pvgis_hourly`:\n",
      "\n",
      "https://github.com/pvlib/pvlib-python/blob/3def7e3375002ee3a5492b7bc609d3fb63a8edb1/pvlib/iotools/pvgis.py#L79-L81\n",
      "\n",
      "This inconsistency is a shame.  However, I don't see any way to switch it to pvlib's convention without a hard break, which is also a shame.  I wonder how others view the cost/benefit analysis here.\n",
      "\n",
      "See also https://github.com/pvlib/pvlib-python/pull/1395#discussion_r1181853794\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 41 lines----------------\n",
      "It should be impossible to instantiate a PVSystem with no Arrays\n",
      "**Describe the bug**\n",
      "It should be impossible to instantiate a `PVSystem` with no `Arrays`. Currently this is possible via `PVSystem(arrays=[])`.\n",
      "\n",
      "**To Reproduce**\n",
      "Steps to reproduce the behavior:\n",
      "```python\n",
      "from pvlib import pvsystem\n",
      "pvsystem.PVSystem(arrays=[])\n",
      "```\n",
      "results in this PVSystem:\n",
      "```\n",
      "PVSystem:\n",
      "  name: None\n",
      "  inverter: None\n",
      "```\n",
      "**Expected behavior**\n",
      "A `ValueError` should be raised indicating that a PVSystem must have at least one `Array` and suggesting that a system with an arbitrary default array can be constructed by passing `arrays=None` or not passing the `arrays` parameter at all.\n",
      "\n",
      "**Versions:**\n",
      " - ``pvlib.__version__``: 0.8.1+\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 227 lines----------------\n",
      "Add recombination current params to all bishop88 functions\n",
      "The changes made in #163 incorporate recombination current into the `bishop88()` function.  Functions that build on the `bishop88()` function should likewise accept these parameters.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 137 lines----------------\n",
      "access private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "someone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\n",
      "\n",
      "```python\n",
      ">>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\n",
      ">>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\n",
      "        pvgis_data = _parse_pvgis_tmy_csv(f)\n",
      "```\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "If I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "I was able to do it by just calling the private function and it worked, so that's an alternative also\n",
      "\n",
      "**Additional context**\n",
      "related to #845 and #849 \n",
      "\n",
      "access private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n",
      "**Is your feature request related to a problem? Please describe.**\n",
      "someone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\n",
      "\n",
      "```python\n",
      ">>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\n",
      ">>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\n",
      "        pvgis_data = _parse_pvgis_tmy_csv(f)\n",
      "```\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "If I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\n",
      "\n",
      "**Describe alternatives you've considered**\n",
      "I was able to do it by just calling the private function and it worked, so that's an alternative also\n",
      "\n",
      "**Additional context**\n",
      "related to #845 and #849 \n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 166 lines----------------\n",
      "remove **kwargs from PVSystem, ModelChain, Location\n",
      "These objects accept arbitrary kwargs so that users can be lazy about splatting dictionaries into the object constructors. I guess this is nice in some situations. But it also leads to bugs when users mistype a parameter name because python doesn't raise an exception. I ran into this when working on #1022 and #1027. \n",
      "\n",
      "I propose that we remove the kwargs without deprecation in 0.8.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 254 lines----------------\n",
      "Altitude lookup table\n",
      "Currently, altitude for `pvlib.location` based algorithms defaults to zero, but if we include a low-resolution altitude lookup, we can provide better results when altitude is not specified.\n",
      "We can make this altitude lookup the same format as [LinkeTurbidities.h5](https://github.com/pvlib/pvlib-python/blob/master/pvlib/data/LinkeTurbidities.h5), so it wouldn't require that much new code or any new dependencies.\n",
      "I was able to build an altitude map using [open data aggregated by tilezen](https://github.com/tilezen/joerd/blob/master/docs/data-sources.md). My test H5 file is currently `13 mb` using `4320x2160` resolution, `uint16` altitude, and `gzip` compression. We are free to distribute this data, but we do need to do is add [this attribution](https://github.com/tilezen/joerd/blob/master/docs/attribution.md) somewhere in the documentation.\n",
      "Would you guys be interested in this feature? Should I make a pull request?\n",
      "\n",
      "Here is a plot of my sample\n",
      "![altitude](https://user-images.githubusercontent.com/17040442/182914007-aedbdd53-5f74-4657-b0cb-60158b6aa26d.png)\n",
      ":\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 12 lines----------------\n",
      "The Perez diffuse model should not be forcing the horizon coefficient up to zero\n",
      "The perez model in irradiance.py forces F2, and thus the horizon component of diffuse, to be non-negative.  This restriction should not happen.  F2 and the horizon coefficient should be allowed to be negative and to reduce overall diffuse.\n",
      "\n",
      "According to the original paper at https://www.osti.gov/servlets/purl/7024029\n",
      "Section III.2 states this explicitly for the horizon component:\n",
      "\"(2) The horizon brightening coefficient, F2, is negative for overcast and low E occurrences -- indicative of brightening of the zenithal region of the sky for these conditions. This becomes positive past intermediate conditions and increases substantially with clearness.\"\n",
      "\n",
      "We observed a higher than expected POAI, coming from poa diffuse, on cloudy days at certain sites.\n",
      "Expected:\n",
      "Horizon (burgundy) can be less than zero and sky diffuse (green) is less than isotropic (blue)\n",
      "![image](https://user-images.githubusercontent.com/81724637/119172295-9ebc7900-ba1a-11eb-8e1a-3a170e1f995a.png)\n",
      "\n",
      "Observed from PVLib:\n",
      "Horizon is prevented from being negative and sky diffuse ends up higher than isotropic.\n",
      "![image](https://user-images.githubusercontent.com/81724637/119172410-c4498280-ba1a-11eb-8e91-540db0ddc609.png)\n",
      "\n",
      "Repro'd on PVLib 0.8.1\n",
      "\n",
      "See added test case in the PR for this repro case.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 52 lines----------------\n",
      "Update get_cams protocol to https\n",
      "According to an email sent out by Transvalor on May 12th 2022, the SoDa websites and services will switch from using \"HTTP\" to only \"HTTPS\". \n",
      "\n",
      "The existing HTTP endpoints will redirect to the correct HTTPS sites, hence without any changes the [`get_cams`](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html) function will continue to work correctly (as the request package handles redirects automatically). However, for good practice and to avoid unnecessary redirects, updating the existing URLs and endpoints to HTTPS is surely a good idea:\n",
      "https://github.com/pvlib/pvlib-python/blob/a0812b12584cfd5e662fa5aeb8972090763a671f/pvlib/iotools/sodapro.py#L188\n",
      "\n",
      "<br>\n",
      "For reference, here's a screen-shot of Transvalor email:\n",
      "\n",
      "![image](https://user-images.githubusercontent.com/39184289/168595497-095c17c1-3fec-43c9-b6fd-49c928b51d78.png)\n",
      "\n",
      "FYI: this is a good first issue to tackle to get familiar with contribution to pvlib :) \n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 125 lines----------------\n",
      "change eta_m to module_efficiency\n",
      "`temperature.noct_sam` uses `eta_m_ref` to describe the module efficiency at reference conditions and `temperature.pvsyst_cell` uses `eta_m` to describe the module efficiency generically.\n",
      "\n",
      "Just calling both of these `module_efficiency` would make the function signatures easily understandable by many more people. I'd be ok with `module_efficiency_ref` but I don't think that precision is very important.\n",
      "\n",
      "I skimmed [pvterms](https://duramat.github.io/pv-terms/) and didn't see a suggestion for this quantity.\n",
      "\n",
      "`temperature.noct_sam` has not yet been released and it's just a positional argument, so changing the name is trivial. `temperature.pvsyst_cell` would need a deprecation cycle.\n",
      "\n",
      "Originally discussed in https://github.com/pvlib/pvlib-python/pull/1177#discussion_r589081257\n",
      "\n",
      "Assignment of milestone indicates that we will act on this or close it forever before 0.9 is released.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pvlib/pvlib-python----------------\n",
      "-------------Fix: 16 lines----------------\n",
      "pvlib.irradiance.reindl() model generates NaNs when GHI = 0\n",
      "**Describe the bug**\n",
      "The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \n",
      "\n",
      "**Expected behavior**\n",
      "The reindl function should result in zero sky diffuse when GHI is zero.\n",
      "\n",
      "\n",
      "pvlib.irradiance.reindl() model generates NaNs when GHI = 0\n",
      "**Describe the bug**\n",
      "The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \n",
      "\n",
      "**Expected behavior**\n",
      "The reindl function should result in zero sky diffuse when GHI is zero.\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 913 lines----------------\n",
      "Consider creating a ``UninferableType`` or ``_Uninferable`` class\n",
      "I opened https://github.com/microsoft/pyright/issues/3641 as I wondered why `pyright` didn't recognise how we type `Uninferable`. Normally they are a little bit more up to date than `mypy` so I wondered if this was intentional.\n",
      "\n",
      "Turns out it is. According to them, the way we currently handle the typing of `Uninferable` is incorrect and should ideally be refactored.\n",
      "As we're stille early days into the typing of `astroid` I think there is still chance to do this.\n",
      "\n",
      "Their suggestion is to create a private class (`_Uninferable`) which `Uninferable` can then instantiate. One of the issues with this is that we tend to require `Uninferable` as a type in `pylint` as well and so we would need to import that private class as well.\n",
      "We could also create a public class, perhaps suffixed with `Type`, which we document as only being useful for typing.\n",
      "\n",
      "Let me know if you guys thinks this is something we should do and what approach is best.\n",
      "\n",
      "/CC @cdce8p As you're likely interested in this.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 40 lines----------------\n",
      "Infer calls to str.format() on names\n",
      "Future enhancement could infer this value instead of giving an empty string:\n",
      "\n",
      "```python\n",
      "from astroid import extract_node\n",
      "call = extract_node(\"\"\"\n",
      "x = 'python is {}'\n",
      "x.format('helpful sometimes')\n",
      "\"\"\")\n",
      "call.inferred()[0].value  # gives \"\"\n",
      "```\n",
      "\n",
      "_Originally posted by @jacobtylerwalls in https://github.com/PyCQA/astroid/pull/1602#discussion_r893423433_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 57 lines----------------\n",
      "Deprecation warnings from numpy\n",
      "### Steps to reproduce\n",
      "\n",
      "1. Run pylint over the following test case:\n",
      "\n",
      "```\n",
      "\"\"\"Test case\"\"\"\n",
      "\n",
      "import numpy as np\n",
      "value = np.random.seed(1234)\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "```\n",
      "/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
      "  getattr(sys.modules[modname], name)\n",
      "/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
      "  getattr(sys.modules[modname], name)\n",
      "```\n",
      "\n",
      "### Expected behavior\n",
      "There should be no future warnings.\n",
      "\n",
      "### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\n",
      "2.12.13\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 21 lines----------------\n",
      "error during inference of class inheriting from another with `mod.Type` format\n",
      "Consider package a `level` with a class `Model` defined in `level`'s `__init__.py` file.\n",
      "\n",
      "```\n",
      "class Model:\n",
      "    data: int = 1\n",
      "```\n",
      "\n",
      "If a class `Test` inherits from `Model` as `class Test(Model)`, and `Model` comes from `from level import Model`,  then inferring `Test.data` works fine (below, A is an alias for astroid).\n",
      "\n",
      "<img width=\"248\" alt=\"Screen Shot 2021-02-19 at 09 41 09\" src=\"https://user-images.githubusercontent.com/2905588/108505730-9b3c1900-7296-11eb-8bb8-5b66b7253cf4.png\">\n",
      "\n",
      "However, if a `Test` inherits from `Model` as `class Test(level.Model)` and `level` comes from `import level`, then inference triggers an exception.\n",
      "\n",
      "<img width=\"784\" alt=\"Screen Shot 2021-02-19 at 09 42 09\" src=\"https://user-images.githubusercontent.com/2905588/108505815-beff5f00-7296-11eb-92a2-641be827e1f0.png\">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n",
      "### Steps to reproduce\n",
      "\n",
      "Install PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "With astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\n",
      "\n",
      "```pytb\n",
      "Exception on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\n",
      "    callback(astroid)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\n",
      "    self._check_module_attrs(node, module, name.split(\".\"))\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\n",
      "    module = next(module.getattr(name)[0].infer())\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\n",
      "    result = [self.import_module(name, relative_only=True)]\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\n",
      "    return AstroidManager().ast_from_module_name(\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\n",
      "    return self.ast_from_module(named_module, modname)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\n",
      "    return AstroidBuilder(self).module_build(module, modname)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\n",
      "    node = self.inspect_build(module, modname=modname, path=path)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\n",
      "    self.object_build(node, module)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\n",
      "    elif hasattr(member, \"__all__\"):\n",
      "RuntimeError: wrapped C/C++ object of type QApplication has been deleted\n",
      "x.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\n",
      "```\n",
      "\n",
      "It looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\n",
      "\n",
      "Bisected to:\n",
      "\n",
      "- #1885 \n",
      "\n",
      "It looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\n",
      "\n",
      "cc @nickdrozd \n",
      "\n",
      "(took the freedom to add appropriate labels already, hope that's fine)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 243 lines----------------\n",
      "`.arguments` property ignores keyword-only args, *args, and **kwargs\n",
      "```python\n",
      ">>> from astroid import extract_node\n",
      ">>> node = extract_node(\"\"\"def a(*args, b=None, c=None, **kwargs): ...\"\"\")\n",
      ">>> node.args.arguments\n",
      "[]\n",
      "```\n",
      "\n",
      "Expected to find all the arguments from the function signature.\n",
      "\n",
      "The wanted data can be found here:\n",
      "\n",
      "```python\n",
      ">>> node.args.vararg\n",
      "'args'\n",
      ">>> node.args.kwarg\n",
      "'kwargs'\n",
      ">>> node.args.kwonlyargs\n",
      "[<AssignName.b l.1 at 0x1048189b0>, <AssignName.c l.1 at 0x104818830>]\n",
      "```\n",
      "\n",
      "Discussed at https://github.com/pylint-dev/pylint/pull/7577#discussion_r989000829.\n",
      "\n",
      "Notice that positional-only args are found for some reason ðŸ¤· \n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 57 lines----------------\n",
      "@property members defined in metaclasses of a base class are not correctly inferred\n",
      "Ref https://github.com/PyCQA/astroid/issues/927#issuecomment-817244963\n",
      "\n",
      "Inference works on the parent class but not the child in the following example:\n",
      "\n",
      "```python\n",
      "class BaseMeta(type):\n",
      "    @property\n",
      "    def __members__(cls):\n",
      "        return ['a', 'property']\n",
      "class Parent(metaclass=BaseMeta):\n",
      "    pass\n",
      "class Derived(Parent):\n",
      "    pass\n",
      "Parent.__members__  # [<Set.set l.10 at 0x...>]\n",
      "Derived.__members__  # [<Property.__members__ l.8 at 0x...>]\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 318 lines----------------\n",
      "Replace `cachedproperty` with `functools.cached_property` (>= 3.8)\n",
      "I thought about this PR recently again. Typing `cachedproperty` might not work, but it can be replaced with `functools.cached_property`. We only need to `sys` guard it for `< 3.8`. This should work\n",
      "```py\n",
      "if sys.version_info >= (3, 8):\n",
      "    from functools import cached_property\n",
      "else:\n",
      "    from astroid.decorators import cachedproperty as cached_property\n",
      "```\n",
      "\n",
      "Additionally, the deprecation warning can be limited to `>= 3.8`.\n",
      "\n",
      "_Originally posted by @cdce8p in https://github.com/PyCQA/astroid/issues/1243#issuecomment-1052834322_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "MRO failure on Python 3.7 with typing_extensions\n",
      "### Steps to reproduce\n",
      "\n",
      "Run the following script on Python 3.7:\n",
      "\n",
      "```python\n",
      "from astroid import parse\n",
      "module = parse(\"\"\"\n",
      "import abc\n",
      "import typing\n",
      "import dataclasses\n",
      "\n",
      "import typing_extensions\n",
      "\n",
      "T = typing.TypeVar(\"T\")\n",
      "\n",
      "class MyProtocol(typing_extensions.Protocol): pass\n",
      "class EarlyBase(typing.Generic[T], MyProtocol): pass\n",
      "class Base(EarlyBase[T], abc.ABC): pass\n",
      "class Final(Base[object]): pass\n",
      "\"\"\")\n",
      "\n",
      "#                    typing.Protocol\n",
      "#                          |\n",
      "# typing.Generic[T]    MyProtocol\n",
      "#              \\       /\n",
      "#              EarlyBase     abc.ABC\n",
      "#                       \\    /\n",
      "#                        Base\n",
      "#                         |\n",
      "#                        Final\n",
      "\n",
      "final_def = module.body[-1]\n",
      "final_def.mro()\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"xxx.py\", line 31, in <module>\n",
      "    print(\"mro:\", final_def.mro())\n",
      "  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 3009, in mro\n",
      "    return self._compute_mro(context=context)\n",
      "  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2985, in _compute_mro\n",
      "    mro = base._compute_mro(context=context)\n",
      "  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2999, in _compute_mro\n",
      "    return _c3_merge(unmerged_mro, self, context)\n",
      "  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 103, in _c3_merge\n",
      "    context=context,\n",
      "astroid.exceptions.InconsistentMroError: Cannot create a consistent method resolution order for MROs (tuple, object), (EarlyBase, tuple, Generic, object, MyProtocol), (ABC, object), (tuple, EarlyBase, ABC) of class <ClassDef.Base l.1347 at 0x7fa0efd52590>.\n",
      "```\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "No MRO error is raised; Python 3.7 doesn't raise an error.\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "2.6.7-dev0; the test case fails in pylint 2.9.6 and on the main branch at commit 6e8699cef0888631bd827b096533fc6e894d2fb2.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 23 lines----------------\n",
      "Invalid variable lookup when walrus operator is used\n",
      "### Steps to reproduce\n",
      "1. Consider following code in `loop_error.py`:\n",
      "\t```\n",
      "    \"\"\"Test module\"\"\"\n",
      "\n",
      "\n",
      "\tdef walrus_in_comprehension_test_2(some_path, module_namespace):\n",
      "\t    \"\"\"Suspected error\"\"\"\n",
      "\t    for mod in some_path.iterdir():\n",
      "\t        print(mod)\n",
      "\t\n",
      "\t    for org_mod in some_path.iterdir():\n",
      "\t        if org_mod.is_dir():\n",
      "\t            if mod := module_namespace.get_mod_from_alias(org_mod.name):\n",
      "\t                new_name = mod.name\n",
      "\t            else:\n",
      "\t                new_name = org_mod.name\n",
      "\t\n",
      "\t            print(new_name)\n",
      "\t```\n",
      "2. Run `pylint ./loop_error.py`\n",
      "\n",
      "### Current behavior\n",
      "A warning appears: ```W0631: Using possibly undefined loop variable 'mod' (undefined-loop-variable)```\n",
      "\n",
      "### Expected behavior\n",
      "No warning, because the variable `mod` is always defined.\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "- 2.14.1\n",
      "- 2.15.0-dev0 on 56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab with Python 3.10.6\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 16 lines----------------\n",
      "Cython module with import triggers deep introspection for pandas, raises unhandled FutureWarning\n",
      "This is a somewhat complicated situation to reproduce, but basically `pandas` throws `FutureWarning`s for certain attributes, and when you import it into a Cython module (triggering astroid's deep module inspection), these future warnings are not handled by astroid and bubble up as `AstroidError`s through to pylint. Here is a full repro:\n",
      "\n",
      "\n",
      "### Cython module `pyx.pyx`\n",
      "\n",
      "```python\n",
      "# distutils: language = c++\n",
      "import pandas as pd\n",
      "\n",
      "cdef class Test:\n",
      "    def __cinit__(self):\n",
      "        ...\n",
      "```\n",
      "\n",
      "\n",
      "### Python module `test.py`\n",
      "\n",
      "```python\n",
      "import pyx\n",
      "\n",
      "pyx.Test()\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Commands\n",
      "```\n",
      "cythonize -a -i pyx.pyx\n",
      "pylint --extension-pkg-allow-list=pyx,pandas test.py\n",
      "```\n",
      "\n",
      "\n",
      "### Exception\n",
      "```\n",
      "Exception on node <Import l.1 at 0x106b23ca0> in file '/Users/timkpaine/Programs/projects/other/astroid/test.py'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 765, in _get_imported_module\n",
      "    return importnode.do_import_module(modname)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/mixins.py\", line 102, in do_import_module\n",
      "    return mymodule.import_module(\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\n",
      "    return AstroidManager().ast_from_module_name(absmodname)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 168, in ast_from_module_name\n",
      "    return self.ast_from_module(module, modname)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 265, in ast_from_module\n",
      "    return AstroidBuilder(self).module_build(module, modname)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/builder.py\", line 91, in module_build\n",
      "    node = self.inspect_build(module, modname=modname, path=path)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 311, in inspect_build\n",
      "    self.object_build(node, module)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 367, in object_build\n",
      "    self.object_build(module, member)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 325, in object_build\n",
      "    member = getattr(obj, name)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/pandas/__init__.py\", line 198, in __getattr__\n",
      "    warnings.warn(\n",
      "FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\n",
      "    callback(astroid)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 472, in visit_import\n",
      "    imported_module = self._get_imported_module(node, name)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 788, in _get_imported_module\n",
      "    raise astroid.AstroidError from e\n",
      "astroid.exceptions.AstroidError\n",
      "************* Module test\n",
      "test.py:1:0: F0002: test.py: Fatal error while checking 'test.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/timkpaine/Library/Caches/pylint/pylint-crash-2022-07-19-17.txt'. (astroid-error)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Standalone (Non Cython) repro for convenience\n",
      "\n",
      "```python\n",
      "import types\n",
      "import pandas as pd\n",
      "from astroid.builder import AstroidBuilder\n",
      "\n",
      "\n",
      "m = types.ModuleType(\"test\")\n",
      "m.pd = pd\n",
      "\n",
      "AstroidBuilder().module_build(m, \"test\")\n",
      "```\n",
      "\n",
      "\n",
      "xref: https://github.com/PyCQA/pylint/issues/7205\n",
      "xref: https://github.com/PyCQA/astroid/pull/1719\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Crash when inferring `str.format` call involving unpacking kwargs\n",
      "When parsing the following file:\n",
      "\n",
      "<!--\n",
      " If sharing the code is not an option, please state so,\n",
      " but providing only the stacktrace would still be helpful.\n",
      " -->\n",
      "\n",
      "```python\n",
      "class A:\n",
      "    def render(self, audit_log_entry: AuditLogEntry):\n",
      "        return \"joined team {team_slug}\".format(**audit_log_entry.data)\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "pylint crashed with a ``AstroidError`` and with the following stacktrace:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/.../astroid/astroid/inference_tip.py\", line 38, in _inference_tip_cached\n",
      "    result = _cache[func, node]\n",
      "KeyError: (<function _infer_str_format_call at 0x1064a96c0>, <Call l.3 at 0x106c452d0>)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 731, in _check_file\n",
      "    check_astroid_module(ast_node)\n",
      "  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 950, in check_astroid_module\n",
      "    retval = self._check_astroid_module(\n",
      "  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 1000, in _check_astroid_module\n",
      "    walker.walk(node)\n",
      "  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\n",
      "    self.walk(child)\n",
      "  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\n",
      "    self.walk(child)\n",
      "  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 90, in walk\n",
      "    callback(astroid)\n",
      "  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 170, in visit_functiondef\n",
      "    inferred = _safe_infer_call_result(node, node)\n",
      "  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 31, in _safe_infer_call_result\n",
      "    value = next(inferit)\n",
      "  File \"/Users/.../astroid/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1752, in infer_call_result\n",
      "    yield from returnnode.value.infer(context)\n",
      "  File \"/Users/.../astroid/astroid/nodes/node_ng.py\", line 159, in infer\n",
      "    results = list(self._explicit_inference(self, context, **kwargs))\n",
      "  File \"/Users/.../astroid/astroid/inference_tip.py\", line 45, in _inference_tip_cached\n",
      "    result = _cache[func, node] = list(func(*args, **kwargs))\n",
      "  File \"/Users/.../astroid/astroid/brain/brain_builtin_inference.py\", line 948, in _infer_str_format_call\n",
      "    formatted_string = format_template.format(*pos_values, **keyword_values)\n",
      "KeyError: 'team_slug'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 688, in _check_files\n",
      "    self._check_file(get_ast, check_astroid_module, file)\n",
      "  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 733, in _check_file\n",
      "    raise astroid.AstroidError from e\n",
      "astroid.exceptions.AstroidError\n",
      "```\n",
      "***\n",
      "cc @DanielNoord in #1602 \n",
      "found by pylint primer ðŸš€ \n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 83 lines----------------\n",
      "Pyreverse regression after #857 (astroid 2.5)\n",
      "### Steps to reproduce\n",
      "1. Checkout pylint's source (which contains pyreverse)\n",
      "1. cd `<pylint checkout>` \n",
      "2. Run `source .tox/py39/bin/activate` or similar (you may need to run a tox session first)\n",
      "3. Ensure you have `astroid` ac2b173bc8acd2d08f6b6ffe29dd8cda0b2c8814 or later\n",
      "4. Ensure you have installed `astroid` (`python3 -m pip install -e <path-to-astroid>`) as dependencies may be different\n",
      "4. Run `pyreverse --output png --project test tests/data`\n",
      "\n",
      "### Current behaviour\n",
      "A `ModuleNotFoundError` exception is raised.\n",
      "\n",
      "```\n",
      "$ pyreverse --output png --project test tests/data\n",
      "parsing tests/data/__init__.py...\n",
      "parsing /opt/contrib/pylint/pylint/tests/data/suppliermodule_test.py...\n",
      "parsing /opt/contrib/pylint/pylint/tests/data/__init__.py...\n",
      "parsing /opt/contrib/pylint/pylint/tests/data/clientmodule_test.py...\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/bin/pyreverse\", line 8, in <module>\n",
      "    sys.exit(run_pyreverse())\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/__init__.py\", line 39, in run_pyreverse\n",
      "    PyreverseRun(sys.argv[1:])\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 201, in __init__\n",
      "    sys.exit(self.run(args))\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 219, in run\n",
      "    diadefs = handler.get_diadefs(project, linker)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 236, in get_diadefs\n",
      "    diagrams = DefaultDiadefGenerator(linker, self).visit(project)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\n",
      "    self.visit(local_node)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\n",
      "    methods[0](node)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 162, in visit_module\n",
      "    self.linker.visit(node)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\n",
      "    self.visit(local_node)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\n",
      "    methods[0](node)\n",
      "  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/inspector.py\", line 257, in visit_importfrom\n",
      "    relative = astroid.modutils.is_relative(basename, context_file)\n",
      "  File \"/opt/contrib/pylint/astroid/astroid/modutils.py\", line 581, in is_relative\n",
      "    parent_spec = importlib.util.find_spec(name, from_file)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.2_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/util.py\", line 94, in find_spec\n",
      "    parent = __import__(parent_name, fromlist=['__path__'])\n",
      "ModuleNotFoundError: No module named 'pylint.tests'\n",
      "```\n",
      "\n",
      "### Expected behaviour\n",
      "No exception should be raised. Prior to #857 no exception was raised.\n",
      "\n",
      "```\n",
      "$ pyreverse --output png --project test tests/data\n",
      "parsing tests/data/__init__.py...\n",
      "parsing /opt/contributing/pylint/tests/data/suppliermodule_test.py...\n",
      "parsing /opt/contributing/pylint/tests/data/__init__.py...\n",
      "parsing /opt/contributing/pylint/tests/data/clientmodule_test.py...\n",
      "```\n",
      "\n",
      "### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\n",
      "`2.6.0-dev0` (cab9b08737ed7aad2a08ce90718c67155fa5c4a0)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 36 lines----------------\n",
      "ImportError: cannot import name 'Statement' from 'astroid.node_classes' \n",
      "### Steps to reproduce\n",
      "\n",
      "1. run pylint <some_file>\n",
      "\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "```python\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__main__.py\", line 9, in <module>\n",
      "    pylint.run_pylint()\n",
      "  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__init__.py\", line 24, in run_pylint\n",
      "    PylintRun(sys.argv[1:])\n",
      "  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/run.py\", line 331, in __init__\n",
      "    linter.load_plugin_modules(plugins)\n",
      "  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/pylinter.py\", line 551, in load_plugin_modules\n",
      "    module = astroid.modutils.load_module_from_name(modname)\n",
      "  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/modutils.py\", line 218, in load_module_from_name\n",
      "    return importlib.import_module(dotted_name)\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 855, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/user/folder/check_mk/tests/testlib/pylint_checker_cmk_module_layers.py\", line 14, in <module>\n",
      "    from astroid.node_classes import Import, ImportFrom, Statement  # type: ignore[import]\n",
      "ImportError: cannot import name 'Statement' from 'astroid.node_classes' (/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/node_classes.py)\n",
      "```\n",
      "\n",
      "### Expected behavior\n",
      "No exception\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "2.7.3\n",
      "pylint 2.10.2\n",
      "astroid 2.7.3\n",
      "Python 3.9.5 (default, May 11 2021, 08:20:37) \n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 150 lines----------------\n",
      "astroid has an undeclared dependency on setuptools.\n",
      "The dependency is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/astroid/__pkginfo__.py#L29\n",
      "\n",
      "The lack of declaration is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/setup.cfg#L37-L41\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 27 lines----------------\n",
      "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n",
      "### Steps to reproduce\n",
      "> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\n",
      "\n",
      "> Update 2022-01-04: Corrected repro steps and added more environment details\n",
      "\n",
      "1. Set up simple repo with following structure (all files can be empty):\n",
      "```\n",
      "root_dir/\n",
      "|--src/\n",
      "|----project/ # Notice the missing __init__.py\n",
      "|------file.py # It can be empty, but I added `import os` at the top\n",
      "|----__init__.py\n",
      "```\n",
      "2. Open a command prompt\n",
      "3. `cd root_dir`\n",
      "4. `python -m venv venv`\n",
      "5. `venv/Scripts/activate`\n",
      "6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\n",
      "7. `pylint src/project` # Updated from `pylint src`\n",
      "8. Observe failure:\n",
      "```\n",
      "src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "Fails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\n",
      "\n",
      "### Expected behavior\n",
      "Does not fail with error.\n",
      "> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "2.9.1\n",
      "\n",
      "`python 3.9.1`\n",
      "`pylint 2.12.2 `\n",
      "\n",
      "\n",
      "\n",
      "This issue has been observed with astroid `2.9.1` and `2.9.2`\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 14 lines----------------\n",
      "Unhandled AttributeError during str.format template evaluation\n",
      "### Steps to reproduce\n",
      "\n",
      "1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\n",
      "\n",
      "```py\n",
      "daniel_age = 12\n",
      "\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "`2.13.0-dev0`\n",
      "\n",
      "Relates to pycqa/pylint#7939.\n",
      "Unhandled AttributeError during str.format template evaluation\n",
      "### Steps to reproduce\n",
      "\n",
      "1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\n",
      "\n",
      "```py\n",
      "daniel_age = 12\n",
      "\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "`2.13.0-dev0`\n",
      "\n",
      "Relates to pycqa/pylint#7939.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 41 lines----------------\n",
      "getitem does not infer the actual unpacked value\n",
      "When trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\n",
      "\n",
      "- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \n",
      "- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\n",
      "\n",
      "\n",
      "Here is a short reproducer;\n",
      "\n",
      "```py\n",
      "from astroid import parse\n",
      "\n",
      "\n",
      "source = \"\"\"\n",
      "X = {\n",
      "    'A': 'B'\n",
      "}\n",
      "\n",
      "Y = {\n",
      "    **X\n",
      "}\n",
      "\n",
      "KEY = 'A'\n",
      "\"\"\"\n",
      "\n",
      "tree = parse(source)\n",
      "\n",
      "first_dict = tree.body[0].value\n",
      "second_dict = tree.body[1].value\n",
      "key = tree.body[2].value\n",
      "\n",
      "print(f'{first_dict.getitem(key).value = }')\n",
      "print(f'{second_dict.getitem(key).value = }')\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "The current output;\n",
      "\n",
      "```\n",
      " $ python t1.py                                                                                                 3ms\n",
      "first_dict.getitem(key).value = 'B'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\n",
      "    print(f'{second_dict.getitem(key).value = }')\n",
      "  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\n",
      "    return value.getitem(index, context)\n",
      "AttributeError: 'Name' object has no attribute 'getitem'\n",
      "```\n",
      "\n",
      "Expeceted output;\n",
      "```\n",
      " $ python t1.py                                                                                                 4ms\n",
      "first_dict.getitem(key).value = 'B'\n",
      "second_dict.getitem(key).value = 'B'\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\n",
      "Astroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \n",
      "\n",
      "### Steps to reproduce\n",
      "\n",
      "```python\n",
      "from collections.abc import Mapping\n",
      "from typing import Generic, TypeVar, TypedDict\n",
      "from dataclasses import dataclass\n",
      "\n",
      "class Identity(TypedDict):\n",
      "    \"\"\"It's the identity.\"\"\"\n",
      "\n",
      "    name: str\n",
      "\n",
      "T = TypeVar(\"T\", bound=Mapping)\n",
      "\n",
      "@dataclass\n",
      "class Animal(Generic[T]):\n",
      "    \"\"\"It's an animal.\"\"\"\n",
      "\n",
      "    identity: T\n",
      "\n",
      "class Dog(Animal[Identity]):\n",
      "    \"\"\"It's a Dog.\"\"\"\n",
      "\n",
      "dog = Dog(identity=Identity(name=\"Dog\"))\n",
      "print(dog.identity[\"name\"])\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "Pylint (running Astroid 2.15.7) gives the following error for the example above:\n",
      "```\n",
      "E1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\n",
      "```\n",
      "### Expected behavior\n",
      "Astroid should correctly handle a subscriptable type parameter.\n",
      "\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "2.15.7\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "infer_stmts cannot infer multiple uses of the same AssignName\n",
      "Given multiple assignments to the same target which both reference the same AssignName, infer_stmts fails for subsequent attempts after the first.\n",
      "\n",
      "### Steps to reproduce\n",
      "\n",
      "This appears to be a minimum working example, removing any part removes the effect:\n",
      "\n",
      "```python\n",
      "fails = astroid.extract_node(\"\"\"\n",
      "    pair = [1, 2]\n",
      "    ex = pair[0]\n",
      "    if 1 + 1 == 2:\n",
      "        ex = pair[1]\n",
      "    ex\n",
      "\"\"\")\n",
      "print(list(fails.infer()))\n",
      "# [<Const.int l.2 at 0x...>, Uninferable]\n",
      "```\n",
      "\n",
      "For some context, I originally saw this with attributes on an imported module, i.e.\n",
      "\n",
      "```python\n",
      "import mod\n",
      "ex = mod.One()\n",
      "# later ... or in some branch\n",
      "ex = mod.Two()\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "See above.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "Inlining the variable or switching to a different name works fine:\n",
      "\n",
      "```python\n",
      "works = astroid.extract_node(\"\"\"\n",
      "    # pair = [1, 2]\n",
      "    ex = [1, 2][0]\n",
      "    if 1 + 1 == 2:\n",
      "        ex = [1, 2][1]\n",
      "    ex\n",
      "\"\"\")\n",
      "print(list(works.infer()))\n",
      "# [<Const.int l.3 at 0x...>, <Const.int l.5 at 0x...>]\n",
      "\n",
      "works = astroid.extract_node(\"\"\"\n",
      "    first = [1, 2]\n",
      "    second = [1, 2]\n",
      "    ex = first[0]\n",
      "    if 1 + 1 == 2:\n",
      "        ex = second[1]\n",
      "    ex\n",
      "\"\"\")\n",
      "print(list(works.infer()))\n",
      "# [<Const.int l.2 at 0x...>, <Const.int l.3 at 0x...>]\n",
      "```\n",
      "\n",
      "I would expect that the first failing example would work similarly. This (only) worked\n",
      "in astroid 2.5 and appears to have been \"broken\" by the revert of cc3bfc5 in 03d15b0 (astroid 2.5.1 and above).\n",
      "\n",
      "### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\n",
      "\n",
      "```\n",
      "$ python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"\n",
      "2.5-dev\n",
      "$ git rev-parse HEAD\n",
      "03d15b0f32f7d7c9b2cb062b9321e531bd954344\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 17 lines----------------\n",
      "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n",
      "### Steps to reproduce\n",
      "\n",
      "I have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "When running pylint on some code, I get this exception:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\n",
      "    callback(astroid)\n",
      "  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\n",
      "    inferred = _safe_infer_call_result(node, node)\n",
      "  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\n",
      "    value = next(inferit)\n",
      "  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\n",
      "    yield from returnnode.value.infer(context)\n",
      "  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\n",
      "    results = list(self._explicit_inference(self, context, **kwargs))\n",
      "  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\n",
      "    result = _cache[func, node] = list(func(*args, **kwargs))\n",
      "  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\n",
      "    formatted_string = format_template.format(*pos_values, **keyword_values)\n",
      "TypeError: unsupported format string passed to NoneType.__format__\n",
      "```\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "TypeError exception should not happen\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "2.12.10,\n",
      "2.12.12\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 90 lines----------------\n",
      "Yield self is inferred to be of a mistaken type \n",
      "### Steps to reproduce\n",
      "\n",
      "1. Run the following\n",
      "```\n",
      "import astroid\n",
      "\n",
      "\n",
      "print(list(astroid.parse('''\n",
      "import contextlib\n",
      "\n",
      "class A:\n",
      "    @contextlib.contextmanager\n",
      "    def get(self):\n",
      "        yield self\n",
      "\n",
      "class B(A):\n",
      "    def play():\n",
      "        pass\n",
      "\n",
      "with B().get() as b:\n",
      "    b.play()\n",
      "''').ilookup('b')))\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "```Prints [<Instance of .A at 0x...>]```\n",
      "\n",
      "### Expected behavior\n",
      "```Prints [<Instance of .B at 0x...>]```\n",
      "\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "2.6.2\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\n",
      "Astroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \n",
      "\n",
      "### Steps to reproduce\n",
      "\n",
      "```python\n",
      "from collections.abc import Mapping\n",
      "from typing import Generic, TypeVar, TypedDict\n",
      "from dataclasses import dataclass\n",
      "\n",
      "class Identity(TypedDict):\n",
      "    \"\"\"It's the identity.\"\"\"\n",
      "\n",
      "    name: str\n",
      "\n",
      "T = TypeVar(\"T\", bound=Mapping)\n",
      "\n",
      "@dataclass\n",
      "class Animal(Generic[T]):\n",
      "    \"\"\"It's an animal.\"\"\"\n",
      "\n",
      "    identity: T\n",
      "\n",
      "class Dog(Animal[Identity]):\n",
      "    \"\"\"It's a Dog.\"\"\"\n",
      "\n",
      "dog = Dog(identity=Identity(name=\"Dog\"))\n",
      "print(dog.identity[\"name\"])\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "Pylint (running Astroid 2.15.7) gives the following error for the example above:\n",
      "```\n",
      "E1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\n",
      "```\n",
      "### Expected behavior\n",
      "Astroid should correctly handle a subscriptable type parameter.\n",
      "\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "2.15.7\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 672 lines----------------\n",
      "Implement new nodes for PEP 695: Type Parameter Syntax\n",
      "There's a new syntax in python 3.12, we need to handle it before claiming we support 3.12, see https://docs.python.org/3.12/whatsnew/3.12.html#pep-695-type-parameter-syntax\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 57 lines----------------\n",
      "Cannot infer empty functions\n",
      "### Steps to reproduce\n",
      "```python\n",
      "import astroid\n",
      "astroid.extract_node(\"\"\"\n",
      "def f():\n",
      "    pass\n",
      "f()\n",
      "\"\"\").inferred()\n",
      "```\n",
      "### Current behavior\n",
      "raises `StopIteration`\n",
      "\n",
      "### Expected behavior\n",
      "Returns `[const.NoneType]`\n",
      "\n",
      "### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\n",
      "\n",
      "2.0.0\n",
      "\n",
      "This also applies to procedural functions which don't explicitly return any values.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 22 lines----------------\n",
      "'AsStringVisitor' object has no attribute 'visit_unknown'\n",
      "```python\n",
      ">>> import astroid\n",
      ">>> astroid.nodes.Unknown().as_string()\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\n",
      "    return AsStringVisitor()(self)\n",
      "  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\n",
      "    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\n",
      "  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\n",
      "    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\n",
      "AttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\n",
      ">>> \n",
      "```\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "2.8.6-dev0\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Delayed attribute assignment to object() may cause incorrect inference of instance attributes\n",
      "@cdce8p: `aiohttp` and `VLCTelnet` turned out to be red herrings. This case fails on current stable versions:\n",
      "\n",
      "```python\n",
      "class Example:\n",
      "    def prev(self):\n",
      "        pass\n",
      "    def next(self):\n",
      "        pass\n",
      "    def other(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "ex = Example()\n",
      "ex.other()  # no warning\n",
      "ex.prev()   # no warning\n",
      "ex.next()   # no warning\n",
      "\n",
      "import typing\n",
      "\n",
      "ex.other()  # no warning\n",
      "ex.prev()   # false-positive: not-callable\n",
      "ex.next()   # false-positive: not-callable\n",
      "```\n",
      "\n",
      "_Originally posted by @nelfin in https://github.com/PyCQA/astroid/issues/927#issuecomment-818626368_\n",
      "\n",
      "I've bisected this down to 78d5537. Pylint 2.3.1 passes this case with 20a7ae5 and fails with 78d5537\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 463 lines----------------\n",
      "Replace modutils.is_standard_module() logic with sys.stdlib_module_names\n",
      "\n",
      "This extends from the conversation in https://github.com/PyCQA/pylint/pull/8190.\n",
      "\n",
      "The logic in `modutils.is_standard_module()` should largely be able to be replaced with [sys.stdlib_module_names](https://docs.python.org/3/library/sys.html#sys.stdlib_module_names), which was introduced in 3.10. The advantages are it will be faster (no imports, no filesystem traversal), it's not dependent on the local environment,  and it's maintained upstream, generated from source. For the referenced PR, I backported the generating code in CPython to generate sets for a shim to support 3.7 - 3.9.\n",
      "\n",
      "I started working on a PR for Astroid, but it seems `modutils.is_standard_module()` actually does two different things depending on how it's called.\n",
      "1. If no path is specified, it tries to determine if a module is part of the standard library (or a builtin, or compiled in) by inspecting the path of module after importing it.\n",
      "2. If a path is specified, it does the same logic, but ultimately is determining if the module is in the path specified.\n",
      "\n",
      "For the second case, I could only find one case in the wild, in pyreverse.\n",
      "\n",
      "https://github.com/PyCQA/pylint/blob/5bc4cd9a4b4c240227a41786823a6f226864dc4b/pylint/pyreverse/inspector.py#L308\n",
      "\n",
      "These seem like different behaviors to me. I'm unsure how to proceed with PR. Here are some options I've considered.\n",
      "\n",
      "- Option 1:\n",
      "  - Introduce a new function, basically a wrapper for sys.stdlib_module_names and the shim\n",
      "  - Old behavior is preserved\n",
      "  - Advantage of a function, even though it's very simple, is it provides a space to add overriding logic if needed down the road\n",
      "   \n",
      "- Option 2:\n",
      "   - Only introduce the shim, so the code is in a common place for Astroid and Pylint\n",
      "   - Can be dropped with 3.9\n",
      "   - Old behavior is preserved\n",
      "\n",
      "- Option 3:\n",
      "  - Fall back to old behavior if a path is given, check sys.stdlib_module_names otherwise\n",
      "\n",
      "- Option 4:\n",
      "  - Deprecate `is_standard_module()`\n",
      "  - Introduce new functions more specific to how they are used\n",
      "\n",
      "- Option 5:\n",
      "  - Do Nothing\n",
      "\n",
      "I'm sure there are more options, but this is what comes to mind now. Would appreciate your thoughts and ideas.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 41 lines----------------\n",
      "``nodes.Module`` don't have a ``end_lineno`` and ``end_col_offset``\n",
      "### Steps to reproduce\n",
      "\n",
      "```python\n",
      "import astroid\n",
      "\n",
      "code = \"\"\"\n",
      "    print(\"a module\")\n",
      "    \"\"\"\n",
      "\n",
      "module = astroid.parse(code)\n",
      "print(module.end_lineno)\n",
      "print(module.end_col_offset)\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "`AttributeError` on both of the last lines.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "@cdce8p Let me know if I misunderstood you, but I thought we wanted these to be accessible on all nodes, just initialised as `None`.\n",
      "If that was not the case, I would make the case to do so as it allows you to do `node.end_lineno` without running in to `AttributeError`'s.\n",
      "\n",
      "### Version\n",
      "\n",
      "Latest `main`.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 40 lines----------------\n",
      "Decorator.toline is off by 1\n",
      "### Steps to reproduce\n",
      "\n",
      "I came across this inconsistency while debugging why pylint reports `missing-docstring` on the wrong line for the `g2` function in the example. As it turns out, the `toline` of the decorator seems to point to `b=3,` instead of `)`.\n",
      "\n",
      "```python\n",
      "import ast\n",
      "import astroid\n",
      "\n",
      "source = \"\"\"\\\n",
      "@f(a=2,\n",
      "   b=3,\n",
      ")\n",
      "def g2():\n",
      "    pass\n",
      "\"\"\"\n",
      "\n",
      "[f] = ast.parse(source).body\n",
      "[deco] = f.decorator_list\n",
      "print(\"ast\", deco.lineno, deco.end_lineno)\n",
      "\n",
      "[f] = astroid.parse(source).body\n",
      "[deco] = f.decorators.nodes\n",
      "print(\"astroid\", deco.fromlineno, deco.tolineno)\n",
      "```\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "```\n",
      "ast 1 3\n",
      "astroid 1 2\n",
      "```\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "```\n",
      "ast 1 3\n",
      "astroid 1 3\n",
      "```\n",
      "\n",
      "### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\n",
      "\n",
      "2.9.3\n",
      "\n",
      "-------------PROBLEM STATEMENT: pylint-dev/astroid----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n",
      "### Steps to reproduce\n",
      "\n",
      "Install PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\n",
      "\n",
      "### Current behavior\n",
      "\n",
      "With astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\n",
      "\n",
      "```pytb\n",
      "Exception on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\n",
      "    callback(astroid)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\n",
      "    self._check_module_attrs(node, module, name.split(\".\"))\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\n",
      "    module = next(module.getattr(name)[0].infer())\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\n",
      "    result = [self.import_module(name, relative_only=True)]\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\n",
      "    return AstroidManager().ast_from_module_name(\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\n",
      "    return self.ast_from_module(named_module, modname)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\n",
      "    return AstroidBuilder(self).module_build(module, modname)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\n",
      "    node = self.inspect_build(module, modname=modname, path=path)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\n",
      "    self.object_build(node, module)\n",
      "  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\n",
      "    elif hasattr(member, \"__all__\"):\n",
      "RuntimeError: wrapped C/C++ object of type QApplication has been deleted\n",
      "x.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\n",
      "```\n",
      "\n",
      "It looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\n",
      "\n",
      "Bisected to:\n",
      "\n",
      "- #1885 \n",
      "\n",
      "It looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\n",
      "\n",
      "cc @nickdrozd \n",
      "\n",
      "(took the freedom to add appropriate labels already, hope that's fine)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Confusing behaviour of ParametricEllipsoid\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "When creating a ParametricEllispoid using a direction of [0, 1, 0], the ellipsoid is rotated along the y axis.  \n",
      "For example if setting the direction to [1e-5, 1, 0], which corresponds to approximately similar direction, the ellipsoid displays then the correct behaviour.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "\n",
      "ellipsoid = pv.ParametricEllipsoid(300, 100, 10, direction=[0, 1, 0])\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "Date: Wed Sep 06 14:07:38 2023 CEST\n",
      "\n",
      "                OS : Linux\n",
      "            CPU(s) : 8\n",
      "           Machine : x86_64\n",
      "      Architecture : 64bit\n",
      "               RAM : 31.2 GiB\n",
      "       Environment : Jupyter\n",
      "       File system : ext4\n",
      "        GPU Vendor : Intel\n",
      "      GPU Renderer : Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "       GPU Version : 4.6 (Core Profile) Mesa 22.0.1\n",
      "  MathText Support : False\n",
      "\n",
      "  Python 3.8.13 (default, Apr 19 2022, 02:32:06)  [GCC 11.2.0]\n",
      "\n",
      "           pyvista : 0.41.1\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.4\n",
      "        matplotlib : 3.3.4\n",
      "            scooby : 0.7.2\n",
      "             pooch : v1.7.0\n",
      "           imageio : 2.31.1\n",
      "           IPython : 8.12.2\n",
      "        ipywidgets : 8.0.7\n",
      "             scipy : 1.10.1\n",
      "              tqdm : 4.65.0\n",
      "        jupyterlab : 3.6.5\n",
      "             trame : 2.5.2\n",
      "      trame_client : 2.10.0\n",
      "      trame_server : 2.11.7\n",
      "         trame_vtk : 2.5.8\n",
      "      nest_asyncio : 1.5.6\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "Here is the given ellipsoid\n",
      "![confusing_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/f0e1b5f7-eca1-4224-a020-df44385ed68b)\n",
      "Here is what is expected\n",
      "![expected_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/d4f67ead-9928-4af3-9c3a-b6121180b780)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 228 lines----------------\n",
      "Clean up and clarify sampling-like filters\n",
      "### Describe what maintenance you would like added.\n",
      "\n",
      "There was a discussion on slack on the use of sampling-like filters, i.e. `sample`, `probe`, and `interpolate`.  One issue is that it is hard to figure out when to use which filter.  The other issue is that `probe` has the opposite behavior of `sample` and `interpolate` in regards to order of operation (see below).\n",
      "\n",
      "### Links to source code.\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Pseudocode or Screenshots\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "\n",
      "small = pv.ImageData(dimensions=(5, 5, 5))\n",
      "large = pv.ImageData(dimensions=(10, 10, 10))\n",
      "print(small.n_points)\n",
      "print(large.n_points)\n",
      "print(small.probe(large).n_points)  # gives different result\n",
      "print(small.sample(large).n_points)\n",
      "print(small.interpolate(large).n_points)\n",
      "```\n",
      "\n",
      "\n",
      "This  gives\n",
      "\n",
      "```txt\n",
      "125\n",
      "1000\n",
      "1000\n",
      "125\n",
      "125\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 16 lines----------------\n",
      "Boolean Operation freezes/crashes \n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "Apparently, if two polyData have the exact same shape, their boolean operation freezes/crashes the application!\n",
      "\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "p1 = pv.Sphere().triangulate()\n",
      "p2 = pv.Sphere().triangulate()\n",
      "\n",
      "p1.boolean_intersection(p2)\n",
      "``````\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Tue Aug 22 12:17:01 2023 EEST\n",
      "\n",
      "                OS : Darwin\n",
      "            CPU(s) : 12\n",
      "           Machine : x86_64\n",
      "      Architecture : 64bit\n",
      "               RAM : 16.0 GiB\n",
      "       Environment : Jupyter\n",
      "       File system : apfs\n",
      "        GPU Vendor : ATI Technologies Inc.\n",
      "      GPU Renderer : AMD Radeon Pro 5300M OpenGL Engine\n",
      "       GPU Version : 4.1 ATI-4.14.1\n",
      "  MathText Support : False\n",
      "\n",
      "  Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0\n",
      "  (clang-1300.0.29.30)]\n",
      "\n",
      "           pyvista : 0.41.1\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.2\n",
      "        matplotlib : 3.7.1\n",
      "            scooby : 0.7.2\n",
      "             pooch : v1.7.0\n",
      "           IPython : 8.14.0\n",
      "             scipy : 1.10.1\n",
      "        jupyterlab : 4.0.5\n",
      "      nest_asyncio : 1.5.7\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 79 lines----------------\n",
      "Allow passing through cell data in `to_tetrahedra` method in RectilinearGrid\n",
      "### Describe the feature you would like to be added.\n",
      "\n",
      "No cell data is passed through when converting to a tetrahedra.  The user can currently request to pass through the original cell id, but it requires one more step to regenerate the cell data on the tetrahedralized mesh.\n",
      "\n",
      "### Links to VTK Documentation, Examples, or Class Definitions.\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Pseudocode or Screenshots\n",
      "\n",
      "Currently we have to do\n",
      "\n",
      "```python\n",
      "mesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\n",
      "tetra_mesh = mesh.to_tetrahedra(pass_cell_ids=True)\n",
      "tetra_mesh[\"cell_data\"] = mesh[\"cell_data\"][tetra_mesh.cell_data.active_scalars]\n",
      "```\n",
      "\n",
      "It would be better to do something like\n",
      "\n",
      "```python\n",
      "mesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\n",
      "tetra_mesh = mesh.to_tetrahedra(pass_cell_data=True)  # the prior code would occur inside the method\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 54 lines----------------\n",
      "Adding ``CircularArc``s together does not provide a line\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "Don't know if it can be considered a bug or not but...\n",
      "\n",
      "If you define two consecutive ``pv.CircularArc`` and you plot them, weird things start to appear with the new PyVista 0.39 version. Run the following code snippet using ``pyvista==0.38.6`` and ``pyvista==0.39.0``\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "\n",
      "\n",
      "# Define your arcs\n",
      "#\n",
      "#              Y       (s2)\n",
      "#              ^   ____(e1)____\n",
      "#              |  /             \\\n",
      "#              | /               \\\n",
      "#              |/                 \\\n",
      "#          (s1)O ------(c)-------(e2)----> X\n",
      "#\n",
      "#   Let's imagine the above is an arc from (0,0) to (10,10) and origin\n",
      "#   at (10,0); and another consecutive arc from (10,10) to (20,0) and\n",
      "#   origin at (10,0)\n",
      "#\n",
      "arc_1 = pv.CircularArc([0, 0, 0], [10, 10, 0], [10, 0, 0], negative=False)\n",
      "arc_2 = pv.CircularArc([10, 10, 0], [20, 0, 0], [10, 0, 0], negative=False)\n",
      "\n",
      "# ========== CRITICAL BEHAVIOR ==========\n",
      "# I add them together\n",
      "arc = arc_1 + arc_2\n",
      "# ========== CRITICAL BEHAVIOR ==========\n",
      "\n",
      "# Instantiate plotter\n",
      "pl = pv.Plotter()\n",
      "\n",
      "# Add the polydata\n",
      "pl.add_mesh(arc)\n",
      "\n",
      "# Plotter config: view from the top\n",
      "pl.view_vector(vector=[0, 0, 1], viewup=[0, 1, 0])\n",
      "\n",
      "# Plot\n",
      "pl.show()\n",
      "\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "For PyVista 0.38.6\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Thu May 11 13:49:09 2023 Romance Daylight Time\n",
      "\n",
      "                OS : Windows\n",
      "            CPU(s) : 16\n",
      "           Machine : AMD64\n",
      "      Architecture : 64bit\n",
      "       Environment : Python\n",
      "        GPU Vendor : Intel\n",
      "      GPU Renderer : Intel(R) UHD Graphics\n",
      "       GPU Version : 4.5.0 - Build 30.0.100.9955\n",
      "\n",
      "  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\n",
      "  bit (AMD64)]\n",
      "\n",
      "           pyvista : 0.38.6\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.3\n",
      "           imageio : 2.28.1\n",
      "            scooby : 0.7.2\n",
      "             pooch : v1.7.0\n",
      "        matplotlib : 3.7.1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "For PyVista 0.39.0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Thu May 11 13:50:00 2023 Romance Daylight Time\n",
      "\n",
      "                OS : Windows\n",
      "            CPU(s) : 16\n",
      "           Machine : AMD64\n",
      "      Architecture : 64bit\n",
      "       Environment : Python\n",
      "        GPU Vendor : Intel\n",
      "      GPU Renderer : Intel(R) UHD Graphics\n",
      "       GPU Version : 4.5.0 - Build 30.0.100.9955\n",
      "  MathText Support : False\n",
      "\n",
      "  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\n",
      "  bit (AMD64)]\n",
      "\n",
      "           pyvista : 0.39.0\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.3\n",
      "        matplotlib : 3.7.1\n",
      "            scooby : 0.7.2\n",
      "             pooch : v1.7.0\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "PyVista 0.39\n",
      "\n",
      "![PyVista 0.39](https://github.com/pyvista/pyvista/assets/37798125/87bda0a2-2eb7-4171-8005-239f5a4f27c2)\n",
      "\n",
      "PyVista 0.38.6\n",
      "\n",
      "![PyVista 0.38.6](https://github.com/pyvista/pyvista/assets/37798125/b6159e12-97bc-4768-9691-91005fadfb26)\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "``Multiblock``.plot does not work when using ``PointSet``\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "It seems ``MultiBlock`` entities made of ``PointSet`` plot nothing when using ``plot`` method.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "import numpy as np\n",
      "\n",
      "points_arr = np.array(\n",
      "    [\n",
      "        [0.0, 1.0, 0.0],\n",
      "        [0.0, 0.0, 0.0],\n",
      "        [1.0, 1.0, 0.0],\n",
      "        [1.0, 0.0, 0.0],\n",
      "        [0.0, 0.0, 1.0],\n",
      "        [1.0, 0.0, 1.0],\n",
      "        [1.0, 1.0, 1.0],\n",
      "        [0.0, 1.0, 1.0],\n",
      "    ]\n",
      ")\n",
      "\n",
      "points = pv.MultiBlock()\n",
      "for each_kp in points_arr:\n",
      "    points.append(pv.PointSet(each_kp))\n",
      "\n",
      "points.plot()\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Wed May 10 18:07:18 2023 CEST\n",
      "\n",
      "                OS : Darwin\n",
      "            CPU(s) : 8\n",
      "           Machine : arm64\n",
      "      Architecture : 64bit\n",
      "               RAM : 16.0 GiB\n",
      "       Environment : IPython\n",
      "       File system : apfs\n",
      "        GPU Vendor : Apple\n",
      "      GPU Renderer : Apple M2\n",
      "       GPU Version : 4.1 Metal - 83.1\n",
      "  MathText Support : False\n",
      "\n",
      "  Python 3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0\n",
      "  (clang-1400.0.29.202)]\n",
      "\n",
      "           pyvista : 0.39.0\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.3\n",
      "        matplotlib : 3.7.1\n",
      "            scooby : 0.7.1\n",
      "             pooch : v1.7.0\n",
      "           imageio : 2.28.0\n",
      "           IPython : 8.12.1\n",
      "        ipywidgets : 8.0.6\n",
      "             scipy : 1.10.1\n",
      "              tqdm : 4.65.0\n",
      "        jupyterlab : 3.6.3\n",
      "         pythreejs : 2.4.2\n",
      "      nest_asyncio : 1.5.6\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "<img width=\"624\" alt=\"image\" src=\"https://github.com/pyvista/pyvista/assets/28149841/a1b0999f-2d35-4911-a216-eb6503955860\">\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 102 lines----------------\n",
      "to_tetrahedra active scalars\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "#4311 passes cell data through the `to_tetrahedra` call. However, after these changes.  The active scalars information is lost.\n",
      "\n",
      "cc @akaszynski who implemented these changes in that PR.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```py\n",
      "import pyvista as pv\n",
      "import numpy as np\n",
      "mesh = pv.UniformGrid(dimensions=(10, 10, 10))\n",
      "mesh[\"a\"] = np.zeros(mesh.n_cells)\n",
      "mesh[\"b\"] = np.ones(mesh.n_cells)\n",
      "print(mesh.cell_data)\n",
      "tet = mesh.to_tetrahedra()\n",
      "print(tet.cell_data)\n",
      "```\n",
      "\n",
      "```txt\n",
      "pyvista DataSetAttributes\n",
      "Association     : CELL\n",
      "Active Scalars  : a\n",
      "Active Vectors  : None\n",
      "Active Texture  : None\n",
      "Active Normals  : None\n",
      "Contains arrays :\n",
      "    a                       float64    (729,)               SCALARS\n",
      "    b                       float64    (729,)\n",
      "pyvista DataSetAttributes\n",
      "Association     : CELL\n",
      "Active Scalars  : None\n",
      "Active Vectors  : None\n",
      "Active Texture  : None\n",
      "Active Normals  : None\n",
      "Contains arrays :\n",
      "    a                       float64    (3645,)\n",
      "    b                       float64    (3645,)\n",
      "    vtkOriginalCellIds      int32      (3645,)\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\n",
      "\n",
      "           pyvista : 0.39.0\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.2\n",
      "        matplotlib : 3.7.1\n",
      "            scooby : 0.7.1\n",
      "             pooch : v1.7.0\n",
      "           imageio : 2.27.0\n",
      "           IPython : 8.12.0\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 399 lines----------------\n",
      "Diffuse and Specular setters silently ignore invalid values\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "While working on #3870, I noticed that `diffuse` and `specular` do not always get set on `pyvista.Property`. This happens if an invalid value is used. For example, diffuse should be between 0-1, but if you pass a value of 2.0, `vtkProperty` corrects it to 1.0:\n",
      "\n",
      "```py\n",
      ">>> import vtk\n",
      ">>> prop = vtk.vtkProperty()\n",
      ">>> prop.SetDiffuse(2.0)\n",
      ">>> prop.GetDiffuse()\n",
      "1.0\n",
      "```\n",
      "\n",
      "This similarly happens for specular, which should also have a valid range of 0-1.\n",
      "\n",
      "Should we have `pyvista.Property`'s setters for these methods error out when an invalid value is passed? I ask because I definitely wasted time trying to figure out why a diffuse value of 1.0 looks the same as 2.0 before thinking it should be between 0 and 1.\n",
      "\n",
      "Perhaps this at a minimum should be documented in the setters and docstring for `add_mesh()`?\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "\n",
      "pl = pv.Plotter()\n",
      "a = pl.add_mesh(pv.Sphere(), diffuse=3.0, specular=10)\n",
      "# Expected to error for invalid values\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "main branch\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 398 lines----------------\n",
      "Unexpected threshold behavior\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "I'm using simple structed grids of cells, and need to filter-out some \"nodata\" cells. To do this, I'm setting scalar values to the cell data, then using [threshold](https://docs.pyvista.org/api/core/_autosummary/pyvista.DataSetFilters.threshold.html) with the nodata value with `invert=True`. However, I'm getting confusing and inconsistent results compared to ParaView.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pyvista\n",
      "\n",
      "x = np.arange(5, dtype=float)\n",
      "y = np.arange(6, dtype=float)\n",
      "z = np.arange(2, dtype=float)\n",
      "xx, yy, zz = np.meshgrid(x, y, z)\n",
      "mesh = pyvista.StructuredGrid(xx, yy, zz)\n",
      "mesh.cell_data.set_scalars(np.repeat(range(5), 4))\n",
      "\n",
      "# All data\n",
      "mesh.plot(show_edges=True)\n",
      "# output is normal\n",
      "\n",
      "# Filtering out nodata (zero) values\n",
      "mesh.threshold(0, invert=True).plot(show_edges=True)\n",
      "# output does not look normal, only 0-value cells are shown\n",
      "```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Thu Nov 17 15:23:57 2022 New Zealand Daylight Time\n",
      "\n",
      "                OS : Windows\n",
      "            CPU(s) : 12\n",
      "           Machine : AMD64\n",
      "      Architecture : 64bit\n",
      "               RAM : 31.7 GiB\n",
      "       Environment : IPython\n",
      "        GPU Vendor : NVIDIA Corporation\n",
      "      GPU Renderer : NVIDIA RTX A4000/PCIe/SSE2\n",
      "       GPU Version : 4.5.0 NVIDIA 472.39\n",
      "\n",
      "  Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC\n",
      "  v.1929 64 bit (AMD64)]\n",
      "\n",
      "           pyvista : 0.37.0\n",
      "               vtk : 9.1.0\n",
      "             numpy : 1.22.3\n",
      "           imageio : 2.22.0\n",
      "            scooby : 0.7.0\n",
      "             pooch : v1.6.0\n",
      "        matplotlib : 3.6.2\n",
      "             PyQt5 : 5.12.3\n",
      "           IPython : 8.6.0\n",
      "          colorcet : 3.0.1\n",
      "             scipy : 1.8.0\n",
      "              tqdm : 4.63.0\n",
      "            meshio : 5.3.4\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "Normal looking whole grid:\n",
      "![image](https://user-images.githubusercontent.com/895458/202339692-5046b23f-c3c8-4b2c-aaa7-4aa06afbae9f.png)\n",
      "\n",
      "Odd-looking threshold attempt with pyvista, showing only 0-values:\n",
      "![image](https://user-images.githubusercontent.com/895458/202339879-b2270e4c-a71b-4d43-86f8-4f67445b7b69.png)\n",
      "\n",
      "Expected result with ParaView theshold filter with upper/lower set to 0 and invert selected:\n",
      "![image](https://user-images.githubusercontent.com/895458/202340379-fea26838-b0f4-4828-b510-825f53522e87.png)\n",
      "\n",
      "Apologies for any \"user error\", as I'm new to this package.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 121 lines----------------\n",
      "`bounds` property return type is mutable from `MultiBlock`\n",
      "The `bounds` property has a different return type for meshes and `MultiBlock` objects:\n",
      "\n",
      "```\n",
      ">>> import pyvista as pv\n",
      ">>> slices = pv.Sphere().slice_orthogonal()\n",
      "# MultiBlock returns list (mutable)\n",
      ">>> slices.bounds\n",
      "[-0.49926671385765076, 0.49926671385765076, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5]\n",
      "# Mesh returns tuple (immutable)\n",
      ">>> slices[0].bounds\n",
      "(-6.162975822039155e-33, 0.0, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5)\n",
      "```\n",
      "\n",
      "IMO, the return value should be immutable and the `bounds` property should  be cast to a tuple before returning.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 149 lines----------------\n",
      "PolyData faces array is not updatable in-place and has unexpected behavior\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "When accessing `PolyData.faces` (and likely other cell data), we cannot update the array in place. Further, there is some unexpected behavior where accessing `PolyData.faces` will override existing, modified views of the array.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python \n",
      ">>> import pyvista as pv\n",
      ">>> mesh = pv.Sphere()\n",
      ">>> f = mesh.faces\n",
      ">>> f\n",
      "array([  3,   2,  30, ..., 840,  29,  28])\n",
      ">>> a = f[1:4]\n",
      ">>> a\n",
      "array([ 2, 30,  0])\n",
      ">>> b = f[5:8]\n",
      ">>> b\n",
      "array([30, 58,  0])\n",
      ">>> f[1:4] = b\n",
      ">>> f[5:8] = a\n",
      ">>> f\n",
      "array([  3,  30,  58, ..., 840,  29,  28])\n",
      ">>> assert all(f[1:4] == b) and all(f[5:8] == a)\n",
      ">>> mesh.faces  # access overwrites `f` in place which is unexpected and causes the check above to now fail\n",
      ">>> assert all(f[1:4] == b) and all(f[5:8] == a)\n",
      "---------------------------------------------------------------------------\n",
      "AssertionError                            Traceback (most recent call last)\n",
      "<ipython-input-82-08205e08097f> in <cell line: 13>()\n",
      "     11 assert all(f[1:4] == b) and all(f[5:8] == a)\n",
      "     12 mesh.faces  # access overwrites `f` in place\n",
      "---> 13 assert all(f[1:4] == b) and all(f[5:8] == a)\n",
      "\n",
      "AssertionError: \n",
      " ```\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Thu May 26 11:45:54 2022 MDT\n",
      "\n",
      "                OS : Darwin\n",
      "            CPU(s) : 16\n",
      "           Machine : x86_64\n",
      "      Architecture : 64bit\n",
      "               RAM : 64.0 GiB\n",
      "       Environment : Jupyter\n",
      "       File system : apfs\n",
      "        GPU Vendor : ATI Technologies Inc.\n",
      "      GPU Renderer : AMD Radeon Pro 5500M OpenGL Engine\n",
      "       GPU Version : 4.1 ATI-4.8.13\n",
      "\n",
      "  Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38)\n",
      "  [Clang 11.0.1 ]\n",
      "\n",
      "           pyvista : 0.35.dev0\n",
      "               vtk : 9.1.0\n",
      "             numpy : 1.22.1\n",
      "           imageio : 2.9.0\n",
      "           appdirs : 1.4.4\n",
      "            scooby : 0.5.12\n",
      "        matplotlib : 3.5.2\n",
      "           IPython : 7.32.0\n",
      "          colorcet : 3.0.0\n",
      "           cmocean : 2.0\n",
      "        ipyvtklink : 0.2.2\n",
      "             scipy : 1.8.0\n",
      "        itkwidgets : 0.32.1\n",
      "              tqdm : 4.60.0\n",
      "            meshio : 5.3.4\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Code of Conduct\n",
      "\n",
      "- [X] I agree to follow this project's Code of Conduct\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 177 lines----------------\n",
      "Cell wise and dimension reducing filters do not work for PointSet\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "`PointSet` is an odd data type as it has no cells and simply represents a 0-dimensional geometry: point clouds.\n",
      "\n",
      "This means that two common types of operations are not possible on this data type:\n",
      "\n",
      "1. Cell-wise operations like thresholding\n",
      "2. Dimension-reducing operations like contouring\n",
      "\n",
      "Cell wise operations can easily be fixed by adding cells. This can be done with `cast_to_polydata()`\n",
      "\n",
      "Dimension-reducing operations, on the other hand, have no solution and should not be allowed on `PointSet`.\n",
      "\n",
      "How can we properly error out or convert to `PolyData` when calling dataset filters like `threshold()` and `contour()`? Should these types of filters be overridden on the `PointSet` class?\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "### Cell wise operation produces *invalid* output\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "import numpy as np\n",
      "pc = pv.PointSet(np.random.random((100, 3)))\n",
      "pc['foo'] = np.arange(pc.n_points)\n",
      "pc.threshold()\n",
      "```\n",
      "\n",
      "<img width=\"575\" alt=\"Screen Shot 2023-01-11 at 5 47 02 PM 1\" src=\"https://user-images.githubusercontent.com/22067021/211949301-8d10e9ac-172e-4f27-ad81-c3ec2d335263.png\">\n",
      "\n",
      "\n",
      "### Dimension reducing operation produces *no* output\n",
      "\n",
      "```py\n",
      "import pyvista as pv\n",
      "import numpy as np\n",
      "pc = pv.PointSet(np.random.random((100, 3)))\n",
      "pc['foo'] = np.arange(pc.n_points)\n",
      "pc.contour()\n",
      "```\n",
      "<img width=\"417\" alt=\"Screen Shot 2023-01-11 at 5 47 57 PM\" src=\"https://user-images.githubusercontent.com/22067021/211949430-a3e77292-6b1e-4d2d-b2e3-b2a640ed65fc.png\">\n",
      "\n",
      "\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "n/a\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "n/a\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 597 lines----------------\n",
      "Strictly enforce keyword arguments\n",
      "I see folks quite often forget the s in the `scalars` argument for the `BasePlotter.add_mesh()` method. We should allow `scalar` as an alias much like how we allow `rng` and `clim` for the colorbar range/limits\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 28 lines----------------\n",
      "circle creates creates one zero length edge\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "I expected that:\n",
      "> circle = pv.Circle(radius, resolution=n)\n",
      "\n",
      "1. would create a circle with n points and edge. \n",
      "It does yay :-) \n",
      "\n",
      "3. That each edge of the would have similar length.\n",
      "It does _not_ :-(\n",
      "\n",
      "The problems seems circle  closed is doubly:\n",
      "- once by the coordinates (circle.points[:, 0] == approx(circle.points[:, -1])\n",
      "- and a second time by the face (circle.faces[0] == [n, 0, 1, 2, ... n-1])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "```python\n",
      "import pyvista as pv\n",
      "\n",
      "circle = pv.Circle(radius=1, resolution=4) # lets make a low res circle\n",
      "print(circle.faces)  # out: array([4, 0, 1, 2, 3])\n",
      "print(circle.n_points)  # out: 4\n",
      "\n",
      "# the print outputs gives the expectation that circle.plot() will look like a square \n",
      "circle.plot()\n",
      "```\n",
      "![pv.Circle(radius=1, resolution=4).plot()](https://user-images.githubusercontent.com/107837123/207049939-9b24ac31-a8e8-4ca7-97d3-3f4105a524dc.png)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Mon Dec 12 13:55:16 2022 CET\n",
      "\n",
      "                OS : Linux\n",
      "            CPU(s) : 8\n",
      "           Machine : x86_64\n",
      "      Architecture : 64bit\n",
      "       Environment : IPython\n",
      "       GPU Details : error\n",
      "\n",
      "  Python 3.10.4 (main, Mar 23 2022, 20:25:24) [GCC 11.3.0]\n",
      "\n",
      "           pyvista : 0.36.1\n",
      "               vtk : 9.1.0\n",
      "             numpy : 1.23.5\n",
      "           imageio : 2.22.4\n",
      "           appdirs : 1.4.4\n",
      "            scooby : 0.7.0\n",
      "        matplotlib : 3.6.2\n",
      "         pyvistaqt : 0.9.0\n",
      "             PyQt5 : Version unknown\n",
      "           IPython : 8.2.0\n",
      "             scipy : 1.9.3\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 1119 lines----------------\n",
      "vtkVolume needs wrapping like vtkActor\n",
      "We wrap vtkActor nicely and should do the same for vtkVolume to make lookup table modification during volume rendering nicer.\n",
      "\n",
      "```py\n",
      "import pyvista as pv\n",
      "from pyvista import examples\n",
      "\n",
      "vol = examples.download_knee_full()\n",
      "\n",
      "p = pv.Plotter(notebook=0)\n",
      "actor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\n",
      "actor.mapper.lookup_table.cmap = 'viridis'\n",
      "p.show()\n",
      "```\n",
      "\n",
      "```\n",
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "Input In [3], in <cell line: 8>()\n",
      "      6 p = pv.Plotter(notebook=0)\n",
      "      7 actor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\n",
      "----> 8 actor.mapper.lookup_table.cmap = 'viridis'\n",
      "      9 p.show()\n",
      "\n",
      "AttributeError: 'vtkmodules.vtkRenderingCore.vtkVolume' object has no attribute 'mapper'\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pyvista/pyvista----------------\n",
      "-------------Fix: 42 lines----------------\n",
      "Rectilinear grid does not allow Sequences as inputs\n",
      "### Describe the bug, what's wrong, and what you expected.\n",
      "\n",
      "Rectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\n",
      "\n",
      "### Steps to reproduce the bug.\n",
      "\n",
      "This doesn't work\n",
      "```python\n",
      "import pyvista as pv\n",
      "pv.RectilinearGrid([0, 1], [0, 1], [0, 1])\n",
      "```\n",
      "\n",
      "This works\n",
      "```py\n",
      "import pyvista as pv\n",
      "import numpy as np\n",
      "pv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\n",
      "```\n",
      "### System Information\n",
      "\n",
      "```shell\n",
      "--------------------------------------------------------------------------------\n",
      "  Date: Wed Apr 19 20:15:10 2023 UTC\n",
      "\n",
      "                OS : Linux\n",
      "            CPU(s) : 2\n",
      "           Machine : x86_64\n",
      "      Architecture : 64bit\n",
      "       Environment : IPython\n",
      "        GPU Vendor : Mesa/X.org\n",
      "      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\n",
      "       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\n",
      "\n",
      "  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\n",
      "\n",
      "           pyvista : 0.38.5\n",
      "               vtk : 9.2.6\n",
      "             numpy : 1.24.2\n",
      "           imageio : 2.27.0\n",
      "            scooby : 0.7.1\n",
      "             pooch : v1.7.0\n",
      "        matplotlib : 3.7.1\n",
      "           IPython : 8.12.0\n",
      "--------------------------------------------------------------------------------\n",
      "```\n",
      "\n",
      "\n",
      "### Screenshots\n",
      "\n",
      "_No response_\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 83 lines----------------\n",
      "Memory leaks when accessing sequence tags with Dataset.__getattr__.\n",
      "**Describe the bug**\n",
      "Accessing sequences via `Dataset.__getattr__` seems to leak memory. The bug occurred for me when I was processing many DICOMs and manipulating some tags contained in sequences and each leaked a bit of memory, ultimately crashing the process.\n",
      "\n",
      "**Expected behavior**\n",
      "Memory should not leak. It works correctly when you replace the `__getattr__` call with `__getitem__` (by manually constructing the necessary tag beforehand).\n",
      "\n",
      "Without being an expert in the codebase, one difference I think that could explain it is that `__getattr__` sets `value.parent = self` for sequences while `__getitem__` doesn't seem to do that. Maybe this loop of references somehow confuses Python's garbage collection?\n",
      "\n",
      "**Steps To Reproduce**\n",
      "This increases the memory consumption of the Python process by about 700 MB on my machine. The DICOM file I've tested it with is 27MB and has one item in `SourceImageSequence`. Note that the memory leak plateaus after a while in this example, maybe because it's the same file. In my actual workflow when iterating over many different files, the process filled all memory and crashed.\n",
      "\n",
      "```python\n",
      "import pydicom\n",
      "for i in range(100):\n",
      "  dcm = pydicom.dcmread(\"my_dicom.dcm\")\n",
      "  test = dcm.SourceImageSequence\n",
      "```\n",
      "\n",
      "For comparison, this keeps the memory constant. `(0x0008, 0x2112)` is `SourceImageSequence`: \n",
      "\n",
      "```python\n",
      "import pydicom\n",
      "import pydicom.tag\n",
      "for i in range(100):\n",
      "  dcm = pydicom.dcmread(\"my_dicom.dcm\")\n",
      "  test = dcm[pydicom.tag.TupleTag((0x0008, 0x2112))]\n",
      "```\n",
      "\n",
      "**Your environment**\n",
      "\n",
      "```bash\n",
      "Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\n",
      "Python  3.6.8 (default, Jan 14 2019, 11:02:34)\n",
      "pydicom  1.3.0\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 243 lines----------------\n",
      "Add support for Extended Offset Table to encaps module\n",
      "[CP1818](http://webcache.googleusercontent.com/search?q=cache:xeWXtrAs9G4J:ftp://medical.nema.org/medical/dicom/final/cp1818_ft_whenoffsettabletoosmall.pdf) added the use of an Extended Offset Table for encapsulated pixel data when the Basic Offset Table isn't suitable.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 31 lines----------------\n",
      "Handle odd-sized dicoms with warning\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\n",
      "\n",
      "We have some uncompressed dicoms with an odd number of pixel bytes (saved by older versions of pydicom actually). \n",
      "\n",
      "When we re-open with pydicom 1.2.2, we're now unable to extract the image, due to the change made by https://github.com/pydicom/pydicom/pull/601\n",
      "\n",
      "Would it be possible to emit a warning instead of rejecting the dicom for such cases?\n",
      "\n",
      "#### Version\n",
      "1.2.2\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 20 lines----------------\n",
      "LUT Descriptor tag with no value yields TypeError\n",
      "**Describe the bug**\n",
      "I have a DICOM image with the following tag (copied from ImageJ)\n",
      "\n",
      "```\n",
      "0028,1101  Red Palette Color Lookup Table Descriptor: \n",
      "```\n",
      "\n",
      "which corresponds to the raw data element, produced by [`DataElement_from_raw`](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L699):\n",
      "```\n",
      "RawDataElement(tag=(0028, 1101), VR='US', length=0, value=None, value_tell=1850, is_implicit_VR=False, is_little_endian=True)\n",
      "```\n",
      "\n",
      "Because this tag is matched by the [LUT Descriptor tags](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L696) and the value is empty (`None`), the [following line](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L761):\n",
      "```\n",
      "if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\n",
      "```\n",
      "results in \n",
      "```\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "\n",
      "Given that I discovered this by parsing what seems to be a set of faulty DICOMs (mangled pixel data), I'm not sure if an error should be raised if the colour attribute value is not provided.\n",
      "\n",
      "However, given that `value` can be `None` for other tags, the simple fix is\n",
      "\n",
      "```python\n",
      "try:\n",
      "    if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\n",
      "        # We only fix the first value as the third value is 8 or 16\n",
      "        value[0] += 65536\n",
      "except TypeError:\n",
      "    pass\n",
      "```\n",
      "\n",
      "(or test if `value` is iterable).\n",
      "\n",
      "**Your environment**\n",
      "```\n",
      "Darwin-19.3.0-x86_64-i386-64bit\n",
      "Python  3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:05:27)\n",
      "[Clang 9.0.1 ]\n",
      "pydicom  1.4.1\n",
      "```\n",
      "\n",
      "Many thanks!\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 25 lines----------------\n",
      "Embedded Null character\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\n",
      "    624         try:\n",
      "--> 625             py_encodings.append(python_encoding[encoding])\n",
      "    626         except KeyError:\n",
      "\n",
      "KeyError: 'ISO_IR 100\\x00'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-12-605c3c3edcf4> in <module>\n",
      "      4 print(filename)\n",
      "      5 dcm = pydicom.dcmread(filename,force=True)\n",
      "----> 6 dcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm',force=True)\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in dcmread(fp, defer_size, stop_before_pixels, force, specific_tags)\n",
      "    848     try:\n",
      "    849         dataset = read_partial(fp, stop_when, defer_size=defer_size,\n",
      "--> 850                                force=force, specific_tags=specific_tags)\n",
      "    851     finally:\n",
      "    852         if not caller_owns_file:\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_partial(fileobj, stop_when, defer_size, force, specific_tags)\n",
      "    726         dataset = read_dataset(fileobj, is_implicit_VR, is_little_endian,\n",
      "    727                                stop_when=stop_when, defer_size=defer_size,\n",
      "--> 728                                specific_tags=specific_tags)\n",
      "    729     except EOFError:\n",
      "    730         pass  # error already logged in read_dataset\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_dataset(fp, is_implicit_VR, is_little_endian, bytelength, stop_when, defer_size, parent_encoding, specific_tags)\n",
      "    361     try:\n",
      "    362         while (bytelength is None) or (fp.tell() - fp_start < bytelength):\n",
      "--> 363             raw_data_element = next(de_gen)\n",
      "    364             # Read data elements. Stop on some errors, but return what was read\n",
      "    365             tag = raw_data_element.tag\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in data_element_generator(fp, is_implicit_VR, is_little_endian, stop_when, defer_size, encoding, specific_tags)\n",
      "    203                 # Store the encoding value in the generator\n",
      "    204                 # for use with future elements (SQs)\n",
      "--> 205                 encoding = convert_encodings(encoding)\n",
      "    206 \n",
      "    207             yield RawDataElement(tag, VR, length, value, value_tell,\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\n",
      "    626         except KeyError:\n",
      "    627             py_encodings.append(\n",
      "--> 628                 _python_encoding_for_corrected_encoding(encoding))\n",
      "    629 \n",
      "    630     if len(encodings) > 1:\n",
      "\n",
      "~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in _python_encoding_for_corrected_encoding(encoding)\n",
      "    664     # fallback: assume that it is already a python encoding\n",
      "    665     try:\n",
      "--> 666         codecs.lookup(encoding)\n",
      "    667         return encoding\n",
      "    668     except LookupError:\n",
      "\n",
      "ValueError: embedded null character\n",
      "#### Steps/Code to Reproduce\n",
      "<!--\n",
      "Example:\n",
      "```py\n",
      "from io import BytesIO\n",
      "from pydicom import dcmread\n",
      "\n",
      "bytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\n",
      "             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\n",
      "             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\n",
      "             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\n",
      "             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\n",
      "\n",
      "fp = BytesIO(bytestream)\n",
      "ds = dcmread(fp, force=True)\n",
      "\n",
      "print(ds.PatientID)\n",
      "```\n",
      "If the code is too long, feel free to put it in a public gist and link\n",
      "it in the issue: https://gist.github.com\n",
      "\n",
      "When possible use pydicom testing examples to reproduce the errors. Otherwise, provide\n",
      "an anonymous version of the data in order to replicate the errors.\n",
      "-->\n",
      "import pydicom\n",
      "dcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm')\n",
      "\n",
      "#### Expected Results\n",
      "<!-- Please paste or describe the expected results.\n",
      "Example: No error is thrown and the name of the patient is printed.-->\n",
      "No error\n",
      "I used dcmread function in matlab to read the same file and it was ok. So it should not be the problem of the file.\n",
      "#### Actual Results\n",
      "<!-- Please paste or specifically describe the actual output or traceback.\n",
      "(Use %xmode to deactivate ipython's trace beautifier)\n",
      "Example: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\n",
      "-->\n",
      "Error: Embedded Null character\n",
      "#### Versions\n",
      "<!--\n",
      "Please run the following snippet and paste the output below.\n",
      "import platform; print(platform.platform())\n",
      "import sys; print(\"Python\", sys.version)\n",
      "import pydicom; print(\"pydicom\", pydicom.__version__)\n",
      "-->\n",
      "1.3.0\n",
      "\n",
      "<!-- Thanks for contributing! -->\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 77 lines----------------\n",
      "Mypy errors\n",
      "**Describe the bug**\n",
      "Several of the type hints are problematic and result in mypy errors.\n",
      "\n",
      "One example:\n",
      "\n",
      "```none\n",
      "cat << EOF > /tmp/test.py\n",
      "from pydicom import Dataset, dcmread\n",
      "\n",
      "dataset = Dataset()\n",
      "dataset.Rows = 10\n",
      "dataset.Columns = 20\n",
      "dataset.NumberOfFrames = \"5\"\n",
      "\n",
      "assert int(dataset.NumberOfFrames) == 5\n",
      "\n",
      "filename = '/tmp/test.dcm'\n",
      "dataset.save_as(str(filename))\n",
      "\n",
      "dataset = dcmread(filename)\n",
      "\n",
      "assert int(dataset.NumberOfFrames) == 5\n",
      "EOF\n",
      "```\n",
      "\n",
      "```none\n",
      "mypy /tmp/test.py\n",
      "/tmp/test.py:15: error: No overload variant of \"int\" matches argument type \"object\"\n",
      "/tmp/test.py:15: note: Possible overload variant:\n",
      "/tmp/test.py:15: note:     def int(self, x: Union[str, bytes, SupportsInt, _SupportsIndex] = ...) -> int\n",
      "/tmp/test.py:15: note:     <1 more non-matching overload not shown>\n",
      "Found 1 error in 1 file (checked 1 source file)\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "Mypy should not report any errors.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "See above\n",
      "\n",
      "**Your environment**\n",
      "```none\n",
      "python -m pydicom.env_info\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | macOS-10.15.6-x86_64-i386-64bit\n",
      "Python       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\n",
      "pydicom      | 2.1.0\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | 1.19.3\n",
      "PIL          | 8.0.1\n",
      "```\n",
      "ImportError: cannot import name 'NoReturn'\n",
      "**Describe the bug**\n",
      "throw following excetion when import pydicom package:\n",
      "```\n",
      "xxx/python3.6/site-packages/pydicom/filebase.py in <module>\n",
      "5 from struct import unpack, pack\n",
      "      6 from types import TracebackType\n",
      "----> 7 from typing import (\n",
      "      8     Tuple, Optional, NoReturn, BinaryIO, Callable, Type, Union, cast, TextIO,\n",
      "      9     TYPE_CHECKING, Any\n",
      "\n",
      "ImportError: cannot import name 'NoReturn'\n",
      "```\n",
      "\n",
      "**Expected behavior**\n",
      "imort pydicom sucessfully\n",
      "\n",
      "**Steps To Reproduce**\n",
      "How to reproduce the issue. Please include a minimum working code sample, the\n",
      "traceback (if any) and the anonymized DICOM dataset (if relevant).\n",
      "\n",
      "**Your environment**\n",
      "python:3.6.0\n",
      "pydicom:2.1\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 159 lines----------------\n",
      "Add support for missing VRs\n",
      "Missing: OV, SV, UV\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 191 lines----------------\n",
      "dcmread cannot handle pathlib.Path objects\n",
      "**Describe the bug**\n",
      "The `dcmread()` currently fails when passed an instance of `pathlib.Path`. The problem is the following line:\n",
      "https://github.com/pydicom/pydicom/blob/8b0bbaf92d7a8218ceb94dedbee3a0463c5123e3/pydicom/filereader.py#L832\n",
      "\n",
      "**Expected behavior**\n",
      "`dcmread()` should open and read the file to which the `pathlib.Path` object points.\n",
      "\n",
      "The line above should probably be:\n",
      "```python\n",
      "if isinstance(fp, (str, Path)):\n",
      "````\n",
      "\n",
      "**Steps To Reproduce**\n",
      "```python\n",
      "from pathlib import Path\n",
      "from pydicom.filereader import dcmread\n",
      "\n",
      "dcm_filepath = Path('path/to/file')\n",
      "dcmread(dcm_filepath)\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 18 lines----------------\n",
      "Unable to assign single element list to PN field\n",
      "I am getting `AttributeError` while trying to assign a list of single element to a `PN` field.\n",
      "It's converting `val` to a 2D array [here](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L328) when `VM` is 1. \n",
      "\n",
      "**Code**\n",
      "```\n",
      ">>> from pydicom import dcmread, dcmwrite\n",
      ">>> ds = dcmread(\"SOP1.dcm\")\n",
      ">>> a = [\"name1\"]\n",
      ">>> b = [\"name1\", \"name2\"]\n",
      ">>> ds.PatientName = a\n",
      ">>> dcmwrite(\"out.dcm\", ds)     # throws the error below\n",
      ">>> ds.PatientName = b\n",
      ">>> dcmwrite(\"out.dcm\", ds)     # works fine\n",
      "```\n",
      "\n",
      "**Error**\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\n",
      "    yield\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\n",
      "    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\n",
      "    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\n",
      "    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\n",
      "    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\n",
      "AttributeError: 'MultiValue' object has no attribute 'encode'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 1153, in dcmwrite\n",
      "    _write_dataset(fp, dataset, write_like_original)\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 889, in _write_dataset\n",
      "    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\n",
      "    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 32, in tag_in_exception\n",
      "    raise type(exc)(msg) from exc\n",
      "AttributeError: With tag (0010, 0010) got exception: 'MultiValue' object has no attribute 'encode'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\n",
      "    yield\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\n",
      "    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\n",
      "    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\n",
      "    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\n",
      "  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\n",
      "    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\n",
      "AttributeError: 'MultiValue' object has no attribute 'encode'\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 16 lines----------------\n",
      "Ambiguous VR element could be read in <=1.1.0 but is broken in >=1.2.0\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "Attribute Error thrown when printing (0x0028, 0x0120) PixelPaddingValue\n",
      "\n",
      "#### Steps/Code to Reproduce\n",
      "Using pydicom 1.2.2 and above (including master branch as of issue creation date):\n",
      "```\n",
      "from pydicom import dcmread\n",
      "\n",
      "ds = dcmread('rtss.dcm')\n",
      "ds\n",
      "\n",
      "Exception in thread Thread-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 157, in correct_ambiguous_vr_element\n",
      "    _correct_ambiguous_vr_element(elem, ds, is_little_endian)\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 75, in _correct_ambiguous_vr_element\n",
      "    if ds.PixelRepresentation == 0:\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 711, in __getattr__\n",
      "    return super(Dataset, self).__getattribute__(name)\n",
      "AttributeError: 'FileDataset' object has no attribute 'PixelRepresentation'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/apanchal/Projects/test.py\", line 107, in processing_thread\n",
      "    dp.ds, name, patientid, patientdob)\n",
      "  File \"/Users/apanchal/Projects/test.py\", line 144, in UpdateElements\n",
      "    for item in data:\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 1045, in __iter__\n",
      "    yield self[tag]\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 805, in __getitem__\n",
      "    self[tag], self, data_elem[6])\n",
      "  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 161, in correct_ambiguous_vr_element\n",
      "    raise AttributeError(reason)\n",
      "AttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'\n",
      "```\n",
      "\n",
      "Anonymized RTSTRUCT file is attached: [RTSTRUCT.zip](https://github.com/pydicom/pydicom/files/3124625/RTSTRUCT.zip)\n",
      "\n",
      "#### Expected Results\n",
      "The dataset is printed. This worked in pydicom 1.1.0 and below.\n",
      "\n",
      "Since `PixelRepresentation` is not defined in the dataset, this attribute cannot be printed anymore.\n",
      "\n",
      "What's strange is that according to the standard PixelPaddingValue (0028, 0120) is 1C for RTSTRUCT, but in this file it has no other tags referencing PixelData. So it probably should not have been included by the vendor.\n",
      "\n",
      "I am wondering if there should be another path like in #809 that can handle the missing PixelRepresentation attribute.\n",
      "\n",
      "#### Actual Results\n",
      "```AttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'```\n",
      "\n",
      "#### Versions\n",
      "```\n",
      "Darwin-17.7.0-x86_64-i386-64bit\n",
      "Python 3.7.0 (default, Jul 23 2018, 20:22:55) \n",
      "[Clang 9.1.0 (clang-902.0.39.2)]\n",
      "pydicom 1.2.2\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 113 lines----------------\n",
      "Dataset.pixel_array doesn't change unless PixelData does\n",
      "#### Description\n",
      "Currently `ds.pixel_array` produces a numpy array that depends on element values for Rows, Columns, Samples Per Pixel, etc, however the code for `ds.pixel_array` only changes the returned array if the value for `ds.PixelData` changes. This may lead to confusion/undesirable behaviour if the values for related elements are changed after `ds.pixel_array` is called but not the underlying pixel data.\n",
      "\n",
      "I can't think of any real use cases except maybe in an interactive session when debugging a non-conformant dataset, but I suggest we change the way `Dataset._pixel_id` is calculated so that it takes into account changes in related elements as well.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 13 lines----------------\n",
      "Segmented LUTs are incorrectly expanded\n",
      "**Describe the bug**\n",
      "`pydicom.pixel_data_handlers.util._expand_segmented_lut()` expands segmented LUTs to an incorrect length.\n",
      "\n",
      "**Expected behavior**\n",
      "A correct length LUT to be produced.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "Initialize the following variables.\n",
      "```\n",
      "import numpy as np\n",
      "length = 48\n",
      "y0 = 163\n",
      "y1 = 255\n",
      "```\n",
      "\n",
      "Run the following two lines from [`pydicom.pixel_data_handlers.util._expand_segmented_lut()`](https://github.com/pydicom/pydicom/blob/699c9f0a8e190d463dd828822106250523d38154/pydicom/pixel_data_handlers/util.py#L875\n",
      ")\n",
      "```\n",
      "step = (y1 - y0) / length\n",
      "vals = np.around(np.arange(y0 + step, y1 + step, step))\n",
      "```\n",
      "\n",
      "Confirm that variable `vals` if of incorrect length\n",
      "```\n",
      "print(len(vals) == length)\n",
      "> False\n",
      "```\n",
      "\n",
      "Alternatively, the code below produces similarly false results\n",
      "\n",
      "```\n",
      "from pydicom.pixel_data_handlers.util import _expand_segmented_lut \n",
      "lut = _expand_segmented_lut(([0, 1, 163, 1, 48, 255]), \"B\")\n",
      "print(len(lut) == (1+48))\n",
      "> False\n",
      "```\n",
      "\n",
      "`np.arange` [explicitly states](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) that it's \"results will often not be consistent\" when using \"non-integer step\", which is a very possible scenario in this function. The following alternative code does function correctly:\n",
      "\n",
      "```\n",
      "vals = np.around(np.linspace(y0 + step, y1, length))\n",
      "```\n",
      "\n",
      "**Your environment**\n",
      "```bash\n",
      "$ python -m pydicom.env_info\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | Darwin-20.5.0-x86_64-i386-64bit\n",
      "Python       | 3.7.10 (default, Feb 26 2021, 10:16:00)  [Clang 10.0.0 ]\n",
      "pydicom      | 2.1.2\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | 1.20.3\n",
      "PIL          | 8.2.0\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 52 lines----------------\n",
      "Heuristic for Explicit VR acting in sequence datasets\n",
      "**Describe the bug**\n",
      "There is a check to confirm implicit VR by looking for two ascii characters and switching to explicit with a warning (#823).  It was thought this was safe because length in first data elements would not be that large.  However, in sequence item datasets this may not be true.\n",
      "\n",
      "Noted in google group conversation at https://groups.google.com/forum/#!topic/pydicom/VUmvUYmQxc0 (note that the title of that thread is not correct. that was not the problem).\n",
      "\n",
      "Test demonstrating it and fix already done - PR to follow shortly.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 14 lines----------------\n",
      "apply_color_lut() incorrect exception when missing RedPaletteColorLUTDescriptor\n",
      "**Describe the bug**\n",
      "`AttributeError` when used on a dataset without `RedPaletteColorLookupTableDescriptor `\n",
      "\n",
      "**Expected behavior**\n",
      "Should raise `ValueError` for consistency with later exceptions\n",
      "\n",
      "**Steps To Reproduce**\n",
      "```python\n",
      "from pydicom.pixel_data_handlers.util import apply_color_lut\n",
      "ds = dcmread(\"CT_small.dcm\")\n",
      "arr = ds.apply_color_lut(arr, ds)\n",
      "```\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \".../pydicom/pixel_data_handlers/util.py\", line 116, in apply_color_lut\n",
      "    lut_desc = ds.RedPaletteColorLookupTableDescriptor\n",
      "  File \".../pydicom/dataset.py\", line 768, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'FileDataset' object has no attribute 'RedPaletteColorLookupTableDescriptor'\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 47 lines----------------\n",
      "Exception decompressing RLE encoded data with non-conformant padding\n",
      "Getting Following error \n",
      "\"Could not convert:  The amount of decoded RLE segment data doesn't match the expected amount (786433 vs. 786432 bytes)\"\n",
      "For following code\n",
      "plt.imsave(os.path.join(output_folder,file)+'.png', convert_color_space(ds.pixel_array, ds[0x28,0x04].value, 'RGB'))\n",
      "Also attaching DICOM file \n",
      "[US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt](https://github.com/pydicom/pydicom/files/6799721/US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt)\n",
      "\n",
      "Please remove .txt extension to use DICOM file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 22 lines----------------\n",
      "can open my dicom, error in re.match('^ISO[^_]IR', encoding)\n",
      "```\n",
      "(test) root@DESKTOP-COPUCVT:/mnt/e/test# python3 mydicom.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 625, in convert_encodings\n",
      "    py_encodings.append(python_encoding[encoding])\n",
      "KeyError: 73\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"mydicom.py\", line 12, in <module>\n",
      "    pydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\")\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 850, in dcmread\n",
      "    force=force, specific_tags=specific_tags)\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 728, in read_partial\n",
      "    specific_tags=specific_tags)\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 382, in read_dataset\n",
      "    encoding = convert_encodings(char_set)\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 628, in convert_encodings\n",
      "    _python_encoding_for_corrected_encoding(encoding))\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 647, in _python_encoding_for_corrected_encoding\n",
      "    if re.match('^ISO[^_]IR', encoding) is not None:\n",
      "  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/re.py\", line 172, in match\n",
      "    return _compile(pattern, flags).match(string)\n",
      "TypeError: expected string or bytes-like object\n",
      "```\n",
      "\n",
      "#### Description\n",
      " I dont know why pydicom cant open my pictures, but other python library can read the picture and read some meta data correctly. I suspect \" if re.match('^ISO[^_]IR', encoding) is not None:\"  the encoding here is not string for my dicom picture.   I am new to pydicom, \n",
      "Has anyone encountered a similar problem? how to solve it?  need help,thanks!\n",
      "\n",
      "here is some dicom tags:\n",
      "![image](https://user-images.githubusercontent.com/32253100/61868213-8016f500-af0b-11e9-8736-8703230229cf.png)\n",
      "![image](https://user-images.githubusercontent.com/32253100/61868247-91600180-af0b-11e9-8767-a4045e901b8f.png)\n",
      "![image](https://user-images.githubusercontent.com/32253100/61868284-a50b6800-af0b-11e9-88fd-10180e0acf56.png)\n",
      "\n",
      "\n",
      "\n",
      "#### Steps/Code to Reproduce\n",
      "```py\n",
      "\n",
      "import pydicom\n",
      "import os\n",
      "import numpy\n",
      "child_path = \"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"\n",
      "pydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"ï¼‰\n",
      "\n",
      "```\n",
      "\n",
      "#### Expected Results\n",
      "Example: read the file without error\n",
      "\n",
      "#### Actual Results\n",
      "cant read the file\n",
      "\n",
      "#### Versions\n",
      "v1.3.0\n",
      "\n",
      "python v3.6\n",
      "\n",
      "<!-- Thanks for contributing! -->\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 82 lines----------------\n",
      "Inconsistencies in value testing for PersonName3\n",
      "```python\n",
      "from pydicom.dataset import Dataset\n",
      "\n",
      "ds = Dataset()\n",
      "ds.PatientName = None  # or ''\n",
      "if ds.PatientName:\n",
      "    print('Has a value')\n",
      "else:\n",
      "    print('Has no value')\n",
      "\n",
      "if None:  # or ''\n",
      "    print('Evaluates as True')\n",
      "else:\n",
      "    print('Evaluates as False')\n",
      "```\n",
      "Prints `Has a value` then `Evaluates as False`. Should print `Has no value` instead (encoded dataset will have a zero-length element).\n",
      "\n",
      "Current master, python 3.6.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 89 lines----------------\n",
      "Handling of DS too long to be encoded in explicit encoding\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "This is probably not a bug, but I'm not sure about the wanted behavior.\n",
      "An RTPlan dataset encoded as Little Endian Implicit contains multiple values in the DS tag DHV Data (3004,0058) with an overall length not fitting into 2 bytes. Trying to write this as explicit Little Endian fails with an exception (`\"ushort format requires 0 &lt;= number &lt;= (0x7fff * 2 + 1)\"`) which is raised by the `pack` call in `write_leUS` while trying to write the length.\n",
      "\n",
      "The standard says for this case in PS3.5, Table 6.2-1 (for VR DS):\n",
      "```\n",
      "Note\n",
      "Data Elements with multiple values using this VR may not be properly encoded if Explicit-VR transfer syntax is used and the VL of this attribute exceeds 65534 bytes.\n",
      "```\n",
      "So, as I understand it, this is valid DICOM, that cannot be converted to explicit encoding without data loss.\n",
      "The question is how to handle this. What comes to mind:\n",
      "- truncate the value and log a warning\n",
      "- raise a meaningful exception\n",
      "- adapt the behavior depending on some config setting\n",
      "\n",
      "Any thoughts?\n",
      "\n",
      "<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\n",
      "\n",
      "#### Steps/Code to Reproduce\n",
      "<!--\n",
      "Example:\n",
      "```py\n",
      "from io import BytesIO\n",
      "from pydicom import dcmread\n",
      "\n",
      "bytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\n",
      "             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\n",
      "             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\n",
      "             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\n",
      "             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\n",
      "\n",
      "fp = BytesIO(bytestream)\n",
      "ds = dcmread(fp, force=True)\n",
      "\n",
      "print(ds.PatientID)\n",
      "```\n",
      "If the code is too long, feel free to put it in a public gist and link\n",
      "it in the issue: https://gist.github.com\n",
      "\n",
      "When possible use pydicom testing examples to reproduce the errors. Otherwise, provide\n",
      "an anonymous version of the data in order to replicate the errors.\n",
      "-->\n",
      "\n",
      "#### Expected Results\n",
      "<!-- Please paste or describe the expected results.\n",
      "Example: No error is thrown and the name of the patient is printed.-->\n",
      "\n",
      "#### Actual Results\n",
      "<!-- Please paste or specifically describe the actual output or traceback.\n",
      "(Use %xmode to deactivate ipython's trace beautifier)\n",
      "Example: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\n",
      "-->\n",
      "\n",
      "#### Versions\n",
      "<!--\n",
      "Please run the following snippet and paste the output below.\n",
      "import platform; print(platform.platform())\n",
      "import sys; print(\"Python\", sys.version)\n",
      "import pydicom; print(\"pydicom\", pydicom.__version__)\n",
      "-->\n",
      "\n",
      "\n",
      "<!-- Thanks for contributing! -->\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 117 lines----------------\n",
      "to_json does not work with binary data in pixel_array\n",
      "**Describe the issue**\n",
      "Loading a dicom file and then performing a to_json() on it does not work with binary data in pixel_array.\n",
      "\n",
      "\n",
      "\n",
      "**Expected behavior**\n",
      "I would have expected that a base64 conversion is first performed on the binary data and then encoded to json. \n",
      "\n",
      "**Steps To Reproduce**\n",
      "How to reproduce the issue. Please include:\n",
      "1. A minimum working code sample\n",
      "\n",
      "import pydicom\n",
      "ds = pydicom.dcmread('path_to_file')\n",
      "output = ds.to_json()\n",
      "\n",
      "\n",
      "2. The traceback (if one occurred)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 2003, in to_json\n",
      "    dump_handler=dump_handler\n",
      "  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 1889, in _data_element_to_json\n",
      "    binary_value = data_element.value.encode('utf-8')\n",
      "AttributeError: 'bytes' object has no attribute 'encode'\n",
      "\n",
      "\n",
      "3. Which of the following packages are available and their versions:\n",
      "  * Numpy\n",
      "numpy==1.17.2\n",
      "  * Pillow\n",
      "Pillow==6.1.0\n",
      "  * JPEG-LS\n",
      "  * GDCM\n",
      "4. The anonymized DICOM dataset (if possible).\n",
      "\n",
      "**Your environment**\n",
      "Please run the following and paste the output.\n",
      "```bash\n",
      "$ python -c \"import platform; print(platform.platform())\"\n",
      "Darwin-19.2.0-x86_64-i386-64bit\n",
      "$ python -c \"import sys; print('Python ', sys.version)\"\n",
      "Python  3.7.6 (default, Dec 30 2019, 19:38:26) \n",
      "[Clang 11.0.0 (clang-1100.0.33.16)]\n",
      "$ python -c \"import pydicom; print('pydicom ', pydicom.__version__)\"\n",
      "pydicom  1.3.0\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 25 lines----------------\n",
      "Error writing values with VR OF\n",
      "[Related to this comment](https://github.com/pydicom/pydicom/issues/452#issuecomment-614038937) (I think)\n",
      "\n",
      "```python\n",
      "from pydicom.dataset import Dataset\n",
      "ds = Dataset()\n",
      "ds.is_little_endian = True\n",
      "ds.is_implicit_VR = True\n",
      "ds.FloatPixelData = b'\\x00\\x00\\x00\\x00'\n",
      "ds.save_as('out.dcm')\n",
      "```\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \".../pydicom/filewriter.py\", line 228, in write_numbers\n",
      "    value.append  # works only if list, not if string or number\n",
      "AttributeError: 'bytes' object has no attribute 'append'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \".../pydicom/filewriter.py\", line 230, in write_numbers\n",
      "    fp.write(pack(format_string, value))\n",
      "struct.error: required argument is not a float\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \".../pydicom/tag.py\", line 27, in tag_in_exception\n",
      "    yield\n",
      "  File \".../pydicom/filewriter.py\", line 543, in write_dataset\n",
      "    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\n",
      "  File \".../pydicom/filewriter.py\", line 472, in write_data_element\n",
      "    writer_function(buffer, data_element, writer_param)\n",
      "  File \".../pydicom/filewriter.py\", line 236, in write_numbers\n",
      "    \"{0}\\nfor data_element:\\n{1}\".format(str(e), str(data_element)))\n",
      "OSError: required argument is not a float\n",
      "for data_element:\n",
      "(7fe0, 0008) Float Pixel Data                    OF: b'\\x00\\x00\\x00\\x00'\n",
      "\n",
      "[skip]\n",
      "```\n",
      "[Error in filewriter](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L1007) using `write_numbers` instead of `write_OBvalue`/`write_OWvalue`. Looks like it's been wrong [since 2008](https://github.com/pydicom/pydicom/commit/5d3ea61ffe6877ae79267bf233f258c07c726998). I'm a bit surprised it hasn't come up before.\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 45 lines----------------\n",
      "Pixel Representation attribute should be optional for pixel data handler\n",
      "**Describe the bug**\n",
      "The NumPy pixel data handler currently [requires the Pixel Representation attribute](https://github.com/pydicom/pydicom/blob/8da0b9b215ebfad5756051c891def88e426787e7/pydicom/pixel_data_handlers/numpy_handler.py#L46). This is problematic, because in case of Float Pixel Data or Double Float Pixel Data the attribute shall be absent. Compare [Floating Point Image Pixel Module Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.24.html) versus [Image Pixel Description Macro Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html#table_C.7-11c)\n",
      "\n",
      "**Expected behavior**\n",
      "I would expect the `Dataset.pixel_array` property to be able to decode a Float Pixel Data or Double Float Pixel Data element without presence of the Pixel Representation element in the metadata.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "```python\n",
      "import numpy as np\n",
      "from pydicom.dataset import Dataset, FileMetaDataset\n",
      "\n",
      "\n",
      "ds = Dataset()\n",
      "ds.file_meta = FileMetaDataset()\n",
      "ds.file_meta.TransferSyntaxUID = '1.2.840.10008.1.2.1'\n",
      "\n",
      "ds.BitsAllocated = 32\n",
      "ds.SamplesPerPixel = 1\n",
      "ds.Rows = 5\n",
      "ds.Columns = 5\n",
      "ds.PhotometricInterpretation = 'MONOCHROME2'\n",
      "\n",
      "pixel_array = np.zeros((ds.Rows, ds.Columns), dtype=np.float32)\n",
      "ds.FloatPixelData = pixel_array.flatten().tobytes()\n",
      "\n",
      "np.array_equal(ds.pixel_array, pixel_array)\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 153 lines----------------\n",
      "Strict adherence to VR during parsing is detrimental due to commonplace vendor interpretations\n",
      "**Describe the bug**\n",
      "DICOM Files from GE modalities, which when parsed, raise a TypeError caused by \"violating\" the VR imposed by the DICOM standard; however, real world modalities have and continue to generate such files for good cause.\n",
      "\n",
      "For example the following is raised\n",
      "\n",
      "`TypeError('Could not convert value to integer without loss')`\n",
      "\n",
      "by a real world DICOM file which has a value\n",
      "\n",
      "`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\n",
      "\n",
      "where IS is a Value Representation defined as\n",
      "\n",
      "> IS - Integer String\n",
      "\n",
      "> A string of characters representing an Integer in base-10 (decimal), shall contain only the characters 0 - 9, with an optional leading \"+\" or \"-\". It may be padded with leading and/or trailing spaces. Embedded spaces are not allowed.\n",
      "\n",
      "> The integer, n, represented shall be in the range: -231<= n <= (231-1).\n",
      "\n",
      "[See DICOM Part 5 Section 6.2](https://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\n",
      "\n",
      "which means `14.5` is an invalid value due to the fractional portion .5 which definitely would lead to a loss in precision if converted to a pure integer value (of 14). \n",
      "\n",
      "After discussion with a senior engineer for the vendor, the following dialogue was obtained which quotes an article by David Clune, a well-respected, long-time member of the DICOM committee and community:\n",
      "\n",
      "> The tag pair in question is meant to contain the mAs value used for the exposure, which is not constrained to integer values, but for some reason the DICOM standard defines it as such.\n",
      "\n",
      "> An interesting article from someone responsible for maintaining the DICOM documentation explains the conundrum quite well:  \n",
      "\n",
      "http://dclunie.blogspot.com/2008/11/dicom-exposure-attribute-fiasco.html\n",
      "\n",
      "> Of note are two excerpts from that article:\n",
      "\n",
      "> \"The original ACR-NEMA standard specified ASCII numeric data elements for Exposure, Exposure Time and X-Ray Tube Current that could be decimal values; for no apparent reason DICOM 3.0 in 1993 constrained these to be integers, which for some modalities and subjects are too small to be sufficiently precise\"\n",
      "\n",
      "> and\n",
      "\n",
      "> \"The authors of DICOM, in attempting to maintain some semblance of backward compatibility with ACR-NEMA and at the same time apply more precise constraints, re-defined all ACR-NEMA data elements of VR AN as either IS or DS, the former being the AN integer numbers (with new size constraints), and the latter being the AN fixed point and floating point numbers. In the process of categorizing the old data elements into either IS or DS, not only were the obvious integers (like counts of images and other things) made into integers, but it appears that also any \"real world\" attribute that in somebody's expert opinion did not need greater precision than a whole integer, was so constrained as well.\"\n",
      "\n",
      "> I have inspected a few random DICOM files generated by various modalities and the value is stored accurately, even though it is a violation of the explicit value representation. Additionally, I have worked with (and support) various PACS platforms, and this is the first time this has been raised as an issue. So technically, you are correct that encoding that value as decimal violates the explicit VR, but it appears to be common practice to do so. \n",
      "\n",
      "**Expected behavior**\n",
      "To deal with the reality of history with respect to the current standard, my opinion, as a long-standing DICOM PACS implementer at Medstrat, is that there is nothing to gain and everything to lose by raising a `TypeError` here. For cases where an integer VR, such as `IS`, could be read as a floating point number instead, then it should be allowed to be so, for at least a limited whitelist of tags.\n",
      "\n",
      "Arguments against which come to mind are of the ilk that do not heed \"Although practicality beats purity\" as can be read if you \n",
      "\n",
      "[`>>> import this`](https://peps.python.org/pep-0020/)\n",
      "\n",
      "> Special cases aren't special enough to break the rules.\n",
      "> Although practicality beats purity.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "\n",
      "`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\n",
      "\n",
      "Set any DICOM file to have the above for `Exposure` and then do this:\n",
      "\n",
      "```\n",
      ">>> from pydicom import config\n",
      ">>> pydicom.__version__\n",
      "'2.3.0'\n",
      ">>> config.settings.reading_validation_mode = config.IGNORE\n",
      ">>> ds = pydicom.dcmread('1.2.840.113619.2.107.20220429121335.1.1.dcm')\n",
      ">>> ds\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2306, in __str__\n",
      "    return self._pretty_str()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2020, in _pretty_str\n",
      "    for elem in self:\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 1240, in __iter__\n",
      "    yield self[tag]\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 939, in __getitem__\n",
      "    self[tag] = DataElement_from_raw(elem, character_set, self)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\n",
      "    value = convert_value(vr, raw, encoding)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 771, in convert_value\n",
      "    return converter(byte_string, is_little_endian, num_format)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 348, in convert_IS_string\n",
      "    return MultiString(num_string, valtype=pydicom.valuerep.IS)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1213, in MultiString\n",
      "    return valtype(splitup[0])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1131, in __new__\n",
      "    raise TypeError(\"Could not convert value to integer without loss\")\n",
      "TypeError: Could not convert value to integer without loss\n",
      "```\n",
      "\n",
      "**Your environment**\n",
      "\n",
      "```bash\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | Darwin-21.5.0-x86_64-i386-64bit\n",
      "Python       | 3.7.5 (v3.7.5:5c02a39a0b, Oct 14 2019, 18:49:57)  [Clang 6.0 (clang-600.0.57)]\n",
      "pydicom      | 2.2.2\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | _module not found_\n",
      "PIL          | _module not found_\n",
      "pylibjpeg    | _module not found_\n",
      "openjpeg     | _module not found_\n",
      "libjpeg      | _module not found_\n",
      "```\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 33 lines----------------\n",
      "LookupError: unknown encoding: Not Supplied\n",
      "#### Description\n",
      "Output from `ds = pydicom.read_file(dcmFile)` (an RTSTRUCT dicom file, SOP UID 1.2.840.10008.5.1.4.1.1.481.3) results in some tags throwing a LookupError: \"LookupError: unknown encoding: Not Supplied\"\n",
      "Specific tags which cannot be decoded are as follows:\n",
      "['DeviceSerialNumber',\n",
      " 'Manufacturer',\n",
      " 'ManufacturerModelName',\n",
      " 'PatientID',\n",
      " 'PatientName',\n",
      " 'RTROIObservationsSequence',\n",
      " 'ReferringPhysicianName',\n",
      " 'SeriesDescription',\n",
      " 'SoftwareVersions',\n",
      " 'StructureSetLabel',\n",
      " 'StructureSetName',\n",
      " 'StructureSetROISequence',\n",
      " 'StudyDescription',\n",
      " 'StudyID']\n",
      "\n",
      "I suspect that it's due to the fact that `ds.SpecificCharacterSet = 'Not Supplied'`, but when I try to set `ds.SpecificCharacterSet` to something reasonable (ie ISO_IR_100 or 'iso8859'), it doesn't seem to make any difference.\n",
      "\n",
      "Reading the same file, with NO modifications, in gdcm does not result in any errors and all fields are readable.\n",
      "\n",
      "#### Steps/Code to Reproduce\n",
      "```py\n",
      "import pydicom \n",
      "ds = pydicom.read_file(dcmFile)\n",
      "print(ds.PatientName)\n",
      "```\n",
      "\n",
      "#### Expected Results\n",
      "No error is thrown and the name of the patient is printed.\n",
      "\n",
      "#### Actual Results\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 706, in __str__\n",
      "    return '='.join(self.components).__str__()\n",
      "  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 641, in components\n",
      "    self._components = _decode_personname(groups, self.encodings)\n",
      "  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in _decode_personname\n",
      "    for comp in components]\n",
      "  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in <listcomp>\n",
      "    for comp in components]\n",
      "  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\charset.py\", line 129, in decode_string\n",
      "    return value.decode(encodings[0])\n",
      "LookupError: unknown encoding: Not Supplied\n",
      "\n",
      "#### Versions\n",
      "Platform: Windows-10-10.0.17763-SP0\n",
      "Python Version: Python 3.6.4 |Anaconda, Inc.| (default, Mar 12 2018, 20:20:50) [MSC v.1900 64 bit (AMD64)]\n",
      "pydicom Version: pydicom 1.2.2\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 211 lines----------------\n",
      "To_Json 'str' object has no attribute 'components'\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\n",
      "\n",
      "When converting a dataset to json the following error occurs.\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"~/pacs-proxy/pacs/service.py\", line 172, in saveFunction\n",
      "    jsonObj = ds.to_json()\n",
      "  File \"~/lib/python3.6/site-packages/pydicom/dataset.py\", line 2046, in to_json\n",
      "    dump_handler=dump_handler\n",
      "  File \"~/lib/python3.6/site-packages/pydicom/dataelem.py\", line 447, in to_json\n",
      "    if len(elem_value.components) > 2:\n",
      "AttributeError: 'str' object has no attribute 'components'\n",
      "```\n",
      "#### Steps/Code to Reproduce\n",
      "\n",
      "ds = pydicom.dcmread(\"testImg\")\n",
      "jsonObj = ds.to_json()\n",
      "\n",
      "I'm working on getting an anonymous version of the image, will update. But any advice, suggestions would be appreciated.\n",
      "\n",
      "#### \n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 247 lines----------------\n",
      "Codify not generating content sequences correctly\n",
      "**Describe the bug**\n",
      "I am trying to generate a radiation dose structure report. I ran Codify on an existing RDSR to generate a template. The sequence content is reproduced but does not seem to be attached  to the base dataset. When I run the generated python file the dicom file it saves has no sequence content information.\n",
      "\n",
      "**Expected behavior**\n",
      "I expect the dicom file generated by the python code from Codify to be similar to the original file.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "$ python codify X-RayRadiationDoseReport001_ESR.dcm rdsr.py\n",
      "$ python rsdr.py\n",
      "\n",
      "I am not able to attached the above files but can supply them.\n",
      "\n",
      "**Your environment**\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | Linux-5.18.7-200.fc36.x86_64-x86_64-with-glibc2.35\n",
      "Python       | 3.10.5 (main, Jun  9 2022, 00:00:00) [GCC 12.1.1 20220507 (Red Hat 12.1.1-1)]\n",
      "pydicom      | 2.3.0\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | 1.22.4\n",
      "PIL          | 9.2.0\n",
      "pylibjpeg    | _module not found_\n",
      "openjpeg     | _module not found_\n",
      "libjpeg      | _module not found_\n",
      "\n",
      "Regards\n",
      "Alan\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 182 lines----------------\n",
      "Add Tag and VR to the bulk data handling in `from_json`\n",
      "Currently, if you convert back to a Dataset format from a JSON format, you MUST re-hydrate all of the bulk data URI's or you will loose the information.\n",
      "\n",
      "This causes a problem if you just wish to use the Dataset's header (maybe to extract some data, or rearrange some data), because now you have to pay the cost of getting all the pixel data and then handling the pixel data again upon conversion back to JSON\n",
      "\n",
      "**Describe the solution you'd like**\n",
      "Add the tag and the vr to the bulk data handler in `from_json` (this can be done in a backwards compatible way). This will allow the user to store the BulkDataURI's by tag in a map, return dummy data large enough to trigger the bulk handling when to_json is called next, and to use the map to convert back to the original URI's when bulk handling is triggered from to_json.\n",
      "\n",
      "I'm going to drop a PR tomorrow that does this in a fully backward compatible, non-breaking fashion.\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 50 lines----------------\n",
      "DA class is inconsistent\n",
      "**Describe the bug**\n",
      "pydicom.valuerep.DA accepts strings or datetime.date objects - but DA objects created with datetime.date inputs are invalid. \n",
      "\n",
      "**Expected behavior**\n",
      "I would expect both of these expressions to generate the same output:\n",
      "```\n",
      "print(f'DA(\"20201117\") => {DA(\"20201117\")}')\n",
      "print(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\n",
      "```\n",
      "but instead I get\n",
      "```\n",
      "DA(\"20201117\") => 20201117\n",
      "DA(date(2020, 11, 17)) => 2020-11-17\n",
      "```\n",
      "The hyphens inserted into the output are not valid DICOM - see the DA description in [Table 6.2-1](http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\n",
      "\n",
      "**Steps To Reproduce**\n",
      "Run the following commands:\n",
      "```\n",
      "from pydicom.valuerep import DA\n",
      "from pydicom.dataset import Dataset\n",
      "from datetime import date, datetime\n",
      "\n",
      "print(f'DA(\"20201117\") => {DA(\"20201117\")}')\n",
      "print(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\n",
      "\n",
      "# 1. JSON serialization with formatted string works\n",
      "ds = Dataset()\n",
      "ds.ContentDate = '20201117'\n",
      "json_output = ds.to_json()\n",
      "print(f'json_output works = {json_output}')\n",
      "\n",
      "# 2. JSON serialization with date object input is invalid.\n",
      "ds = Dataset()\n",
      "ds.ContentDate = str(DA(date(2020, 11, 17)))\n",
      "json_output = ds.to_json()\n",
      "print(f'json_output with str(DA..) - invalid DICOM {json_output}')\n",
      "\n",
      "# 3. JSON serialization with date object fails\n",
      "ds = Dataset()\n",
      "ds.ContentDate = DA(date(2020, 11, 17))\n",
      "\n",
      "# Exception on this line: TypeError: Object of type DA is not JSON serializable\n",
      "json_output = ds.to_json()\n",
      "\n",
      "```\n",
      "\n",
      "I believe that all three approaches should work - but only the first is valid. The method signature on DA's `__new__` method accepts datetime.date objects. \n",
      "\n",
      "**Your environment**\n",
      "```\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | macOS-10.15.7-x86_64-i386-64bit\n",
      "Python       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\n",
      "pydicom      | 2.1.0\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | 1.19.4\n",
      "PIL          | 8.0.1\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 52 lines----------------\n",
      "Empty data elements with value representation SQ are set to None\n",
      "**Describe the bug**\n",
      "In the current `master`, empty data elements are not read correctly from files. The attribute value is set to `None` instead of `[]`.\n",
      "\n",
      "**Expected behavior**\n",
      "Create empty list `[]` for empty sequence, i.e., a sequence with zero items.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "```python\n",
      "import pydicom\n",
      "ds = pydicom.Dataset()\n",
      "ds.AcquisitionContextSequence = []\n",
      "print(ds)\n",
      "ds.is_little_endian = True\n",
      "ds.is_implicit_VR = True\n",
      "ds.save_as('/tmp/test.dcm')\n",
      "\n",
      "reloaded_ds = pydicom.dcmread('/tmp/test.dcm', force=True)\n",
      "print(reloaded_ds)\n",
      "```\n",
      "This prints:\n",
      "```\n",
      "(0040, 0555)  Acquisition Context Sequence   0 item(s) ----\n",
      "...\n",
      "TypeError: With tag (0040, 0555) got exception: object of type 'NoneType' has no len()\n",
      "Traceback (most recent call last):\n",
      "  File \"/private/tmp/pydicom/pydicom/tag.py\", line 30, in tag_in_exception\n",
      "    yield\n",
      "  File \"/private/tmp/pydicom/pydicom/dataset.py\", line 1599, in _pretty_str\n",
      "    len(data_element.value)))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "```\n",
      "\n",
      "**Your environment**\n",
      "```\n",
      "Darwin-18.6.0-x86_64-i386-64bit\n",
      "Python  3.7.3 (default, Mar 27 2019, 09:23:15)\n",
      "[Clang 10.0.1 (clang-1001.0.46.3)]\n",
      "pydicom  1.4.0.dev0\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 203 lines----------------\n",
      "Pickling/unpickling timezone in DT does not work\n",
      "**Describe the bug**\n",
      "\n",
      "The following tests fail because the timezone is not set in the unpickled `DT`:\n",
      "```py\n",
      "    def test_pickling_with_timezone():\n",
      "        dt = pydicom.valuerep.DT(\"19111213212123-0630\")\n",
      "        loaded_dt = pickle.loads(pickle.dumps(dt))\n",
      "        assert dt == loaded_dt\n",
      "\n",
      "    def test_pickling_dt_from_datetime_with_timezone():\n",
      "        tz_info = timezone(timedelta(seconds=-23400), '-0630')\n",
      "        dt_object = datetime(2022, 12, 31, 23, 59, 59, 42, tzinfo=tz_info)\n",
      "        dt = pydicom.valuerep.DT(dt_object)\n",
      "        loaded_dt = pickle.loads(pickle.dumps(dt))\n",
      "        assert dt == loaded_dt\n",
      "```\n",
      "\n",
      "This is a spin-off of PR #1365, see [this comment](https://github.com/pydicom/pydicom/pull/1365#issuecomment-829544827).\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 313 lines----------------\n",
      "Strings with Value Representation DS are too long\n",
      "**Describe the bug**\n",
      "Strings of Value Representation DS are restricted to a maximum length of 16 bytes according to [Part 5 Section 6.2](http://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html#para_15754884-9ca2-4b12-9368-d66f32bc8ce1), but `pydicom.valuerep.DS` may represent numbers with more than 16 bytes.\n",
      "\n",
      "**Expected behavior**\n",
      "`pydicom.valuerep.DS` should create a string of maximum length 16, when passed a fixed point number with many decimals.\n",
      "\n",
      "**Steps To Reproduce**\n",
      "```python\n",
      "len(str(pydicom.valuerep.DS(3.14159265358979323846264338327950288419716939937510582097)).encode('utf-8'))\n",
      "len(str(pydicom.valuerep.DS(\"3.14159265358979323846264338327950288419716939937510582097\")).encode('utf-8'))\n",
      "```\n",
      "returns `17` and `58`, respectively, instead of `16`.\n",
      "\n",
      "**Your environment**\n",
      "```\n",
      "module       | version\n",
      "------       | -------\n",
      "platform     | macOS-10.15.6-x86_64-i386-64bit\n",
      "Python       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\n",
      "pydicom      | 2.0.0\n",
      "gdcm         | _module not found_\n",
      "jpeg_ls      | _module not found_\n",
      "numpy        | 1.19.4\n",
      "PIL          | 8.0.1\n",
      "```\n",
      "\n",
      "-------------PROBLEM STATEMENT: pydicom/pydicom----------------\n",
      "-------------Fix: 139 lines----------------\n",
      "Encoding to ISO 2022 IR 159 doesn't work\n",
      "<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\n",
      "\n",
      "#### Description\n",
      "Encoding to ISO 2022 IR 159 doesn't work even if 'ISO 2022 IR 159' is passed to pydicom.charset.convert_encodings.\n",
      "\n",
      "#### Steps/Code to Reproduce\n",
      "ISO 2022 IR 159 is designed as supplement characters to ISO 2022 IR 87. So these characters are not frequent use. But person name sometimes contains them. In the following example, the letter of \"é·—\" is only in ISO 2022 IR 159. But we cannot encode them correctly. \n",
      "\n",
      "```\n",
      "import pydicom\n",
      "\n",
      "japanese_pn = u\"Mori^Ogai=æ£®^é·—å¤–=ã‚‚ã‚Š^ãŠã†ãŒã„\"\n",
      "specific_character_sets = [\"ISO 2022 IR 6\", \"ISO 2022 IR 87\", \"ISO 2022 IR 159\"]\n",
      "expect_encoded = (\n",
      "    b\"\\x4d\\x6f\\x72\\x69\\x5e\\x4f\\x67\\x61\\x69\\x3d\\x1b\\x24\\x42\\x3f\"\n",
      "    b\"\\x39\\x1b\\x28\\x42\\x5e\\x1b\\x24\\x28\\x44\\x6c\\x3f\\x1b\\x24\\x42\"\n",
      "    b\"\\x33\\x30\\x1b\\x28\\x42\\x3d\\x1b\\x24\\x42\\x24\\x62\\x24\\x6a\\x1b\"\n",
      "    b\"\\x28\\x42\\x5e\\x1b\\x24\\x42\\x24\\x2a\\x24\\x26\\x24\\x2c\\x24\\x24\"\n",
      "    b\"\\x1b\\x28\\x42\"\n",
      ")\n",
      "\n",
      "python_encodings = pydicom.charset.convert_encodings(specific_character_sets)\n",
      "actual_encoded = pydicom.charset.encode_string(japanese_pn, python_encodings)\n",
      "\n",
      "print(\"actual:{}\".format(actual_encoded))\n",
      "print(\"expect:{}\".format(expect_encoded))\n",
      "```\n",
      "#### Expected Results\n",
      "<!-- Please paste or describe the expected results.\n",
      "Example: No error is thrown and the name of the patient is printed.-->\n",
      "```\n",
      "b'Mori^Ogai=\\x1b$B?9\\x1b(B^\\x1b$(Dl?\\x1b$B30\\x1b(B=\\x1b$B$b$j\\x1b(B^\\x1b$B$*$&$,$$\\x1b(B'\n",
      "```\n",
      "#### Actual Results\n",
      "<!-- Please paste or specifically describe the actual output or traceback.\n",
      "(Use %xmode to deactivate ipython's trace beautifier)\n",
      "Example: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\n",
      "-->\n",
      "```\n",
      "b'Mori^Ogai=?^??=??^????'\n",
      "```\n",
      "\n",
      "And the followin exception occurs.\n",
      "\n",
      "```\n",
      "/PATH/TO/MY/PYTHON/PACKAGES/pydicom/charset.py:488: UserWarning: Failed to encode value with encodings: iso8859, iso2022_jp, iso-2022-jp - using replacement characters in encoded string\n",
      "  .format(', '.join(encodings)))\n",
      "```\n",
      "\n",
      "#### Versions\n",
      "<!--\n",
      "Please run the following snippet and paste the output below.\n",
      "import platform; print(platform.platform())\n",
      "import sys; print(\"Python\", sys.version)\n",
      "import pydicom; print(\"pydicom\", pydicom.__version__)\n",
      "-->\n",
      "```\n",
      "Linux-4.15.0-55-generic-x86_64-with-debian-buster-sid\n",
      "Python 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)\n",
      "[GCC 7.3.0]\n",
      "pydicom 1.3.0\n",
      "```\n",
      "\n",
      "\n",
      "<!-- Thanks for contributing! -->\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_row(row):\n",
    "    print(\"------ REPO ------\")\n",
    "    print(row[\"repo\"])\n",
    "    print(\"------ CREATED AT ------\")\n",
    "    print(row[\"created_at\"])\n",
    "    if row[\"hints_text\"]:\n",
    "        print(\"------ HINTS ------\")\n",
    "        print(row[\"hints_text\"])\n",
    "    print(\"------ PROBLEM STATEMENT ------\")\n",
    "    print(row[\"problem_statement\"])\n",
    "    print(\"------ PATCH ------\")\n",
    "    print(row[\"patch\"])\n",
    "    print(\"------ TESTS ------\")\n",
    "    print(row[\"test_patch\"])\n",
    "\n",
    "from itertools import islice\n",
    "for i, row in islice(df.iterrows(), 200):\n",
    "    # visualize_row(row)\n",
    "    print(f\"-------------PROBLEM STATEMENT: {row['repo']}----------------\")\n",
    "    print(f\"-------------Fix: {len(row['patch'].split('\\n'))} lines----------------\")\n",
    "    print(row[\"problem_statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo: <class 'str'>\n",
      "instance_id: <class 'str'>\n",
      "base_commit: <class 'str'>\n",
      "patch: <class 'str'>\n",
      "test_patch: <class 'str'>\n",
      "problem_statement: <class 'str'>\n",
      "hints_text: <class 'str'>\n",
      "created_at: <class 'str'>\n",
      "version: <class 'str'>\n",
      "FAIL_TO_PASS: <class 'str'>\n",
      "PASS_TO_PASS: <class 'str'>\n",
      "environment_setup_commit: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for k,v in df.iloc[199].items():\n",
    "    print(f\"{k}: {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements-dev.lock          (lock      ) has 138 lines\n",
      "codecov.yml                    (yml       ) has 16 lines\n",
      "LICENSE                        (          ) has 21 lines\n",
      "CHANGELOG.md                   (md        ) has 111 lines\n",
      "pyproject.toml                 (toml      ) has 62 lines\n",
      "README.md                      (md        ) has 143 lines\n",
      "requirements.lock              (lock      ) has 138 lines\n",
      "test_evaluation.py             (py        ) has 35 lines\n",
      "test_collect_cli.py            (py        ) has 27 lines\n",
      "test_cli.py                    (py        ) has 17 lines\n",
      "README_CN.md                   (md        ) has 89 lines\n",
      "README_JP.md                   (md        ) has 90 lines\n",
      "README_TW.md                   (md        ) has 89 lines\n",
      "README.md                      (md        ) has 76 lines\n",
      "update_swe_bench.ipynb         (ipynb     ) has 632 lines\n",
      "README.md                      (md        ) has 155 lines\n",
      "sweep_conda_links.py           (py        ) has 61 lines\n",
      "check_harness.ipynb            (ipynb     ) has 115 lines\n",
      "report.md                      (md        ) has 24 lines\n",
      "get_devin_preds.ipynb          (ipynb     ) has 145 lines\n",
      "collection.md                  (md        ) has 87 lines\n",
      "build_deploy.sh                (sh        ) has 6 lines\n",
      "evaluation.md                  (md        ) has 32 lines\n",
      "evaluation.png                 (png       ) has 0 lines\n",
      "swellama_banner.png            (png       ) has 0 lines\n",
      "collection.png                 (png       ) has 0 lines\n",
      "validation.png                 (png       ) has 0 lines\n",
      "teaser.png                     (png       ) has 0 lines\n",
      "PKG-INFO                       (          ) has 189 lines\n",
      "SOURCES.txt                    (txt       ) has 51 lines\n",
      "requires.txt                   (txt       ) has 26 lines\n",
      "top_level.txt                  (txt       ) has 1 lines\n",
      "dependency_links.txt           (txt       ) has 1 lines\n",
      "__init__.py                    (py        ) has 69 lines\n",
      "get_top_pypi.py                (py        ) has 110 lines\n",
      "run_get_tasks_pipeline.sh      (sh        ) has 10 lines\n",
      "build_dataset_ft.py            (py        ) has 83 lines\n",
      "print_pulls.py                 (py        ) has 79 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "get_tasks_pipeline.py          (py        ) has 162 lines\n",
      "README.md                      (md        ) has 54 lines\n",
      "run_build_dataset_ft.sh        (sh        ) has 6 lines\n",
      "utils.py                       (py        ) has 404 lines\n",
      "build_dataset.py               (py        ) has 190 lines\n",
      "make_repo.sh                   (sh        ) has 73 lines\n",
      "call_make_repo.py              (py        ) has 18 lines\n",
      "remove_envs.py                 (py        ) has 96 lines\n",
      "delete_gh_workflows.py         (py        ) has 54 lines\n",
      "criteria.py                    (py        ) has 152 lines\n",
      "README.md                      (md        ) has 13 lines\n",
      "make_lite.py                   (py        ) has 80 lines\n",
      "run_evaluation.py              (py        ) has 569 lines\n",
      "prepare_images.py              (py        ) has 97 lines\n",
      "dockerfiles.py                 (py        ) has 70 lines\n",
      "docker_utils.py                (py        ) has 318 lines\n",
      "grading.py                     (py        ) has 264 lines\n",
      "test_spec.py                   (py        ) has 303 lines\n",
      "constants.py                   (py        ) has 1496 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "utils.py                       (py        ) has 330 lines\n",
      "remove_containers.py           (py        ) has 54 lines\n",
      "docker_build.py                (py        ) has 532 lines\n",
      "log_parsers.py                 (py        ) has 278 lines\n",
      "codellama_device_maps.json     (json      ) has 1 lines\n",
      "run_api.py                     (py        ) has 563 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "run_llama.py                   (py        ) has 414 lines\n",
      "run_live.py                    (py        ) has 270 lines\n",
      "README.md                      (md        ) has 69 lines\n",
      "distributed_attention.py       (py        ) has 75 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "modeling_flash_llama.py        (py        ) has 875 lines\n",
      "eval_retrieval.py              (py        ) has 59 lines\n",
      "create_instance.py             (py        ) has 435 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "tokenize_dataset.py            (py        ) has 209 lines\n",
      "README.md                      (md        ) has 77 lines\n",
      "utils.py                       (py        ) has 300 lines\n",
      "bm25_retrieval.py              (py        ) has 545 lines\n",
      "create_text_dataset.py         (py        ) has 242 lines\n",
      "get_versions.py                (py        ) has 393 lines\n",
      "constants.py                   (py        ) has 56 lines\n",
      "__init__.py                    (py        ) has 0 lines\n",
      "README.md                      (md        ) has 25 lines\n",
      "utils.py                       (py        ) has 45 lines\n",
      "run_get_versions.sh            (sh        ) has 15 lines\n",
      "get_versions_pvlib-python.py   (py        ) has 70 lines\n",
      "get_versions_sqlfluff.py       (py        ) has 94 lines\n",
      "get_versions_astropy.py        (py        ) has 73 lines\n",
      "get_versions_matplotlib.py     (py        ) has 57 lines\n",
      "get_versions_xarray.py         (py        ) has 62 lines\n",
      "get_versions_pydicom.py        (py        ) has 41 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from swe_bench_solver.ls import files_list\n",
    "\n",
    "@dataclass\n",
    "class File:\n",
    "    name: str\n",
    "    fullpath: str\n",
    "    size: int\n",
    "    n_lines: int\n",
    "    type: str\n",
    "\n",
    "code_dir = os.path.expanduser(\"~/SWE-bench\")\n",
    "file_list = files_list(code_dir)\n",
    "\n",
    "for file in file_list:\n",
    "    print(f\"{file.name.ljust(30)} ({file.type.ljust(10)}) has {file.n_lines} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
